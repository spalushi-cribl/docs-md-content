
<h1 id="cribl-edge-api-destinations">Cribl Edge API - Destinations v4.15.0-f275b803</h1>

> Scroll down for example requests and responses.

This API Reference lists available REST endpoints, along with their supported operations for accessing, creating, updating, or deleting resources. See our complementary product documentation at [docs.cribl.io](http://docs.cribl.io).

Base URLs:

* <a href="/">/</a>

Web: <a href="https://portal.support.cribl.io">Support</a> 

# Authentication

- HTTP Authentication, scheme: bearer 

<h1 id="cribl-edge-api-destinations-destinations">Destinations</h1>

## List all Destinations

<a id="opIdlistOutput"></a>

> Code samples

`GET /system/outputs`

Get a list of all Destinations.

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "default",
      "pipeline": "string",
      "systemFields": [
        "cribl_pipe"
      ],
      "environment": "string",
      "streamtags": [],
      "defaultId": "string"
    }
  ]
}
```

<h3 id="list-all-destinations-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="list-all-destinations-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDefault](#schemaoutputdefault)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» defaultId|string|true|none|ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWebhook](#schemaoutputwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» format|string|false|none|How to format events before sending out|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Authentication method to use for the HTTP request|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» description|string|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» url|string|false|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinel](#schemaoutputsentinel)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» loginUrl|string|true|none|URL for OAuth|
|»»» secret|string|true|none|Secret parameter value to pass in request body|
|»»» client_id|string|true|none|JavaScript expression to compute the Client ID for the Azure application. Can be a constant.|
|»»» scope|string|false|none|Scope to pass in the OAuth request|
|»»» endpointURLConfiguration|string|true|none|Enter the data collection endpoint URL or the individual ID|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» format|any|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|
|»»» dcrID|string|false|none|Immutable ID for the Data Collection Rule (DCR)|
|»»» dceEndpoint|string|false|none|Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`|
|»»» streamName|string|false|none|The name of the stream (Sentinel table) in which to store the events|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDevnull](#schemaoutputdevnull)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSyslog](#schemaoutputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|The network protocol to use for sending out syslog messages|
|»»» facility|integer|false|none|Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.|
|»»» severity|integer|false|none|Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.|
|»»» appName|string|false|none|Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.|
|»»» messageFormat|string|false|none|The syslog message format depending on the receiver's support|
|»»» timestampFormat|string|false|none|Timestamp format to use when serializing event's time field|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» octetCountFraming|boolean|false|none|Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» description|string|false|none|none|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» maxRecordSize|number|false|none|Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.|
|»»» udpDnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.|
|»»» enableIpSpoofing|boolean|false|none|Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunk](#schemaoutputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» host|string|true|none|The hostname of the receiver|
|»»» port|number|true|none|The port to connect to on the provided host|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkLb](#schemaoutputsplunklb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» indexerDiscovery|boolean|false|none|Automatically discover indexers in indexer clustering environment.|
|»»» senderUnhealthyTimeAllowance|number|false|none|How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» indexerDiscoveryConfigs|object|false|none|List of configurations to set up indexer discovery in Splunk Indexer clustering environment.|
|»»»» site|string|true|none|Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.|
|»»»» masterUri|string|true|none|Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089|
|»»»» refreshIntervalSec|number|true|none|Time interval, in seconds, between two consecutive indexer list fetches from cluster manager|
|»»»» rejectUnauthorized|boolean|false|none|During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.|
|»»»» authTokens|[object]|false|none|Tokens required to authenticate to cluster manager for indexer discovery|
|»»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|true|none|Set of Splunk indexers to load-balance data to.|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkHec](#schemaoutputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» nextQueue|string|false|none|In the Splunk app, define which Splunk processing queue to send the events after HEC processing.|
|»»» tcpRouting|string|false|none|In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|Splunk HEC authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputTcpjson](#schemaoutputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» sendHeader|boolean|false|none|Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Optional authentication token to include as part of the connection header|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWavefront](#schemaoutputwavefront)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» domain|string|true|none|WaveFront domain name, e.g. "longboard"|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSignalfx](#schemaoutputsignalfx)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» realm|string|true|none|SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputFilesystem](#schemaoutputfilesystem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|true|none|Final destination for the output files|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputS3](#schemaoutputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureBlob](#schemaoutputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» containerName|string|true|none|The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» createContainer|boolean|false|none|Create the configured container in Azure Blob Storage if it does not already exist|
|»»» destPath|string|false|none|Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» authType|string|false|none|none|
|»»» storageClass|string|false|none|none|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureDataExplorer](#schemaoutputazuredataexplorer)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» clusterUrl|string|true|none|The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.|
|»»» database|string|true|none|Name of the database containing the table where data will be ingested|
|»»» table|string|true|none|Name of the table to ingest data into|
|»»» validateDatabaseSettings|boolean|false|none|When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.|
|»»» ingestMode|string|false|none|none|
|»»» oauthEndpoint|string|true|none|Endpoint used to acquire authentication tokens from Azure|
|»»» tenantId|string|true|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»» clientId|string|true|none|client_id to pass in the OAuth request parameter|
|»»» scope|string|true|none|Scope to pass in the OAuth request parameter|
|»»» oauthType|string|true|none|The type of OAuth 2.0 client credentials grant flow to use|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret that you generated for your app in the Azure portal|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|false|none|The certificate you registered as credentials for your app in the Azure portal|
|»»» format|string|false|none|Format of the output data|
|»»» compress|string|true|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» isMappingObj|boolean|false|none|Send a JSON mapping object instead of specifying an existing named data mapping|
|»»» mappingObj|string|false|none|Enter a JSON object that defines your desired data mapping|
|»»» mappingRef|string|false|none|Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.|
|»»» ingestUrl|string|false|none|The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushImmediately|boolean|false|none|Bypass the data management service's aggregation mechanism|
|»»» retainBlobOnSuccess|boolean|false|none|Prevent blob deletion after ingestion is complete|
|»»» extentTags|[object]|false|none|Strings or tags associated with the extent (ingested data shard)|
|»»»» prefix|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» ingestIfNotExists|[object]|false|none|Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists|
|»»»» value|string|true|none|none|
|»»» reportLevel|string|false|none|Level of ingestion status reporting. Defaults to FailuresOnly.|
|»»» reportMethod|string|false|none|Target of the ingestion status reporting. Defaults to Queue.|
|»»» additionalProperties|[object]|false|none|Optionally, enter additional configuration properties to send to the ingestion service|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureLogs](#schemaoutputazurelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logType|string|true|none|The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.|
|»»» resourceId|string|false|none|Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» apiUrl|string|false|none|The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter workspace ID and workspace key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» workspaceId|string|false|none|Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.|
|»»» workspaceKey|string|false|none|Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.|
|»»» keypairSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKinesis](#schemaoutputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» streamName|string|true|none|Kinesis stream name to send events to.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» concurrency|number|false|none|Maximum number of ongoing put requests before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» compression|string|false|none|Compression type to use for records|
|»»» useListShards|boolean|false|none|Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.|
|»»» asNdjson|boolean|false|none|Batch events into a single record as NDJSON|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» maxEventsPerFlush|number|false|none|Maximum number of records to send in a single request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHoneycomb](#schemaoutputhoneycomb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dataset|string|true|none|Name of the dataset to send events to – e.g., observability|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» team|string|false|none|Team API key where the dataset belongs|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureEventhub](#schemaoutputazureeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topic|string|true|none|The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleChronicle](#schemaoutputgooglechronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» logFormatType|string|true|none|none|
|»»» region|string|false|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» extraLogTypes|[object]|false|none|Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.|
|»»»» logType|string|true|none|none|
|»»»» description|string|false|none|none|
|»»» logType|string|false|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» customerId|string|false|none|A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every batch|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» udmType|string|false|none|Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.|
|»»» apiKey|string|false|none|Organization's API key in Google SecOps|
|»»» apiKeySecret|string|false|none|Select or create a stored text secret|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudStorage](#schemaoutputgooglecloudstorage)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» awsApiKey|string|false|none|HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudLogging](#schemaoutputgooglecloudlogging)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logLocationType|string|true|none|none|
|»»» logNameExpression|string|true|none|JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» sanitizeLogNames|boolean|false|none|none|
|»»» payloadFormat|string|false|none|Format to use when sending payload. Defaults to Text.|
|»»» logLabels|[object]|false|none|Labels to apply to the log entry|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» resourceTypeExpression|string|false|none|JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".|
|»»» resourceTypeLabels|[object]|false|none|Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» severityExpression|string|false|none|JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".|
|»»» insertIdExpression|string|false|none|JavaScript expression to compute the value of the insert ID field.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body.|
|»»» maxPayloadEvents|number|false|none|Max number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second.|
|»»» requestMethodExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestUrlExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» statusExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» responseSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» userAgentExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» remoteIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» serverIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» refererExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» latencyExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheLookupExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheHitExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheValidatedExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheFillBytesExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» protocolExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» idExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» producerExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» firstExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» lastExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» fileExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» lineExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» functionExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» uidExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» indexExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» totalSplitsExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» traceExpression|string|false|none|A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» spanIdExpression|string|false|none|A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» traceSampledExpression|string|false|none|A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» logLocationExpression|string|true|none|JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» payloadExpression|string|false|none|JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGooglePubsub](#schemaoutputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicName|string|true|none|ID of the topic to send events to.|
|»»» createTopic|boolean|false|none|If enabled, create topic if it does not exist.|
|»»» orderedDelivery|boolean|false|none|If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.|
|»»» region|string|false|none|Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» batchSize|number|false|none|The maximum number of items the Google API should batch before it sends them to the topic.|
|»»» batchTimeout|number|false|none|The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send.|
|»»» flushPeriod|number|false|none|Maximum time to wait before sending a batch (when batch size limit is not reached)|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputExabeam](#schemaoutputexabeam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» encodedConfiguration|string|false|none|Enter an encoded string containing Exabeam configurations|
|»»» collectorInstanceId|string|true|none|ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888|
|»»» siteName|string|false|none|Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.|
|»»» siteId|string|false|none|Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.|
|»»» timezoneOffset|string|false|none|none|
|»»» awsApiKey|string|false|none|HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKafka](#schemaoutputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputConfluentCloud](#schemaoutputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMsk](#schemaoutputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElastic](#schemaoutputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» index|string|true|none|Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» docType|string|false|none|Document type to use for events. Can be overwritten by an event's __type field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» extraParams|[object]|false|none|none|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticVersion|string|false|none|Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.|
|»»» elasticPipeline|string|false|none|Optional Elasticsearch destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» writeAction|string|false|none|Action to use when writing events. Must be set to `Create` when writing to a data stream.|
|»»» retryPartialErrors|boolean|false|none|Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElasticCloud](#schemaoutputelasticcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Enter Cloud ID of the Elastic Cloud environment to send events to|
|»»» index|string|true|none|Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» extraParams|[object]|false|none|Extra parameters to use in HTTP requests|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticPipeline|string|false|none|Optional Elastic Cloud Destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelic](#schemaoutputnewrelic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» logType|string|false|none|Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.|
|»»» messageField|string|false|none|Name of field to send as log message value. If not present, event will be serialized and sent as JSON.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelicEvents](#schemaoutputnewrelicevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» accountId|string|true|none|New Relic account ID|
|»»» eventType|string|true|none|Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputInfluxdb](#schemaoutputinfluxdb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write|
|»»» useV2API|boolean|false|none|The v2 API can be enabled with InfluxDB versions 1.8 and later.|
|»»» timestampPrecision|string|false|none|Sets the precision for the supplied Unix time values. Defaults to milliseconds.|
|»»» dynamicValueFieldName|boolean|false|none|Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.|
|»»» valueFieldName|string|false|none|Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|InfluxDB authentication type|
|»»» description|string|false|none|none|
|»»» database|string|false|none|Database to write to.|
|»»» bucket|string|false|none|Bucket to write to.|
|»»» org|string|false|none|Organization ID for this bucket.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudwatch](#schemaoutputcloudwatch)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logGroupName|string|true|none|CloudWatch log group to associate events with|
|»»» logStreamName|string|true|none|Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the CloudWatchLogs is located|
|»»» endpoint|string|false|none|CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access CloudWatchLogs|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMinio](#schemaoutputminio)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|MinIO service url (e.g. http://minioHost:9000)|
|»»» bucket|string|true|none|Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|string|false|none|Region where the MinIO service/cluster is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsd](#schemaoutputstatsd)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsdExt](#schemaoutputstatsdext)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGraphite](#schemaoutputgraphite)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRouter](#schemaoutputrouter)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» rules|[object]|true|none|Event routing rules|
|»»»» filter|string|true|none|JavaScript expression to select events to send to output|
|»»»» output|string|true|none|Output to send matching events to|
|»»»» description|string|false|none|Description of this rule's purpose|
|»»»» final|boolean|false|none|Flag to control whether to stop the event from being checked against other rules|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSns](#schemaoutputsns)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicArn|string|true|none|The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`|
|»»» messageGroupId|string|true|none|Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» maxRetries|number|false|none|Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the SNS is located|
|»»» endpoint|string|false|none|SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SNS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SNS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSqs](#schemaoutputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created). Defaults to Standard.|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» messageGroupId|string|false|none|This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSnmp](#schemaoutputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more SNMP destinations to forward traps to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 162|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSumoLogic](#schemaoutputsumologic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Sumo Logic HTTP collector URL to which events should be sent|
|»»» customSource|string|false|none|Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.|
|»»» customCategory|string|false|none|Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.|
|»»» format|string|false|none|Preserve the raw event format instead of JSONifying it|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatadog](#schemaoutputdatadog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» contentType|string|false|none|The content type to use when sending logs|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» source|string|false|none|Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.|
|»»» host|string|false|none|Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.|
|»»» service|string|false|none|Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.|
|»»» tags|[string]|false|none|List of tags to send with logs, such as 'env:prod' and 'env_staging:east'|
|»»» batchByTags|boolean|false|none|Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.|
|»»» allowApiKeyFromEvents|boolean|false|none|Allow API key to be set from the event's '__agent_api_key' field|
|»»» severity|string|false|none|Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.|
|»»» site|string|false|none|Datadog site to which events should be sent|
|»»» sendCountersAsCount|boolean|false|none|If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|Organization's API key in Datadog|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGrafanaCloud](#schemaoutputgrafanacloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» lokiUrl|string|false|none|The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net|
|»»» prometheusUrl|string|false|none|The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputLoki](#schemaoutputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send logs to|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» authType|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» enableDynamicHeaders|boolean|false|none|Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» username|string|false|none|Username for authentication|
|»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputPrometheus](#schemaoutputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send metrics to|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» sendMetadata|boolean|false|none|Generate and send metadata (`type` and `metricFamilyName`) requests|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» description|string|false|none|none|
|»»» metricsFlushPeriodSec|number|false|none|How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRing](#schemaoutputring)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» format|string|false|none|Format of the output data.|
|»»» partitionExpr|string|false|none|JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.|
|»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»» compress|string|false|none|none|
|»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputOpenTelemetry](#schemaoutputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|Select a transport option for OpenTelemetry|
|»»» endpoint|string|true|none|The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputServiceNow](#schemaoutputservicenow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» protocol|string|true|none|Select a transport option for OpenTelemetry|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDataset](#schemaoutputdataset)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» messageField|string|false|none|Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.|
|»»» serverHostField|string|false|none|Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.|
|»»» timestampField|string|false|none|Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.|
|»»» defaultSeverity|string|false|none|Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» site|string|false|none|DataSet site to which events should be sent|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|A 'Log Write Access' API key for the DataSet account|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblTcp](#schemaoutputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblHttp](#schemaoutputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHumioHec](#schemaoutputhumiohec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|CrowdStrike Falcon LogScale authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCrowdstrikeNextGenSiem](#schemaoutputcrowdstrikenextgensiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL provided from a CrowdStrike data connector. <br>Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|none|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDlS3](#schemaoutputdls3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» partitioningFields|[string]|false|none|List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSecurityLake](#schemaoutputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|true|none|Region where the Amazon Security Lake is located.|
|»»» awsSecretKey|string|false|none|none|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Amazon Security Lake requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|true|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» accountId|string|true|none|ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.|
|»»» customSource|string|true|none|Name of the custom source configured in Amazon Security Lake|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblLake](#schemaoutputcribllake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|false|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Lake dataset to send the data to.|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» format|string|false|none|none|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDiskSpool](#schemaoutputdiskspool)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputClickHouse](#schemaoutputclickhouse)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of the ClickHouse instance. Example: http://localhost:8123/|
|»»» authType|string|false|none|none|
|»»» database|string|true|none|none|
|»»» tableName|string|true|none|Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".|
|»»» format|string|false|none|Data format to use when sending data to ClickHouse. Defaults to JSON Compact.|
|»»» mappingType|string|false|none|How event fields are mapped to ClickHouse columns.|
|»»» asyncInserts|boolean|false|none|Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» dumpFormatErrorsToDisk|boolean|false|none|Log the most recent event that fails to match the table schema|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» sqlUsername|string|false|none|Username for certificate authentication|
|»»» waitForAsyncInserts|boolean|false|none|Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.|
|»»» excludeMappingFields|[string]|false|none|Fields to exclude from sending to ClickHouse|
|»»» describeTable|string|false|none|Retrieves the table schema from ClickHouse and populates the Column Mapping table|
|»»» columnMappings|[object]|false|none|none|
|»»»» columnName|string|true|none|Name of the column in ClickHouse that will store field value|
|»»»» columnType|string|false|none|Type of the column in the ClickHouse database|
|»»»» columnValueExpression|string|true|none|JavaScript expression to compute value to be inserted into ClickHouse table|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputXsiam](#schemaoutputxsiam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Enter a token directly, or provide a secret referencing a token|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» url|string|false|none|XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|any|true|none|none|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|XSIAM authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNetflow](#schemaoutputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more NetFlow destinations to forward events to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 2055|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceHttp](#schemaoutputdynatracehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» format|string|true|none|How to format events before sending. Defaults to JSON. Plaintext is not currently supported.|
|»»» endpoint|string|true|none|none|
|»»» telemetryType|string|true|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» environmentId|string|false|none|ID of the environment to send to|
|»»» activeGateDomain|string|false|none|ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceOtlp](#schemaoutputdynatraceotlp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Select a transport option for Dynatrace|
|»»» endpoint|string|true|none|The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» endpointType|string|true|none|Select the type of Dynatrace endpoint configured|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinelOneAiSiem](#schemaoutputsentineloneaisiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|true|none|The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.|
|»»» endpoint|string|true|none|Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» baseUrl|string|false|none|Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.|
|»»» hostExpression|string|false|none|Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').|
|»»» sourceExpression|string|false|none|Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').|
|»»» sourceTypeExpression|string|false|none|Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.|
|»»» dataSourceCategoryExpression|string|false|none|Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.|
|»»» dataSourceNameExpression|string|false|none|Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').|
|»»» dataSourceVendorExpression|string|false|none|Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').|
|»»» eventTypeExpression|string|false|none|Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').|
|»»» host|string|false|none|Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').|
|»»» source|string|false|none|Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.|
|»»» sourceType|string|false|none|Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.|
|»»» dataSourceCategory|string|false|none|Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.|
|»»» dataSourceName|string|false|none|Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.|
|»»» dataSourceVendor|string|false|none|Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.|
|»»» eventType|string|false|none|Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputChronicle](#schemaoutputchronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» region|string|true|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» ingestionMethod|string|false|none|none|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» logType|string|true|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» gcpProjectId|string|true|none|The Google Cloud Platform (GCP) project ID to send events to|
|»»» gcpInstance|string|true|none|The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every event|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»»» rbacEnabled|boolean|false|none|Designate this label for role-based access control and filtering|
|»»» description|string|false|none|none|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatabricks](#schemaoutputdatabricks)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|false|none|Optional path to prepend to files before uploading.|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» workspaceId|string|true|none|Databricks workspace ID|
|»»» scope|string|true|none|OAuth scope for Unity Catalog authentication|
|»»» clientId|string|true|none|OAuth client ID for Unity Catalog authentication|
|»»» catalog|string|true|none|Name of the catalog to use for the output|
|»»» schema|string|true|none|Name of the catalog schema to use for the output|
|»»» eventsVolumeName|string|true|none|Name of the events volume in Databricks|
|»»» clientTextSecret|string|true|none|OAuth client secret for Unity Catalog authentication|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMicrosoftFabric](#schemaoutputmicrosoftfabric)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topic|string|true|none|Topic name from Fabric Eventstream's endpoint|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. This should always be $ConnectionString.|
|»»»» textSecret|string|false|none|Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» bootstrap_server|string|true|none|Bootstrap server from Fabric Eventstream's endpoint|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudflareR2](#schemaoutputcloudflarer2)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)|
|»»» bucket|string|true|none|Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|any|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|any|false|none|none|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|default|
|type|webhook|
|method|POST|
|method|PUT|
|method|PATCH|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|oauth|
|endpointURLConfiguration|url|
|endpointURLConfiguration|ID|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|devnull|
|type|syslog|
|protocol|tcp|
|protocol|udp|
|facility|0|
|facility|1|
|facility|2|
|facility|3|
|facility|4|
|facility|5|
|facility|6|
|facility|7|
|facility|8|
|facility|9|
|facility|10|
|facility|11|
|facility|12|
|facility|13|
|facility|14|
|facility|15|
|facility|16|
|facility|17|
|facility|18|
|facility|19|
|facility|20|
|facility|21|
|severity|0|
|severity|1|
|severity|2|
|severity|3|
|severity|4|
|severity|5|
|severity|6|
|severity|7|
|messageFormat|rfc3164|
|messageFormat|rfc5424|
|timestampFormat|syslog|
|timestampFormat|iso8601|
|tls|inherit|
|tls|off|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_lb|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|authType|manual|
|authType|secret|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_hec|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|tcpjson|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|wavefront|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|signalfx|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|filesystem|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_blob|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|storageClass|Inferred|
|storageClass|Hot|
|storageClass|Cool|
|storageClass|Cold|
|storageClass|Archive|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_data_explorer|
|ingestMode|batching|
|ingestMode|streaming|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|oauthType|clientSecret|
|oauthType|clientTextSecret|
|oauthType|certificate|
|format|json|
|format|raw|
|format|parquet|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|prefix|dropBy|
|prefix|ingestBy|
|reportLevel|failuresOnly|
|reportLevel|doNotReport|
|reportLevel|failuresAndSuccesses|
|reportMethod|queue|
|reportMethod|table|
|reportMethod|queueAndTable|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_logs|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|kinesis|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|compression|none|
|compression|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|honeycomb|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_eventhub|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_chronicle|
|apiVersion|v1|
|apiVersion|v2|
|authenticationMethod|manual|
|authenticationMethod|secret|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|logFormatType|unstructured|
|logFormatType|udm|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|udmType|entities|
|udmType|logs|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_cloud_storage|
|signatureVersion|v2|
|signatureVersion|v4|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|google_cloud_logging|
|logLocationType|project|
|logLocationType|organization|
|logLocationType|billingAccount|
|logLocationType|folder|
|payloadFormat|text|
|payloadFormat|json|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_pubsub|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|exabeam|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|type|kafka|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|confluent_cloud|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|msk|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|elasticVersion|auto|
|elasticVersion|6|
|elasticVersion|7|
|writeAction|index|
|writeAction|create|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic_cloud|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic|
|region|US|
|region|EU|
|region|Custom|
|name|service|
|name|hostname|
|name|timestamp|
|name|auditId|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic_events|
|region|US|
|region|EU|
|region|Custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|influxdb|
|timestampPrecision|ns|
|timestampPrecision|u|
|timestampPrecision|ms|
|timestampPrecision|s|
|timestampPrecision|m|
|timestampPrecision|h|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudwatch|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|minio|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|statsd|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|statsd_ext|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|graphite|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|router|
|type|sns|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sqs|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|snmp|
|type|sumo_logic|
|format|json|
|format|raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|datadog|
|contentType|text|
|contentType|json|
|severity|emergency|
|severity|alert|
|severity|critical|
|severity|error|
|severity|warning|
|severity|notice|
|severity|info|
|severity|debug|
|site|us|
|site|us3|
|site|us5|
|site|eu|
|site|fed1|
|site|ap1|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|grafana_cloud|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|loki|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|prometheus|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|ring|
|format|json|
|format|raw|
|compress|none|
|compress|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|type|open_telemetry|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|service_now|
|otlpVersion|1.3.1|
|protocol|grpc|
|protocol|http|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dataset|
|defaultSeverity|finest|
|defaultSeverity|finer|
|defaultSeverity|fine|
|defaultSeverity|info|
|defaultSeverity|warning|
|defaultSeverity|error|
|defaultSeverity|fatal|
|site|us|
|site|eu|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_tcp|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_http|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|compression|none|
|compression|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|humio_hec|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|crowdstrike_next_gen_siem|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dl_s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|security_lake|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|cribl_lake|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|auto_rpc|
|awsAuthenticationMethod|manual|
|format|json|
|format|parquet|
|format|ddss|
|type|disk_spool|
|compress|none|
|compress|gzip|
|type|click_house|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|sslUserCertificate|
|authType|token|
|authType|textSecret|
|authType|oauth|
|format|json-compact-each-row-with-names|
|format|json-each-row|
|mappingType|automatic|
|mappingType|custom|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|xsiam|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|token|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|netflow|
|type|dynatrace_http|
|method|POST|
|method|PUT|
|method|PATCH|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|token|
|authType|textSecret|
|format|json_array|
|format|plaintext|
|endpoint|cloud|
|endpoint|activeGate|
|endpoint|manual|
|telemetryType|logs|
|telemetryType|metrics|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dynatrace_otlp|
|protocol|http|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|endpointType|saas|
|endpointType|ag|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel_one_ai_siem|
|region|US|
|region|CA|
|region|EMEA|
|region|AP|
|region|APS|
|region|AU|
|region|Custom|
|endpoint|/services/collector/event|
|endpoint|/services/collector/raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|chronicle|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|databricks|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|microsoft_fabric|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudflare_r2|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|secret|
|awsAuthenticationMethod|manual|
|signatureVersion|v2|
|signatureVersion|v4|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<aside class="success">
This operation does not require authentication
</aside>

## Create a Destination

<a id="opIdcreateOutput"></a>

> Code samples

`POST /system/outputs`

Create a new Destination.

> Body parameter

```json
{
  "id": "string",
  "type": "default",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "defaultId": "string"
}
```

<h3 id="create-a-destination-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|body|body|[Output](#schemaoutput)|true|Output object|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "default",
      "pipeline": "string",
      "systemFields": [
        "cribl_pipe"
      ],
      "environment": "string",
      "streamtags": [],
      "defaultId": "string"
    }
  ]
}
```

<h3 id="create-a-destination-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="create-a-destination-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDefault](#schemaoutputdefault)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» defaultId|string|true|none|ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWebhook](#schemaoutputwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» format|string|false|none|How to format events before sending out|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Authentication method to use for the HTTP request|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» description|string|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» url|string|false|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinel](#schemaoutputsentinel)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» loginUrl|string|true|none|URL for OAuth|
|»»» secret|string|true|none|Secret parameter value to pass in request body|
|»»» client_id|string|true|none|JavaScript expression to compute the Client ID for the Azure application. Can be a constant.|
|»»» scope|string|false|none|Scope to pass in the OAuth request|
|»»» endpointURLConfiguration|string|true|none|Enter the data collection endpoint URL or the individual ID|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» format|any|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|
|»»» dcrID|string|false|none|Immutable ID for the Data Collection Rule (DCR)|
|»»» dceEndpoint|string|false|none|Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`|
|»»» streamName|string|false|none|The name of the stream (Sentinel table) in which to store the events|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDevnull](#schemaoutputdevnull)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSyslog](#schemaoutputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|The network protocol to use for sending out syslog messages|
|»»» facility|integer|false|none|Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.|
|»»» severity|integer|false|none|Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.|
|»»» appName|string|false|none|Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.|
|»»» messageFormat|string|false|none|The syslog message format depending on the receiver's support|
|»»» timestampFormat|string|false|none|Timestamp format to use when serializing event's time field|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» octetCountFraming|boolean|false|none|Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» description|string|false|none|none|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» maxRecordSize|number|false|none|Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.|
|»»» udpDnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.|
|»»» enableIpSpoofing|boolean|false|none|Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunk](#schemaoutputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» host|string|true|none|The hostname of the receiver|
|»»» port|number|true|none|The port to connect to on the provided host|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkLb](#schemaoutputsplunklb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» indexerDiscovery|boolean|false|none|Automatically discover indexers in indexer clustering environment.|
|»»» senderUnhealthyTimeAllowance|number|false|none|How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» indexerDiscoveryConfigs|object|false|none|List of configurations to set up indexer discovery in Splunk Indexer clustering environment.|
|»»»» site|string|true|none|Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.|
|»»»» masterUri|string|true|none|Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089|
|»»»» refreshIntervalSec|number|true|none|Time interval, in seconds, between two consecutive indexer list fetches from cluster manager|
|»»»» rejectUnauthorized|boolean|false|none|During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.|
|»»»» authTokens|[object]|false|none|Tokens required to authenticate to cluster manager for indexer discovery|
|»»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|true|none|Set of Splunk indexers to load-balance data to.|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkHec](#schemaoutputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» nextQueue|string|false|none|In the Splunk app, define which Splunk processing queue to send the events after HEC processing.|
|»»» tcpRouting|string|false|none|In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|Splunk HEC authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputTcpjson](#schemaoutputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» sendHeader|boolean|false|none|Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Optional authentication token to include as part of the connection header|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWavefront](#schemaoutputwavefront)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» domain|string|true|none|WaveFront domain name, e.g. "longboard"|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSignalfx](#schemaoutputsignalfx)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» realm|string|true|none|SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputFilesystem](#schemaoutputfilesystem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|true|none|Final destination for the output files|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputS3](#schemaoutputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureBlob](#schemaoutputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» containerName|string|true|none|The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» createContainer|boolean|false|none|Create the configured container in Azure Blob Storage if it does not already exist|
|»»» destPath|string|false|none|Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» authType|string|false|none|none|
|»»» storageClass|string|false|none|none|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureDataExplorer](#schemaoutputazuredataexplorer)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» clusterUrl|string|true|none|The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.|
|»»» database|string|true|none|Name of the database containing the table where data will be ingested|
|»»» table|string|true|none|Name of the table to ingest data into|
|»»» validateDatabaseSettings|boolean|false|none|When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.|
|»»» ingestMode|string|false|none|none|
|»»» oauthEndpoint|string|true|none|Endpoint used to acquire authentication tokens from Azure|
|»»» tenantId|string|true|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»» clientId|string|true|none|client_id to pass in the OAuth request parameter|
|»»» scope|string|true|none|Scope to pass in the OAuth request parameter|
|»»» oauthType|string|true|none|The type of OAuth 2.0 client credentials grant flow to use|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret that you generated for your app in the Azure portal|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|false|none|The certificate you registered as credentials for your app in the Azure portal|
|»»» format|string|false|none|Format of the output data|
|»»» compress|string|true|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» isMappingObj|boolean|false|none|Send a JSON mapping object instead of specifying an existing named data mapping|
|»»» mappingObj|string|false|none|Enter a JSON object that defines your desired data mapping|
|»»» mappingRef|string|false|none|Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.|
|»»» ingestUrl|string|false|none|The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushImmediately|boolean|false|none|Bypass the data management service's aggregation mechanism|
|»»» retainBlobOnSuccess|boolean|false|none|Prevent blob deletion after ingestion is complete|
|»»» extentTags|[object]|false|none|Strings or tags associated with the extent (ingested data shard)|
|»»»» prefix|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» ingestIfNotExists|[object]|false|none|Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists|
|»»»» value|string|true|none|none|
|»»» reportLevel|string|false|none|Level of ingestion status reporting. Defaults to FailuresOnly.|
|»»» reportMethod|string|false|none|Target of the ingestion status reporting. Defaults to Queue.|
|»»» additionalProperties|[object]|false|none|Optionally, enter additional configuration properties to send to the ingestion service|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureLogs](#schemaoutputazurelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logType|string|true|none|The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.|
|»»» resourceId|string|false|none|Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» apiUrl|string|false|none|The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter workspace ID and workspace key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» workspaceId|string|false|none|Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.|
|»»» workspaceKey|string|false|none|Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.|
|»»» keypairSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKinesis](#schemaoutputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» streamName|string|true|none|Kinesis stream name to send events to.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» concurrency|number|false|none|Maximum number of ongoing put requests before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» compression|string|false|none|Compression type to use for records|
|»»» useListShards|boolean|false|none|Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.|
|»»» asNdjson|boolean|false|none|Batch events into a single record as NDJSON|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» maxEventsPerFlush|number|false|none|Maximum number of records to send in a single request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHoneycomb](#schemaoutputhoneycomb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dataset|string|true|none|Name of the dataset to send events to – e.g., observability|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» team|string|false|none|Team API key where the dataset belongs|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureEventhub](#schemaoutputazureeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topic|string|true|none|The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleChronicle](#schemaoutputgooglechronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» logFormatType|string|true|none|none|
|»»» region|string|false|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» extraLogTypes|[object]|false|none|Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.|
|»»»» logType|string|true|none|none|
|»»»» description|string|false|none|none|
|»»» logType|string|false|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» customerId|string|false|none|A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every batch|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» udmType|string|false|none|Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.|
|»»» apiKey|string|false|none|Organization's API key in Google SecOps|
|»»» apiKeySecret|string|false|none|Select or create a stored text secret|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudStorage](#schemaoutputgooglecloudstorage)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» awsApiKey|string|false|none|HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudLogging](#schemaoutputgooglecloudlogging)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logLocationType|string|true|none|none|
|»»» logNameExpression|string|true|none|JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» sanitizeLogNames|boolean|false|none|none|
|»»» payloadFormat|string|false|none|Format to use when sending payload. Defaults to Text.|
|»»» logLabels|[object]|false|none|Labels to apply to the log entry|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» resourceTypeExpression|string|false|none|JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".|
|»»» resourceTypeLabels|[object]|false|none|Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» severityExpression|string|false|none|JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".|
|»»» insertIdExpression|string|false|none|JavaScript expression to compute the value of the insert ID field.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body.|
|»»» maxPayloadEvents|number|false|none|Max number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second.|
|»»» requestMethodExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestUrlExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» statusExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» responseSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» userAgentExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» remoteIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» serverIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» refererExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» latencyExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheLookupExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheHitExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheValidatedExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheFillBytesExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» protocolExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» idExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» producerExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» firstExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» lastExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» fileExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» lineExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» functionExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» uidExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» indexExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» totalSplitsExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» traceExpression|string|false|none|A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» spanIdExpression|string|false|none|A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» traceSampledExpression|string|false|none|A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» logLocationExpression|string|true|none|JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» payloadExpression|string|false|none|JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGooglePubsub](#schemaoutputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicName|string|true|none|ID of the topic to send events to.|
|»»» createTopic|boolean|false|none|If enabled, create topic if it does not exist.|
|»»» orderedDelivery|boolean|false|none|If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.|
|»»» region|string|false|none|Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» batchSize|number|false|none|The maximum number of items the Google API should batch before it sends them to the topic.|
|»»» batchTimeout|number|false|none|The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send.|
|»»» flushPeriod|number|false|none|Maximum time to wait before sending a batch (when batch size limit is not reached)|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputExabeam](#schemaoutputexabeam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» encodedConfiguration|string|false|none|Enter an encoded string containing Exabeam configurations|
|»»» collectorInstanceId|string|true|none|ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888|
|»»» siteName|string|false|none|Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.|
|»»» siteId|string|false|none|Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.|
|»»» timezoneOffset|string|false|none|none|
|»»» awsApiKey|string|false|none|HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKafka](#schemaoutputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputConfluentCloud](#schemaoutputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMsk](#schemaoutputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElastic](#schemaoutputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» index|string|true|none|Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» docType|string|false|none|Document type to use for events. Can be overwritten by an event's __type field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» extraParams|[object]|false|none|none|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticVersion|string|false|none|Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.|
|»»» elasticPipeline|string|false|none|Optional Elasticsearch destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» writeAction|string|false|none|Action to use when writing events. Must be set to `Create` when writing to a data stream.|
|»»» retryPartialErrors|boolean|false|none|Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElasticCloud](#schemaoutputelasticcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Enter Cloud ID of the Elastic Cloud environment to send events to|
|»»» index|string|true|none|Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» extraParams|[object]|false|none|Extra parameters to use in HTTP requests|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticPipeline|string|false|none|Optional Elastic Cloud Destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelic](#schemaoutputnewrelic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» logType|string|false|none|Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.|
|»»» messageField|string|false|none|Name of field to send as log message value. If not present, event will be serialized and sent as JSON.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelicEvents](#schemaoutputnewrelicevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» accountId|string|true|none|New Relic account ID|
|»»» eventType|string|true|none|Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputInfluxdb](#schemaoutputinfluxdb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write|
|»»» useV2API|boolean|false|none|The v2 API can be enabled with InfluxDB versions 1.8 and later.|
|»»» timestampPrecision|string|false|none|Sets the precision for the supplied Unix time values. Defaults to milliseconds.|
|»»» dynamicValueFieldName|boolean|false|none|Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.|
|»»» valueFieldName|string|false|none|Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|InfluxDB authentication type|
|»»» description|string|false|none|none|
|»»» database|string|false|none|Database to write to.|
|»»» bucket|string|false|none|Bucket to write to.|
|»»» org|string|false|none|Organization ID for this bucket.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudwatch](#schemaoutputcloudwatch)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logGroupName|string|true|none|CloudWatch log group to associate events with|
|»»» logStreamName|string|true|none|Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the CloudWatchLogs is located|
|»»» endpoint|string|false|none|CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access CloudWatchLogs|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMinio](#schemaoutputminio)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|MinIO service url (e.g. http://minioHost:9000)|
|»»» bucket|string|true|none|Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|string|false|none|Region where the MinIO service/cluster is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsd](#schemaoutputstatsd)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsdExt](#schemaoutputstatsdext)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGraphite](#schemaoutputgraphite)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRouter](#schemaoutputrouter)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» rules|[object]|true|none|Event routing rules|
|»»»» filter|string|true|none|JavaScript expression to select events to send to output|
|»»»» output|string|true|none|Output to send matching events to|
|»»»» description|string|false|none|Description of this rule's purpose|
|»»»» final|boolean|false|none|Flag to control whether to stop the event from being checked against other rules|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSns](#schemaoutputsns)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicArn|string|true|none|The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`|
|»»» messageGroupId|string|true|none|Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» maxRetries|number|false|none|Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the SNS is located|
|»»» endpoint|string|false|none|SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SNS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SNS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSqs](#schemaoutputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created). Defaults to Standard.|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» messageGroupId|string|false|none|This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSnmp](#schemaoutputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more SNMP destinations to forward traps to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 162|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSumoLogic](#schemaoutputsumologic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Sumo Logic HTTP collector URL to which events should be sent|
|»»» customSource|string|false|none|Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.|
|»»» customCategory|string|false|none|Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.|
|»»» format|string|false|none|Preserve the raw event format instead of JSONifying it|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatadog](#schemaoutputdatadog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» contentType|string|false|none|The content type to use when sending logs|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» source|string|false|none|Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.|
|»»» host|string|false|none|Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.|
|»»» service|string|false|none|Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.|
|»»» tags|[string]|false|none|List of tags to send with logs, such as 'env:prod' and 'env_staging:east'|
|»»» batchByTags|boolean|false|none|Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.|
|»»» allowApiKeyFromEvents|boolean|false|none|Allow API key to be set from the event's '__agent_api_key' field|
|»»» severity|string|false|none|Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.|
|»»» site|string|false|none|Datadog site to which events should be sent|
|»»» sendCountersAsCount|boolean|false|none|If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|Organization's API key in Datadog|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGrafanaCloud](#schemaoutputgrafanacloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» lokiUrl|string|false|none|The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net|
|»»» prometheusUrl|string|false|none|The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputLoki](#schemaoutputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send logs to|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» authType|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» enableDynamicHeaders|boolean|false|none|Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» username|string|false|none|Username for authentication|
|»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputPrometheus](#schemaoutputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send metrics to|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» sendMetadata|boolean|false|none|Generate and send metadata (`type` and `metricFamilyName`) requests|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» description|string|false|none|none|
|»»» metricsFlushPeriodSec|number|false|none|How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRing](#schemaoutputring)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» format|string|false|none|Format of the output data.|
|»»» partitionExpr|string|false|none|JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.|
|»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»» compress|string|false|none|none|
|»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputOpenTelemetry](#schemaoutputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|Select a transport option for OpenTelemetry|
|»»» endpoint|string|true|none|The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputServiceNow](#schemaoutputservicenow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» protocol|string|true|none|Select a transport option for OpenTelemetry|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDataset](#schemaoutputdataset)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» messageField|string|false|none|Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.|
|»»» serverHostField|string|false|none|Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.|
|»»» timestampField|string|false|none|Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.|
|»»» defaultSeverity|string|false|none|Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» site|string|false|none|DataSet site to which events should be sent|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|A 'Log Write Access' API key for the DataSet account|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblTcp](#schemaoutputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblHttp](#schemaoutputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHumioHec](#schemaoutputhumiohec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|CrowdStrike Falcon LogScale authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCrowdstrikeNextGenSiem](#schemaoutputcrowdstrikenextgensiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL provided from a CrowdStrike data connector. <br>Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|none|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDlS3](#schemaoutputdls3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» partitioningFields|[string]|false|none|List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSecurityLake](#schemaoutputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|true|none|Region where the Amazon Security Lake is located.|
|»»» awsSecretKey|string|false|none|none|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Amazon Security Lake requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|true|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» accountId|string|true|none|ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.|
|»»» customSource|string|true|none|Name of the custom source configured in Amazon Security Lake|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblLake](#schemaoutputcribllake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|false|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Lake dataset to send the data to.|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» format|string|false|none|none|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDiskSpool](#schemaoutputdiskspool)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputClickHouse](#schemaoutputclickhouse)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of the ClickHouse instance. Example: http://localhost:8123/|
|»»» authType|string|false|none|none|
|»»» database|string|true|none|none|
|»»» tableName|string|true|none|Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".|
|»»» format|string|false|none|Data format to use when sending data to ClickHouse. Defaults to JSON Compact.|
|»»» mappingType|string|false|none|How event fields are mapped to ClickHouse columns.|
|»»» asyncInserts|boolean|false|none|Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» dumpFormatErrorsToDisk|boolean|false|none|Log the most recent event that fails to match the table schema|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» sqlUsername|string|false|none|Username for certificate authentication|
|»»» waitForAsyncInserts|boolean|false|none|Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.|
|»»» excludeMappingFields|[string]|false|none|Fields to exclude from sending to ClickHouse|
|»»» describeTable|string|false|none|Retrieves the table schema from ClickHouse and populates the Column Mapping table|
|»»» columnMappings|[object]|false|none|none|
|»»»» columnName|string|true|none|Name of the column in ClickHouse that will store field value|
|»»»» columnType|string|false|none|Type of the column in the ClickHouse database|
|»»»» columnValueExpression|string|true|none|JavaScript expression to compute value to be inserted into ClickHouse table|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputXsiam](#schemaoutputxsiam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Enter a token directly, or provide a secret referencing a token|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» url|string|false|none|XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|any|true|none|none|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|XSIAM authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNetflow](#schemaoutputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more NetFlow destinations to forward events to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 2055|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceHttp](#schemaoutputdynatracehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» format|string|true|none|How to format events before sending. Defaults to JSON. Plaintext is not currently supported.|
|»»» endpoint|string|true|none|none|
|»»» telemetryType|string|true|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» environmentId|string|false|none|ID of the environment to send to|
|»»» activeGateDomain|string|false|none|ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceOtlp](#schemaoutputdynatraceotlp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Select a transport option for Dynatrace|
|»»» endpoint|string|true|none|The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» endpointType|string|true|none|Select the type of Dynatrace endpoint configured|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinelOneAiSiem](#schemaoutputsentineloneaisiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|true|none|The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.|
|»»» endpoint|string|true|none|Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» baseUrl|string|false|none|Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.|
|»»» hostExpression|string|false|none|Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').|
|»»» sourceExpression|string|false|none|Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').|
|»»» sourceTypeExpression|string|false|none|Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.|
|»»» dataSourceCategoryExpression|string|false|none|Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.|
|»»» dataSourceNameExpression|string|false|none|Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').|
|»»» dataSourceVendorExpression|string|false|none|Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').|
|»»» eventTypeExpression|string|false|none|Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').|
|»»» host|string|false|none|Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').|
|»»» source|string|false|none|Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.|
|»»» sourceType|string|false|none|Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.|
|»»» dataSourceCategory|string|false|none|Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.|
|»»» dataSourceName|string|false|none|Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.|
|»»» dataSourceVendor|string|false|none|Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.|
|»»» eventType|string|false|none|Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputChronicle](#schemaoutputchronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» region|string|true|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» ingestionMethod|string|false|none|none|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» logType|string|true|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» gcpProjectId|string|true|none|The Google Cloud Platform (GCP) project ID to send events to|
|»»» gcpInstance|string|true|none|The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every event|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»»» rbacEnabled|boolean|false|none|Designate this label for role-based access control and filtering|
|»»» description|string|false|none|none|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatabricks](#schemaoutputdatabricks)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|false|none|Optional path to prepend to files before uploading.|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» workspaceId|string|true|none|Databricks workspace ID|
|»»» scope|string|true|none|OAuth scope for Unity Catalog authentication|
|»»» clientId|string|true|none|OAuth client ID for Unity Catalog authentication|
|»»» catalog|string|true|none|Name of the catalog to use for the output|
|»»» schema|string|true|none|Name of the catalog schema to use for the output|
|»»» eventsVolumeName|string|true|none|Name of the events volume in Databricks|
|»»» clientTextSecret|string|true|none|OAuth client secret for Unity Catalog authentication|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMicrosoftFabric](#schemaoutputmicrosoftfabric)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topic|string|true|none|Topic name from Fabric Eventstream's endpoint|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. This should always be $ConnectionString.|
|»»»» textSecret|string|false|none|Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» bootstrap_server|string|true|none|Bootstrap server from Fabric Eventstream's endpoint|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudflareR2](#schemaoutputcloudflarer2)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)|
|»»» bucket|string|true|none|Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|any|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|any|false|none|none|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|default|
|type|webhook|
|method|POST|
|method|PUT|
|method|PATCH|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|oauth|
|endpointURLConfiguration|url|
|endpointURLConfiguration|ID|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|devnull|
|type|syslog|
|protocol|tcp|
|protocol|udp|
|facility|0|
|facility|1|
|facility|2|
|facility|3|
|facility|4|
|facility|5|
|facility|6|
|facility|7|
|facility|8|
|facility|9|
|facility|10|
|facility|11|
|facility|12|
|facility|13|
|facility|14|
|facility|15|
|facility|16|
|facility|17|
|facility|18|
|facility|19|
|facility|20|
|facility|21|
|severity|0|
|severity|1|
|severity|2|
|severity|3|
|severity|4|
|severity|5|
|severity|6|
|severity|7|
|messageFormat|rfc3164|
|messageFormat|rfc5424|
|timestampFormat|syslog|
|timestampFormat|iso8601|
|tls|inherit|
|tls|off|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_lb|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|authType|manual|
|authType|secret|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_hec|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|tcpjson|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|wavefront|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|signalfx|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|filesystem|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_blob|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|storageClass|Inferred|
|storageClass|Hot|
|storageClass|Cool|
|storageClass|Cold|
|storageClass|Archive|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_data_explorer|
|ingestMode|batching|
|ingestMode|streaming|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|oauthType|clientSecret|
|oauthType|clientTextSecret|
|oauthType|certificate|
|format|json|
|format|raw|
|format|parquet|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|prefix|dropBy|
|prefix|ingestBy|
|reportLevel|failuresOnly|
|reportLevel|doNotReport|
|reportLevel|failuresAndSuccesses|
|reportMethod|queue|
|reportMethod|table|
|reportMethod|queueAndTable|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_logs|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|kinesis|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|compression|none|
|compression|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|honeycomb|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_eventhub|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_chronicle|
|apiVersion|v1|
|apiVersion|v2|
|authenticationMethod|manual|
|authenticationMethod|secret|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|logFormatType|unstructured|
|logFormatType|udm|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|udmType|entities|
|udmType|logs|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_cloud_storage|
|signatureVersion|v2|
|signatureVersion|v4|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|google_cloud_logging|
|logLocationType|project|
|logLocationType|organization|
|logLocationType|billingAccount|
|logLocationType|folder|
|payloadFormat|text|
|payloadFormat|json|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_pubsub|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|exabeam|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|type|kafka|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|confluent_cloud|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|msk|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|elasticVersion|auto|
|elasticVersion|6|
|elasticVersion|7|
|writeAction|index|
|writeAction|create|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic_cloud|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic|
|region|US|
|region|EU|
|region|Custom|
|name|service|
|name|hostname|
|name|timestamp|
|name|auditId|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic_events|
|region|US|
|region|EU|
|region|Custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|influxdb|
|timestampPrecision|ns|
|timestampPrecision|u|
|timestampPrecision|ms|
|timestampPrecision|s|
|timestampPrecision|m|
|timestampPrecision|h|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudwatch|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|minio|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|statsd|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|statsd_ext|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|graphite|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|router|
|type|sns|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sqs|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|snmp|
|type|sumo_logic|
|format|json|
|format|raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|datadog|
|contentType|text|
|contentType|json|
|severity|emergency|
|severity|alert|
|severity|critical|
|severity|error|
|severity|warning|
|severity|notice|
|severity|info|
|severity|debug|
|site|us|
|site|us3|
|site|us5|
|site|eu|
|site|fed1|
|site|ap1|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|grafana_cloud|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|loki|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|prometheus|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|ring|
|format|json|
|format|raw|
|compress|none|
|compress|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|type|open_telemetry|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|service_now|
|otlpVersion|1.3.1|
|protocol|grpc|
|protocol|http|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dataset|
|defaultSeverity|finest|
|defaultSeverity|finer|
|defaultSeverity|fine|
|defaultSeverity|info|
|defaultSeverity|warning|
|defaultSeverity|error|
|defaultSeverity|fatal|
|site|us|
|site|eu|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_tcp|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_http|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|compression|none|
|compression|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|humio_hec|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|crowdstrike_next_gen_siem|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dl_s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|security_lake|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|cribl_lake|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|auto_rpc|
|awsAuthenticationMethod|manual|
|format|json|
|format|parquet|
|format|ddss|
|type|disk_spool|
|compress|none|
|compress|gzip|
|type|click_house|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|sslUserCertificate|
|authType|token|
|authType|textSecret|
|authType|oauth|
|format|json-compact-each-row-with-names|
|format|json-each-row|
|mappingType|automatic|
|mappingType|custom|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|xsiam|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|token|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|netflow|
|type|dynatrace_http|
|method|POST|
|method|PUT|
|method|PATCH|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|token|
|authType|textSecret|
|format|json_array|
|format|plaintext|
|endpoint|cloud|
|endpoint|activeGate|
|endpoint|manual|
|telemetryType|logs|
|telemetryType|metrics|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dynatrace_otlp|
|protocol|http|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|endpointType|saas|
|endpointType|ag|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel_one_ai_siem|
|region|US|
|region|CA|
|region|EMEA|
|region|AP|
|region|APS|
|region|AU|
|region|Custom|
|endpoint|/services/collector/event|
|endpoint|/services/collector/raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|chronicle|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|databricks|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|microsoft_fabric|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudflare_r2|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|secret|
|awsAuthenticationMethod|manual|
|signatureVersion|v2|
|signatureVersion|v4|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<aside class="success">
This operation does not require authentication
</aside>

## Delete a Destination

<a id="opIddeleteOutputById"></a>

> Code samples

`DELETE /system/outputs/{id}`

Delete the specified Destination.

<h3 id="delete-a-destination-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Destination to delete.|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "default",
      "pipeline": "string",
      "systemFields": [
        "cribl_pipe"
      ],
      "environment": "string",
      "streamtags": [],
      "defaultId": "string"
    }
  ]
}
```

<h3 id="delete-a-destination-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="delete-a-destination-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDefault](#schemaoutputdefault)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» defaultId|string|true|none|ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWebhook](#schemaoutputwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» format|string|false|none|How to format events before sending out|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Authentication method to use for the HTTP request|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» description|string|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» url|string|false|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinel](#schemaoutputsentinel)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» loginUrl|string|true|none|URL for OAuth|
|»»» secret|string|true|none|Secret parameter value to pass in request body|
|»»» client_id|string|true|none|JavaScript expression to compute the Client ID for the Azure application. Can be a constant.|
|»»» scope|string|false|none|Scope to pass in the OAuth request|
|»»» endpointURLConfiguration|string|true|none|Enter the data collection endpoint URL or the individual ID|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» format|any|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|
|»»» dcrID|string|false|none|Immutable ID for the Data Collection Rule (DCR)|
|»»» dceEndpoint|string|false|none|Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`|
|»»» streamName|string|false|none|The name of the stream (Sentinel table) in which to store the events|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDevnull](#schemaoutputdevnull)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSyslog](#schemaoutputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|The network protocol to use for sending out syslog messages|
|»»» facility|integer|false|none|Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.|
|»»» severity|integer|false|none|Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.|
|»»» appName|string|false|none|Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.|
|»»» messageFormat|string|false|none|The syslog message format depending on the receiver's support|
|»»» timestampFormat|string|false|none|Timestamp format to use when serializing event's time field|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» octetCountFraming|boolean|false|none|Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» description|string|false|none|none|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» maxRecordSize|number|false|none|Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.|
|»»» udpDnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.|
|»»» enableIpSpoofing|boolean|false|none|Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunk](#schemaoutputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» host|string|true|none|The hostname of the receiver|
|»»» port|number|true|none|The port to connect to on the provided host|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkLb](#schemaoutputsplunklb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» indexerDiscovery|boolean|false|none|Automatically discover indexers in indexer clustering environment.|
|»»» senderUnhealthyTimeAllowance|number|false|none|How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» indexerDiscoveryConfigs|object|false|none|List of configurations to set up indexer discovery in Splunk Indexer clustering environment.|
|»»»» site|string|true|none|Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.|
|»»»» masterUri|string|true|none|Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089|
|»»»» refreshIntervalSec|number|true|none|Time interval, in seconds, between two consecutive indexer list fetches from cluster manager|
|»»»» rejectUnauthorized|boolean|false|none|During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.|
|»»»» authTokens|[object]|false|none|Tokens required to authenticate to cluster manager for indexer discovery|
|»»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|true|none|Set of Splunk indexers to load-balance data to.|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkHec](#schemaoutputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» nextQueue|string|false|none|In the Splunk app, define which Splunk processing queue to send the events after HEC processing.|
|»»» tcpRouting|string|false|none|In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|Splunk HEC authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputTcpjson](#schemaoutputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» sendHeader|boolean|false|none|Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Optional authentication token to include as part of the connection header|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWavefront](#schemaoutputwavefront)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» domain|string|true|none|WaveFront domain name, e.g. "longboard"|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSignalfx](#schemaoutputsignalfx)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» realm|string|true|none|SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputFilesystem](#schemaoutputfilesystem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|true|none|Final destination for the output files|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputS3](#schemaoutputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureBlob](#schemaoutputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» containerName|string|true|none|The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» createContainer|boolean|false|none|Create the configured container in Azure Blob Storage if it does not already exist|
|»»» destPath|string|false|none|Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» authType|string|false|none|none|
|»»» storageClass|string|false|none|none|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureDataExplorer](#schemaoutputazuredataexplorer)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» clusterUrl|string|true|none|The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.|
|»»» database|string|true|none|Name of the database containing the table where data will be ingested|
|»»» table|string|true|none|Name of the table to ingest data into|
|»»» validateDatabaseSettings|boolean|false|none|When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.|
|»»» ingestMode|string|false|none|none|
|»»» oauthEndpoint|string|true|none|Endpoint used to acquire authentication tokens from Azure|
|»»» tenantId|string|true|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»» clientId|string|true|none|client_id to pass in the OAuth request parameter|
|»»» scope|string|true|none|Scope to pass in the OAuth request parameter|
|»»» oauthType|string|true|none|The type of OAuth 2.0 client credentials grant flow to use|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret that you generated for your app in the Azure portal|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|false|none|The certificate you registered as credentials for your app in the Azure portal|
|»»» format|string|false|none|Format of the output data|
|»»» compress|string|true|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» isMappingObj|boolean|false|none|Send a JSON mapping object instead of specifying an existing named data mapping|
|»»» mappingObj|string|false|none|Enter a JSON object that defines your desired data mapping|
|»»» mappingRef|string|false|none|Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.|
|»»» ingestUrl|string|false|none|The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushImmediately|boolean|false|none|Bypass the data management service's aggregation mechanism|
|»»» retainBlobOnSuccess|boolean|false|none|Prevent blob deletion after ingestion is complete|
|»»» extentTags|[object]|false|none|Strings or tags associated with the extent (ingested data shard)|
|»»»» prefix|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» ingestIfNotExists|[object]|false|none|Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists|
|»»»» value|string|true|none|none|
|»»» reportLevel|string|false|none|Level of ingestion status reporting. Defaults to FailuresOnly.|
|»»» reportMethod|string|false|none|Target of the ingestion status reporting. Defaults to Queue.|
|»»» additionalProperties|[object]|false|none|Optionally, enter additional configuration properties to send to the ingestion service|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureLogs](#schemaoutputazurelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logType|string|true|none|The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.|
|»»» resourceId|string|false|none|Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» apiUrl|string|false|none|The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter workspace ID and workspace key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» workspaceId|string|false|none|Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.|
|»»» workspaceKey|string|false|none|Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.|
|»»» keypairSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKinesis](#schemaoutputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» streamName|string|true|none|Kinesis stream name to send events to.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» concurrency|number|false|none|Maximum number of ongoing put requests before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» compression|string|false|none|Compression type to use for records|
|»»» useListShards|boolean|false|none|Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.|
|»»» asNdjson|boolean|false|none|Batch events into a single record as NDJSON|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» maxEventsPerFlush|number|false|none|Maximum number of records to send in a single request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHoneycomb](#schemaoutputhoneycomb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dataset|string|true|none|Name of the dataset to send events to – e.g., observability|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» team|string|false|none|Team API key where the dataset belongs|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureEventhub](#schemaoutputazureeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topic|string|true|none|The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleChronicle](#schemaoutputgooglechronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» logFormatType|string|true|none|none|
|»»» region|string|false|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» extraLogTypes|[object]|false|none|Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.|
|»»»» logType|string|true|none|none|
|»»»» description|string|false|none|none|
|»»» logType|string|false|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» customerId|string|false|none|A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every batch|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» udmType|string|false|none|Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.|
|»»» apiKey|string|false|none|Organization's API key in Google SecOps|
|»»» apiKeySecret|string|false|none|Select or create a stored text secret|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudStorage](#schemaoutputgooglecloudstorage)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» awsApiKey|string|false|none|HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudLogging](#schemaoutputgooglecloudlogging)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logLocationType|string|true|none|none|
|»»» logNameExpression|string|true|none|JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» sanitizeLogNames|boolean|false|none|none|
|»»» payloadFormat|string|false|none|Format to use when sending payload. Defaults to Text.|
|»»» logLabels|[object]|false|none|Labels to apply to the log entry|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» resourceTypeExpression|string|false|none|JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".|
|»»» resourceTypeLabels|[object]|false|none|Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» severityExpression|string|false|none|JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".|
|»»» insertIdExpression|string|false|none|JavaScript expression to compute the value of the insert ID field.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body.|
|»»» maxPayloadEvents|number|false|none|Max number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second.|
|»»» requestMethodExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestUrlExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» statusExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» responseSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» userAgentExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» remoteIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» serverIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» refererExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» latencyExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheLookupExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheHitExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheValidatedExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheFillBytesExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» protocolExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» idExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» producerExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» firstExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» lastExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» fileExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» lineExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» functionExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» uidExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» indexExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» totalSplitsExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» traceExpression|string|false|none|A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» spanIdExpression|string|false|none|A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» traceSampledExpression|string|false|none|A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» logLocationExpression|string|true|none|JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» payloadExpression|string|false|none|JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGooglePubsub](#schemaoutputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicName|string|true|none|ID of the topic to send events to.|
|»»» createTopic|boolean|false|none|If enabled, create topic if it does not exist.|
|»»» orderedDelivery|boolean|false|none|If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.|
|»»» region|string|false|none|Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» batchSize|number|false|none|The maximum number of items the Google API should batch before it sends them to the topic.|
|»»» batchTimeout|number|false|none|The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send.|
|»»» flushPeriod|number|false|none|Maximum time to wait before sending a batch (when batch size limit is not reached)|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputExabeam](#schemaoutputexabeam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» encodedConfiguration|string|false|none|Enter an encoded string containing Exabeam configurations|
|»»» collectorInstanceId|string|true|none|ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888|
|»»» siteName|string|false|none|Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.|
|»»» siteId|string|false|none|Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.|
|»»» timezoneOffset|string|false|none|none|
|»»» awsApiKey|string|false|none|HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKafka](#schemaoutputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputConfluentCloud](#schemaoutputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMsk](#schemaoutputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElastic](#schemaoutputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» index|string|true|none|Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» docType|string|false|none|Document type to use for events. Can be overwritten by an event's __type field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» extraParams|[object]|false|none|none|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticVersion|string|false|none|Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.|
|»»» elasticPipeline|string|false|none|Optional Elasticsearch destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» writeAction|string|false|none|Action to use when writing events. Must be set to `Create` when writing to a data stream.|
|»»» retryPartialErrors|boolean|false|none|Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElasticCloud](#schemaoutputelasticcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Enter Cloud ID of the Elastic Cloud environment to send events to|
|»»» index|string|true|none|Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» extraParams|[object]|false|none|Extra parameters to use in HTTP requests|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticPipeline|string|false|none|Optional Elastic Cloud Destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelic](#schemaoutputnewrelic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» logType|string|false|none|Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.|
|»»» messageField|string|false|none|Name of field to send as log message value. If not present, event will be serialized and sent as JSON.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelicEvents](#schemaoutputnewrelicevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» accountId|string|true|none|New Relic account ID|
|»»» eventType|string|true|none|Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputInfluxdb](#schemaoutputinfluxdb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write|
|»»» useV2API|boolean|false|none|The v2 API can be enabled with InfluxDB versions 1.8 and later.|
|»»» timestampPrecision|string|false|none|Sets the precision for the supplied Unix time values. Defaults to milliseconds.|
|»»» dynamicValueFieldName|boolean|false|none|Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.|
|»»» valueFieldName|string|false|none|Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|InfluxDB authentication type|
|»»» description|string|false|none|none|
|»»» database|string|false|none|Database to write to.|
|»»» bucket|string|false|none|Bucket to write to.|
|»»» org|string|false|none|Organization ID for this bucket.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudwatch](#schemaoutputcloudwatch)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logGroupName|string|true|none|CloudWatch log group to associate events with|
|»»» logStreamName|string|true|none|Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the CloudWatchLogs is located|
|»»» endpoint|string|false|none|CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access CloudWatchLogs|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMinio](#schemaoutputminio)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|MinIO service url (e.g. http://minioHost:9000)|
|»»» bucket|string|true|none|Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|string|false|none|Region where the MinIO service/cluster is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsd](#schemaoutputstatsd)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsdExt](#schemaoutputstatsdext)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGraphite](#schemaoutputgraphite)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRouter](#schemaoutputrouter)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» rules|[object]|true|none|Event routing rules|
|»»»» filter|string|true|none|JavaScript expression to select events to send to output|
|»»»» output|string|true|none|Output to send matching events to|
|»»»» description|string|false|none|Description of this rule's purpose|
|»»»» final|boolean|false|none|Flag to control whether to stop the event from being checked against other rules|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSns](#schemaoutputsns)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicArn|string|true|none|The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`|
|»»» messageGroupId|string|true|none|Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» maxRetries|number|false|none|Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the SNS is located|
|»»» endpoint|string|false|none|SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SNS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SNS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSqs](#schemaoutputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created). Defaults to Standard.|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» messageGroupId|string|false|none|This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSnmp](#schemaoutputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more SNMP destinations to forward traps to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 162|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSumoLogic](#schemaoutputsumologic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Sumo Logic HTTP collector URL to which events should be sent|
|»»» customSource|string|false|none|Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.|
|»»» customCategory|string|false|none|Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.|
|»»» format|string|false|none|Preserve the raw event format instead of JSONifying it|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatadog](#schemaoutputdatadog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» contentType|string|false|none|The content type to use when sending logs|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» source|string|false|none|Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.|
|»»» host|string|false|none|Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.|
|»»» service|string|false|none|Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.|
|»»» tags|[string]|false|none|List of tags to send with logs, such as 'env:prod' and 'env_staging:east'|
|»»» batchByTags|boolean|false|none|Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.|
|»»» allowApiKeyFromEvents|boolean|false|none|Allow API key to be set from the event's '__agent_api_key' field|
|»»» severity|string|false|none|Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.|
|»»» site|string|false|none|Datadog site to which events should be sent|
|»»» sendCountersAsCount|boolean|false|none|If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|Organization's API key in Datadog|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGrafanaCloud](#schemaoutputgrafanacloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» lokiUrl|string|false|none|The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net|
|»»» prometheusUrl|string|false|none|The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputLoki](#schemaoutputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send logs to|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» authType|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» enableDynamicHeaders|boolean|false|none|Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» username|string|false|none|Username for authentication|
|»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputPrometheus](#schemaoutputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send metrics to|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» sendMetadata|boolean|false|none|Generate and send metadata (`type` and `metricFamilyName`) requests|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» description|string|false|none|none|
|»»» metricsFlushPeriodSec|number|false|none|How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRing](#schemaoutputring)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» format|string|false|none|Format of the output data.|
|»»» partitionExpr|string|false|none|JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.|
|»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»» compress|string|false|none|none|
|»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputOpenTelemetry](#schemaoutputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|Select a transport option for OpenTelemetry|
|»»» endpoint|string|true|none|The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputServiceNow](#schemaoutputservicenow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» protocol|string|true|none|Select a transport option for OpenTelemetry|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDataset](#schemaoutputdataset)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» messageField|string|false|none|Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.|
|»»» serverHostField|string|false|none|Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.|
|»»» timestampField|string|false|none|Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.|
|»»» defaultSeverity|string|false|none|Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» site|string|false|none|DataSet site to which events should be sent|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|A 'Log Write Access' API key for the DataSet account|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblTcp](#schemaoutputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblHttp](#schemaoutputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHumioHec](#schemaoutputhumiohec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|CrowdStrike Falcon LogScale authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCrowdstrikeNextGenSiem](#schemaoutputcrowdstrikenextgensiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL provided from a CrowdStrike data connector. <br>Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|none|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDlS3](#schemaoutputdls3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» partitioningFields|[string]|false|none|List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSecurityLake](#schemaoutputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|true|none|Region where the Amazon Security Lake is located.|
|»»» awsSecretKey|string|false|none|none|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Amazon Security Lake requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|true|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» accountId|string|true|none|ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.|
|»»» customSource|string|true|none|Name of the custom source configured in Amazon Security Lake|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblLake](#schemaoutputcribllake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|false|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Lake dataset to send the data to.|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» format|string|false|none|none|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDiskSpool](#schemaoutputdiskspool)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputClickHouse](#schemaoutputclickhouse)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of the ClickHouse instance. Example: http://localhost:8123/|
|»»» authType|string|false|none|none|
|»»» database|string|true|none|none|
|»»» tableName|string|true|none|Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".|
|»»» format|string|false|none|Data format to use when sending data to ClickHouse. Defaults to JSON Compact.|
|»»» mappingType|string|false|none|How event fields are mapped to ClickHouse columns.|
|»»» asyncInserts|boolean|false|none|Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» dumpFormatErrorsToDisk|boolean|false|none|Log the most recent event that fails to match the table schema|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» sqlUsername|string|false|none|Username for certificate authentication|
|»»» waitForAsyncInserts|boolean|false|none|Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.|
|»»» excludeMappingFields|[string]|false|none|Fields to exclude from sending to ClickHouse|
|»»» describeTable|string|false|none|Retrieves the table schema from ClickHouse and populates the Column Mapping table|
|»»» columnMappings|[object]|false|none|none|
|»»»» columnName|string|true|none|Name of the column in ClickHouse that will store field value|
|»»»» columnType|string|false|none|Type of the column in the ClickHouse database|
|»»»» columnValueExpression|string|true|none|JavaScript expression to compute value to be inserted into ClickHouse table|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputXsiam](#schemaoutputxsiam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Enter a token directly, or provide a secret referencing a token|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» url|string|false|none|XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|any|true|none|none|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|XSIAM authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNetflow](#schemaoutputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more NetFlow destinations to forward events to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 2055|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceHttp](#schemaoutputdynatracehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» format|string|true|none|How to format events before sending. Defaults to JSON. Plaintext is not currently supported.|
|»»» endpoint|string|true|none|none|
|»»» telemetryType|string|true|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» environmentId|string|false|none|ID of the environment to send to|
|»»» activeGateDomain|string|false|none|ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceOtlp](#schemaoutputdynatraceotlp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Select a transport option for Dynatrace|
|»»» endpoint|string|true|none|The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» endpointType|string|true|none|Select the type of Dynatrace endpoint configured|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinelOneAiSiem](#schemaoutputsentineloneaisiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|true|none|The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.|
|»»» endpoint|string|true|none|Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» baseUrl|string|false|none|Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.|
|»»» hostExpression|string|false|none|Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').|
|»»» sourceExpression|string|false|none|Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').|
|»»» sourceTypeExpression|string|false|none|Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.|
|»»» dataSourceCategoryExpression|string|false|none|Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.|
|»»» dataSourceNameExpression|string|false|none|Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').|
|»»» dataSourceVendorExpression|string|false|none|Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').|
|»»» eventTypeExpression|string|false|none|Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').|
|»»» host|string|false|none|Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').|
|»»» source|string|false|none|Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.|
|»»» sourceType|string|false|none|Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.|
|»»» dataSourceCategory|string|false|none|Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.|
|»»» dataSourceName|string|false|none|Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.|
|»»» dataSourceVendor|string|false|none|Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.|
|»»» eventType|string|false|none|Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputChronicle](#schemaoutputchronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» region|string|true|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» ingestionMethod|string|false|none|none|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» logType|string|true|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» gcpProjectId|string|true|none|The Google Cloud Platform (GCP) project ID to send events to|
|»»» gcpInstance|string|true|none|The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every event|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»»» rbacEnabled|boolean|false|none|Designate this label for role-based access control and filtering|
|»»» description|string|false|none|none|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatabricks](#schemaoutputdatabricks)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|false|none|Optional path to prepend to files before uploading.|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» workspaceId|string|true|none|Databricks workspace ID|
|»»» scope|string|true|none|OAuth scope for Unity Catalog authentication|
|»»» clientId|string|true|none|OAuth client ID for Unity Catalog authentication|
|»»» catalog|string|true|none|Name of the catalog to use for the output|
|»»» schema|string|true|none|Name of the catalog schema to use for the output|
|»»» eventsVolumeName|string|true|none|Name of the events volume in Databricks|
|»»» clientTextSecret|string|true|none|OAuth client secret for Unity Catalog authentication|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMicrosoftFabric](#schemaoutputmicrosoftfabric)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topic|string|true|none|Topic name from Fabric Eventstream's endpoint|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. This should always be $ConnectionString.|
|»»»» textSecret|string|false|none|Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» bootstrap_server|string|true|none|Bootstrap server from Fabric Eventstream's endpoint|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudflareR2](#schemaoutputcloudflarer2)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)|
|»»» bucket|string|true|none|Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|any|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|any|false|none|none|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|default|
|type|webhook|
|method|POST|
|method|PUT|
|method|PATCH|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|oauth|
|endpointURLConfiguration|url|
|endpointURLConfiguration|ID|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|devnull|
|type|syslog|
|protocol|tcp|
|protocol|udp|
|facility|0|
|facility|1|
|facility|2|
|facility|3|
|facility|4|
|facility|5|
|facility|6|
|facility|7|
|facility|8|
|facility|9|
|facility|10|
|facility|11|
|facility|12|
|facility|13|
|facility|14|
|facility|15|
|facility|16|
|facility|17|
|facility|18|
|facility|19|
|facility|20|
|facility|21|
|severity|0|
|severity|1|
|severity|2|
|severity|3|
|severity|4|
|severity|5|
|severity|6|
|severity|7|
|messageFormat|rfc3164|
|messageFormat|rfc5424|
|timestampFormat|syslog|
|timestampFormat|iso8601|
|tls|inherit|
|tls|off|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_lb|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|authType|manual|
|authType|secret|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_hec|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|tcpjson|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|wavefront|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|signalfx|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|filesystem|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_blob|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|storageClass|Inferred|
|storageClass|Hot|
|storageClass|Cool|
|storageClass|Cold|
|storageClass|Archive|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_data_explorer|
|ingestMode|batching|
|ingestMode|streaming|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|oauthType|clientSecret|
|oauthType|clientTextSecret|
|oauthType|certificate|
|format|json|
|format|raw|
|format|parquet|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|prefix|dropBy|
|prefix|ingestBy|
|reportLevel|failuresOnly|
|reportLevel|doNotReport|
|reportLevel|failuresAndSuccesses|
|reportMethod|queue|
|reportMethod|table|
|reportMethod|queueAndTable|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_logs|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|kinesis|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|compression|none|
|compression|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|honeycomb|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_eventhub|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_chronicle|
|apiVersion|v1|
|apiVersion|v2|
|authenticationMethod|manual|
|authenticationMethod|secret|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|logFormatType|unstructured|
|logFormatType|udm|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|udmType|entities|
|udmType|logs|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_cloud_storage|
|signatureVersion|v2|
|signatureVersion|v4|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|google_cloud_logging|
|logLocationType|project|
|logLocationType|organization|
|logLocationType|billingAccount|
|logLocationType|folder|
|payloadFormat|text|
|payloadFormat|json|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_pubsub|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|exabeam|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|type|kafka|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|confluent_cloud|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|msk|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|elasticVersion|auto|
|elasticVersion|6|
|elasticVersion|7|
|writeAction|index|
|writeAction|create|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic_cloud|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic|
|region|US|
|region|EU|
|region|Custom|
|name|service|
|name|hostname|
|name|timestamp|
|name|auditId|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic_events|
|region|US|
|region|EU|
|region|Custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|influxdb|
|timestampPrecision|ns|
|timestampPrecision|u|
|timestampPrecision|ms|
|timestampPrecision|s|
|timestampPrecision|m|
|timestampPrecision|h|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudwatch|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|minio|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|statsd|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|statsd_ext|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|graphite|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|router|
|type|sns|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sqs|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|snmp|
|type|sumo_logic|
|format|json|
|format|raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|datadog|
|contentType|text|
|contentType|json|
|severity|emergency|
|severity|alert|
|severity|critical|
|severity|error|
|severity|warning|
|severity|notice|
|severity|info|
|severity|debug|
|site|us|
|site|us3|
|site|us5|
|site|eu|
|site|fed1|
|site|ap1|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|grafana_cloud|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|loki|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|prometheus|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|ring|
|format|json|
|format|raw|
|compress|none|
|compress|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|type|open_telemetry|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|service_now|
|otlpVersion|1.3.1|
|protocol|grpc|
|protocol|http|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dataset|
|defaultSeverity|finest|
|defaultSeverity|finer|
|defaultSeverity|fine|
|defaultSeverity|info|
|defaultSeverity|warning|
|defaultSeverity|error|
|defaultSeverity|fatal|
|site|us|
|site|eu|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_tcp|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_http|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|compression|none|
|compression|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|humio_hec|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|crowdstrike_next_gen_siem|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dl_s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|security_lake|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|cribl_lake|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|auto_rpc|
|awsAuthenticationMethod|manual|
|format|json|
|format|parquet|
|format|ddss|
|type|disk_spool|
|compress|none|
|compress|gzip|
|type|click_house|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|sslUserCertificate|
|authType|token|
|authType|textSecret|
|authType|oauth|
|format|json-compact-each-row-with-names|
|format|json-each-row|
|mappingType|automatic|
|mappingType|custom|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|xsiam|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|token|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|netflow|
|type|dynatrace_http|
|method|POST|
|method|PUT|
|method|PATCH|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|token|
|authType|textSecret|
|format|json_array|
|format|plaintext|
|endpoint|cloud|
|endpoint|activeGate|
|endpoint|manual|
|telemetryType|logs|
|telemetryType|metrics|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dynatrace_otlp|
|protocol|http|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|endpointType|saas|
|endpointType|ag|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel_one_ai_siem|
|region|US|
|region|CA|
|region|EMEA|
|region|AP|
|region|APS|
|region|AU|
|region|Custom|
|endpoint|/services/collector/event|
|endpoint|/services/collector/raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|chronicle|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|databricks|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|microsoft_fabric|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudflare_r2|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|secret|
|awsAuthenticationMethod|manual|
|signatureVersion|v2|
|signatureVersion|v4|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<aside class="success">
This operation does not require authentication
</aside>

## Get a Destination

<a id="opIdgetOutputById"></a>

> Code samples

`GET /system/outputs/{id}`

Get the specified Destination.

<h3 id="get-a-destination-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Destination to get.|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "default",
      "pipeline": "string",
      "systemFields": [
        "cribl_pipe"
      ],
      "environment": "string",
      "streamtags": [],
      "defaultId": "string"
    }
  ]
}
```

<h3 id="get-a-destination-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-a-destination-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDefault](#schemaoutputdefault)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» defaultId|string|true|none|ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWebhook](#schemaoutputwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» format|string|false|none|How to format events before sending out|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Authentication method to use for the HTTP request|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» description|string|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» url|string|false|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinel](#schemaoutputsentinel)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» loginUrl|string|true|none|URL for OAuth|
|»»» secret|string|true|none|Secret parameter value to pass in request body|
|»»» client_id|string|true|none|JavaScript expression to compute the Client ID for the Azure application. Can be a constant.|
|»»» scope|string|false|none|Scope to pass in the OAuth request|
|»»» endpointURLConfiguration|string|true|none|Enter the data collection endpoint URL or the individual ID|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» format|any|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|
|»»» dcrID|string|false|none|Immutable ID for the Data Collection Rule (DCR)|
|»»» dceEndpoint|string|false|none|Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`|
|»»» streamName|string|false|none|The name of the stream (Sentinel table) in which to store the events|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDevnull](#schemaoutputdevnull)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSyslog](#schemaoutputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|The network protocol to use for sending out syslog messages|
|»»» facility|integer|false|none|Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.|
|»»» severity|integer|false|none|Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.|
|»»» appName|string|false|none|Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.|
|»»» messageFormat|string|false|none|The syslog message format depending on the receiver's support|
|»»» timestampFormat|string|false|none|Timestamp format to use when serializing event's time field|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» octetCountFraming|boolean|false|none|Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» description|string|false|none|none|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» maxRecordSize|number|false|none|Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.|
|»»» udpDnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.|
|»»» enableIpSpoofing|boolean|false|none|Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunk](#schemaoutputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» host|string|true|none|The hostname of the receiver|
|»»» port|number|true|none|The port to connect to on the provided host|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkLb](#schemaoutputsplunklb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» indexerDiscovery|boolean|false|none|Automatically discover indexers in indexer clustering environment.|
|»»» senderUnhealthyTimeAllowance|number|false|none|How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» indexerDiscoveryConfigs|object|false|none|List of configurations to set up indexer discovery in Splunk Indexer clustering environment.|
|»»»» site|string|true|none|Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.|
|»»»» masterUri|string|true|none|Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089|
|»»»» refreshIntervalSec|number|true|none|Time interval, in seconds, between two consecutive indexer list fetches from cluster manager|
|»»»» rejectUnauthorized|boolean|false|none|During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.|
|»»»» authTokens|[object]|false|none|Tokens required to authenticate to cluster manager for indexer discovery|
|»»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|true|none|Set of Splunk indexers to load-balance data to.|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkHec](#schemaoutputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» nextQueue|string|false|none|In the Splunk app, define which Splunk processing queue to send the events after HEC processing.|
|»»» tcpRouting|string|false|none|In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|Splunk HEC authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputTcpjson](#schemaoutputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» sendHeader|boolean|false|none|Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Optional authentication token to include as part of the connection header|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWavefront](#schemaoutputwavefront)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» domain|string|true|none|WaveFront domain name, e.g. "longboard"|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSignalfx](#schemaoutputsignalfx)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» realm|string|true|none|SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputFilesystem](#schemaoutputfilesystem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|true|none|Final destination for the output files|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputS3](#schemaoutputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureBlob](#schemaoutputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» containerName|string|true|none|The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» createContainer|boolean|false|none|Create the configured container in Azure Blob Storage if it does not already exist|
|»»» destPath|string|false|none|Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» authType|string|false|none|none|
|»»» storageClass|string|false|none|none|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureDataExplorer](#schemaoutputazuredataexplorer)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» clusterUrl|string|true|none|The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.|
|»»» database|string|true|none|Name of the database containing the table where data will be ingested|
|»»» table|string|true|none|Name of the table to ingest data into|
|»»» validateDatabaseSettings|boolean|false|none|When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.|
|»»» ingestMode|string|false|none|none|
|»»» oauthEndpoint|string|true|none|Endpoint used to acquire authentication tokens from Azure|
|»»» tenantId|string|true|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»» clientId|string|true|none|client_id to pass in the OAuth request parameter|
|»»» scope|string|true|none|Scope to pass in the OAuth request parameter|
|»»» oauthType|string|true|none|The type of OAuth 2.0 client credentials grant flow to use|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret that you generated for your app in the Azure portal|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|false|none|The certificate you registered as credentials for your app in the Azure portal|
|»»» format|string|false|none|Format of the output data|
|»»» compress|string|true|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» isMappingObj|boolean|false|none|Send a JSON mapping object instead of specifying an existing named data mapping|
|»»» mappingObj|string|false|none|Enter a JSON object that defines your desired data mapping|
|»»» mappingRef|string|false|none|Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.|
|»»» ingestUrl|string|false|none|The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushImmediately|boolean|false|none|Bypass the data management service's aggregation mechanism|
|»»» retainBlobOnSuccess|boolean|false|none|Prevent blob deletion after ingestion is complete|
|»»» extentTags|[object]|false|none|Strings or tags associated with the extent (ingested data shard)|
|»»»» prefix|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» ingestIfNotExists|[object]|false|none|Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists|
|»»»» value|string|true|none|none|
|»»» reportLevel|string|false|none|Level of ingestion status reporting. Defaults to FailuresOnly.|
|»»» reportMethod|string|false|none|Target of the ingestion status reporting. Defaults to Queue.|
|»»» additionalProperties|[object]|false|none|Optionally, enter additional configuration properties to send to the ingestion service|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureLogs](#schemaoutputazurelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logType|string|true|none|The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.|
|»»» resourceId|string|false|none|Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» apiUrl|string|false|none|The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter workspace ID and workspace key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» workspaceId|string|false|none|Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.|
|»»» workspaceKey|string|false|none|Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.|
|»»» keypairSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKinesis](#schemaoutputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» streamName|string|true|none|Kinesis stream name to send events to.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» concurrency|number|false|none|Maximum number of ongoing put requests before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» compression|string|false|none|Compression type to use for records|
|»»» useListShards|boolean|false|none|Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.|
|»»» asNdjson|boolean|false|none|Batch events into a single record as NDJSON|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» maxEventsPerFlush|number|false|none|Maximum number of records to send in a single request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHoneycomb](#schemaoutputhoneycomb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dataset|string|true|none|Name of the dataset to send events to – e.g., observability|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» team|string|false|none|Team API key where the dataset belongs|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureEventhub](#schemaoutputazureeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topic|string|true|none|The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleChronicle](#schemaoutputgooglechronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» logFormatType|string|true|none|none|
|»»» region|string|false|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» extraLogTypes|[object]|false|none|Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.|
|»»»» logType|string|true|none|none|
|»»»» description|string|false|none|none|
|»»» logType|string|false|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» customerId|string|false|none|A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every batch|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» udmType|string|false|none|Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.|
|»»» apiKey|string|false|none|Organization's API key in Google SecOps|
|»»» apiKeySecret|string|false|none|Select or create a stored text secret|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudStorage](#schemaoutputgooglecloudstorage)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» awsApiKey|string|false|none|HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudLogging](#schemaoutputgooglecloudlogging)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logLocationType|string|true|none|none|
|»»» logNameExpression|string|true|none|JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» sanitizeLogNames|boolean|false|none|none|
|»»» payloadFormat|string|false|none|Format to use when sending payload. Defaults to Text.|
|»»» logLabels|[object]|false|none|Labels to apply to the log entry|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» resourceTypeExpression|string|false|none|JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".|
|»»» resourceTypeLabels|[object]|false|none|Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» severityExpression|string|false|none|JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".|
|»»» insertIdExpression|string|false|none|JavaScript expression to compute the value of the insert ID field.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body.|
|»»» maxPayloadEvents|number|false|none|Max number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second.|
|»»» requestMethodExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestUrlExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» statusExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» responseSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» userAgentExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» remoteIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» serverIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» refererExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» latencyExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheLookupExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheHitExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheValidatedExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheFillBytesExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» protocolExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» idExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» producerExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» firstExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» lastExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» fileExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» lineExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» functionExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» uidExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» indexExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» totalSplitsExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» traceExpression|string|false|none|A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» spanIdExpression|string|false|none|A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» traceSampledExpression|string|false|none|A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» logLocationExpression|string|true|none|JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» payloadExpression|string|false|none|JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGooglePubsub](#schemaoutputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicName|string|true|none|ID of the topic to send events to.|
|»»» createTopic|boolean|false|none|If enabled, create topic if it does not exist.|
|»»» orderedDelivery|boolean|false|none|If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.|
|»»» region|string|false|none|Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» batchSize|number|false|none|The maximum number of items the Google API should batch before it sends them to the topic.|
|»»» batchTimeout|number|false|none|The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send.|
|»»» flushPeriod|number|false|none|Maximum time to wait before sending a batch (when batch size limit is not reached)|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputExabeam](#schemaoutputexabeam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» encodedConfiguration|string|false|none|Enter an encoded string containing Exabeam configurations|
|»»» collectorInstanceId|string|true|none|ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888|
|»»» siteName|string|false|none|Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.|
|»»» siteId|string|false|none|Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.|
|»»» timezoneOffset|string|false|none|none|
|»»» awsApiKey|string|false|none|HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKafka](#schemaoutputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputConfluentCloud](#schemaoutputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMsk](#schemaoutputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElastic](#schemaoutputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» index|string|true|none|Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» docType|string|false|none|Document type to use for events. Can be overwritten by an event's __type field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» extraParams|[object]|false|none|none|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticVersion|string|false|none|Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.|
|»»» elasticPipeline|string|false|none|Optional Elasticsearch destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» writeAction|string|false|none|Action to use when writing events. Must be set to `Create` when writing to a data stream.|
|»»» retryPartialErrors|boolean|false|none|Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElasticCloud](#schemaoutputelasticcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Enter Cloud ID of the Elastic Cloud environment to send events to|
|»»» index|string|true|none|Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» extraParams|[object]|false|none|Extra parameters to use in HTTP requests|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticPipeline|string|false|none|Optional Elastic Cloud Destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelic](#schemaoutputnewrelic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» logType|string|false|none|Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.|
|»»» messageField|string|false|none|Name of field to send as log message value. If not present, event will be serialized and sent as JSON.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelicEvents](#schemaoutputnewrelicevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» accountId|string|true|none|New Relic account ID|
|»»» eventType|string|true|none|Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputInfluxdb](#schemaoutputinfluxdb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write|
|»»» useV2API|boolean|false|none|The v2 API can be enabled with InfluxDB versions 1.8 and later.|
|»»» timestampPrecision|string|false|none|Sets the precision for the supplied Unix time values. Defaults to milliseconds.|
|»»» dynamicValueFieldName|boolean|false|none|Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.|
|»»» valueFieldName|string|false|none|Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|InfluxDB authentication type|
|»»» description|string|false|none|none|
|»»» database|string|false|none|Database to write to.|
|»»» bucket|string|false|none|Bucket to write to.|
|»»» org|string|false|none|Organization ID for this bucket.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudwatch](#schemaoutputcloudwatch)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logGroupName|string|true|none|CloudWatch log group to associate events with|
|»»» logStreamName|string|true|none|Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the CloudWatchLogs is located|
|»»» endpoint|string|false|none|CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access CloudWatchLogs|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMinio](#schemaoutputminio)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|MinIO service url (e.g. http://minioHost:9000)|
|»»» bucket|string|true|none|Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|string|false|none|Region where the MinIO service/cluster is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsd](#schemaoutputstatsd)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsdExt](#schemaoutputstatsdext)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGraphite](#schemaoutputgraphite)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRouter](#schemaoutputrouter)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» rules|[object]|true|none|Event routing rules|
|»»»» filter|string|true|none|JavaScript expression to select events to send to output|
|»»»» output|string|true|none|Output to send matching events to|
|»»»» description|string|false|none|Description of this rule's purpose|
|»»»» final|boolean|false|none|Flag to control whether to stop the event from being checked against other rules|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSns](#schemaoutputsns)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicArn|string|true|none|The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`|
|»»» messageGroupId|string|true|none|Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» maxRetries|number|false|none|Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the SNS is located|
|»»» endpoint|string|false|none|SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SNS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SNS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSqs](#schemaoutputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created). Defaults to Standard.|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» messageGroupId|string|false|none|This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSnmp](#schemaoutputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more SNMP destinations to forward traps to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 162|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSumoLogic](#schemaoutputsumologic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Sumo Logic HTTP collector URL to which events should be sent|
|»»» customSource|string|false|none|Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.|
|»»» customCategory|string|false|none|Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.|
|»»» format|string|false|none|Preserve the raw event format instead of JSONifying it|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatadog](#schemaoutputdatadog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» contentType|string|false|none|The content type to use when sending logs|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» source|string|false|none|Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.|
|»»» host|string|false|none|Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.|
|»»» service|string|false|none|Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.|
|»»» tags|[string]|false|none|List of tags to send with logs, such as 'env:prod' and 'env_staging:east'|
|»»» batchByTags|boolean|false|none|Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.|
|»»» allowApiKeyFromEvents|boolean|false|none|Allow API key to be set from the event's '__agent_api_key' field|
|»»» severity|string|false|none|Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.|
|»»» site|string|false|none|Datadog site to which events should be sent|
|»»» sendCountersAsCount|boolean|false|none|If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|Organization's API key in Datadog|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGrafanaCloud](#schemaoutputgrafanacloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» lokiUrl|string|false|none|The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net|
|»»» prometheusUrl|string|false|none|The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputLoki](#schemaoutputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send logs to|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» authType|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» enableDynamicHeaders|boolean|false|none|Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» username|string|false|none|Username for authentication|
|»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputPrometheus](#schemaoutputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send metrics to|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» sendMetadata|boolean|false|none|Generate and send metadata (`type` and `metricFamilyName`) requests|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» description|string|false|none|none|
|»»» metricsFlushPeriodSec|number|false|none|How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRing](#schemaoutputring)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» format|string|false|none|Format of the output data.|
|»»» partitionExpr|string|false|none|JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.|
|»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»» compress|string|false|none|none|
|»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputOpenTelemetry](#schemaoutputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|Select a transport option for OpenTelemetry|
|»»» endpoint|string|true|none|The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputServiceNow](#schemaoutputservicenow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» protocol|string|true|none|Select a transport option for OpenTelemetry|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDataset](#schemaoutputdataset)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» messageField|string|false|none|Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.|
|»»» serverHostField|string|false|none|Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.|
|»»» timestampField|string|false|none|Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.|
|»»» defaultSeverity|string|false|none|Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» site|string|false|none|DataSet site to which events should be sent|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|A 'Log Write Access' API key for the DataSet account|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblTcp](#schemaoutputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblHttp](#schemaoutputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHumioHec](#schemaoutputhumiohec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|CrowdStrike Falcon LogScale authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCrowdstrikeNextGenSiem](#schemaoutputcrowdstrikenextgensiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL provided from a CrowdStrike data connector. <br>Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|none|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDlS3](#schemaoutputdls3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» partitioningFields|[string]|false|none|List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSecurityLake](#schemaoutputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|true|none|Region where the Amazon Security Lake is located.|
|»»» awsSecretKey|string|false|none|none|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Amazon Security Lake requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|true|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» accountId|string|true|none|ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.|
|»»» customSource|string|true|none|Name of the custom source configured in Amazon Security Lake|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblLake](#schemaoutputcribllake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|false|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Lake dataset to send the data to.|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» format|string|false|none|none|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDiskSpool](#schemaoutputdiskspool)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputClickHouse](#schemaoutputclickhouse)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of the ClickHouse instance. Example: http://localhost:8123/|
|»»» authType|string|false|none|none|
|»»» database|string|true|none|none|
|»»» tableName|string|true|none|Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".|
|»»» format|string|false|none|Data format to use when sending data to ClickHouse. Defaults to JSON Compact.|
|»»» mappingType|string|false|none|How event fields are mapped to ClickHouse columns.|
|»»» asyncInserts|boolean|false|none|Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» dumpFormatErrorsToDisk|boolean|false|none|Log the most recent event that fails to match the table schema|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» sqlUsername|string|false|none|Username for certificate authentication|
|»»» waitForAsyncInserts|boolean|false|none|Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.|
|»»» excludeMappingFields|[string]|false|none|Fields to exclude from sending to ClickHouse|
|»»» describeTable|string|false|none|Retrieves the table schema from ClickHouse and populates the Column Mapping table|
|»»» columnMappings|[object]|false|none|none|
|»»»» columnName|string|true|none|Name of the column in ClickHouse that will store field value|
|»»»» columnType|string|false|none|Type of the column in the ClickHouse database|
|»»»» columnValueExpression|string|true|none|JavaScript expression to compute value to be inserted into ClickHouse table|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputXsiam](#schemaoutputxsiam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Enter a token directly, or provide a secret referencing a token|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» url|string|false|none|XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|any|true|none|none|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|XSIAM authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNetflow](#schemaoutputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more NetFlow destinations to forward events to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 2055|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceHttp](#schemaoutputdynatracehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» format|string|true|none|How to format events before sending. Defaults to JSON. Plaintext is not currently supported.|
|»»» endpoint|string|true|none|none|
|»»» telemetryType|string|true|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» environmentId|string|false|none|ID of the environment to send to|
|»»» activeGateDomain|string|false|none|ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceOtlp](#schemaoutputdynatraceotlp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Select a transport option for Dynatrace|
|»»» endpoint|string|true|none|The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» endpointType|string|true|none|Select the type of Dynatrace endpoint configured|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinelOneAiSiem](#schemaoutputsentineloneaisiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|true|none|The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.|
|»»» endpoint|string|true|none|Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» baseUrl|string|false|none|Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.|
|»»» hostExpression|string|false|none|Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').|
|»»» sourceExpression|string|false|none|Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').|
|»»» sourceTypeExpression|string|false|none|Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.|
|»»» dataSourceCategoryExpression|string|false|none|Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.|
|»»» dataSourceNameExpression|string|false|none|Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').|
|»»» dataSourceVendorExpression|string|false|none|Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').|
|»»» eventTypeExpression|string|false|none|Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').|
|»»» host|string|false|none|Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').|
|»»» source|string|false|none|Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.|
|»»» sourceType|string|false|none|Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.|
|»»» dataSourceCategory|string|false|none|Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.|
|»»» dataSourceName|string|false|none|Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.|
|»»» dataSourceVendor|string|false|none|Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.|
|»»» eventType|string|false|none|Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputChronicle](#schemaoutputchronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» region|string|true|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» ingestionMethod|string|false|none|none|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» logType|string|true|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» gcpProjectId|string|true|none|The Google Cloud Platform (GCP) project ID to send events to|
|»»» gcpInstance|string|true|none|The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every event|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»»» rbacEnabled|boolean|false|none|Designate this label for role-based access control and filtering|
|»»» description|string|false|none|none|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatabricks](#schemaoutputdatabricks)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|false|none|Optional path to prepend to files before uploading.|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» workspaceId|string|true|none|Databricks workspace ID|
|»»» scope|string|true|none|OAuth scope for Unity Catalog authentication|
|»»» clientId|string|true|none|OAuth client ID for Unity Catalog authentication|
|»»» catalog|string|true|none|Name of the catalog to use for the output|
|»»» schema|string|true|none|Name of the catalog schema to use for the output|
|»»» eventsVolumeName|string|true|none|Name of the events volume in Databricks|
|»»» clientTextSecret|string|true|none|OAuth client secret for Unity Catalog authentication|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMicrosoftFabric](#schemaoutputmicrosoftfabric)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topic|string|true|none|Topic name from Fabric Eventstream's endpoint|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. This should always be $ConnectionString.|
|»»»» textSecret|string|false|none|Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» bootstrap_server|string|true|none|Bootstrap server from Fabric Eventstream's endpoint|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudflareR2](#schemaoutputcloudflarer2)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)|
|»»» bucket|string|true|none|Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|any|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|any|false|none|none|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|default|
|type|webhook|
|method|POST|
|method|PUT|
|method|PATCH|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|oauth|
|endpointURLConfiguration|url|
|endpointURLConfiguration|ID|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|devnull|
|type|syslog|
|protocol|tcp|
|protocol|udp|
|facility|0|
|facility|1|
|facility|2|
|facility|3|
|facility|4|
|facility|5|
|facility|6|
|facility|7|
|facility|8|
|facility|9|
|facility|10|
|facility|11|
|facility|12|
|facility|13|
|facility|14|
|facility|15|
|facility|16|
|facility|17|
|facility|18|
|facility|19|
|facility|20|
|facility|21|
|severity|0|
|severity|1|
|severity|2|
|severity|3|
|severity|4|
|severity|5|
|severity|6|
|severity|7|
|messageFormat|rfc3164|
|messageFormat|rfc5424|
|timestampFormat|syslog|
|timestampFormat|iso8601|
|tls|inherit|
|tls|off|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_lb|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|authType|manual|
|authType|secret|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_hec|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|tcpjson|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|wavefront|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|signalfx|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|filesystem|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_blob|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|storageClass|Inferred|
|storageClass|Hot|
|storageClass|Cool|
|storageClass|Cold|
|storageClass|Archive|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_data_explorer|
|ingestMode|batching|
|ingestMode|streaming|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|oauthType|clientSecret|
|oauthType|clientTextSecret|
|oauthType|certificate|
|format|json|
|format|raw|
|format|parquet|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|prefix|dropBy|
|prefix|ingestBy|
|reportLevel|failuresOnly|
|reportLevel|doNotReport|
|reportLevel|failuresAndSuccesses|
|reportMethod|queue|
|reportMethod|table|
|reportMethod|queueAndTable|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_logs|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|kinesis|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|compression|none|
|compression|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|honeycomb|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_eventhub|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_chronicle|
|apiVersion|v1|
|apiVersion|v2|
|authenticationMethod|manual|
|authenticationMethod|secret|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|logFormatType|unstructured|
|logFormatType|udm|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|udmType|entities|
|udmType|logs|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_cloud_storage|
|signatureVersion|v2|
|signatureVersion|v4|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|google_cloud_logging|
|logLocationType|project|
|logLocationType|organization|
|logLocationType|billingAccount|
|logLocationType|folder|
|payloadFormat|text|
|payloadFormat|json|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_pubsub|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|exabeam|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|type|kafka|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|confluent_cloud|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|msk|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|elasticVersion|auto|
|elasticVersion|6|
|elasticVersion|7|
|writeAction|index|
|writeAction|create|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic_cloud|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic|
|region|US|
|region|EU|
|region|Custom|
|name|service|
|name|hostname|
|name|timestamp|
|name|auditId|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic_events|
|region|US|
|region|EU|
|region|Custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|influxdb|
|timestampPrecision|ns|
|timestampPrecision|u|
|timestampPrecision|ms|
|timestampPrecision|s|
|timestampPrecision|m|
|timestampPrecision|h|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudwatch|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|minio|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|statsd|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|statsd_ext|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|graphite|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|router|
|type|sns|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sqs|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|snmp|
|type|sumo_logic|
|format|json|
|format|raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|datadog|
|contentType|text|
|contentType|json|
|severity|emergency|
|severity|alert|
|severity|critical|
|severity|error|
|severity|warning|
|severity|notice|
|severity|info|
|severity|debug|
|site|us|
|site|us3|
|site|us5|
|site|eu|
|site|fed1|
|site|ap1|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|grafana_cloud|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|loki|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|prometheus|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|ring|
|format|json|
|format|raw|
|compress|none|
|compress|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|type|open_telemetry|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|service_now|
|otlpVersion|1.3.1|
|protocol|grpc|
|protocol|http|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dataset|
|defaultSeverity|finest|
|defaultSeverity|finer|
|defaultSeverity|fine|
|defaultSeverity|info|
|defaultSeverity|warning|
|defaultSeverity|error|
|defaultSeverity|fatal|
|site|us|
|site|eu|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_tcp|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_http|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|compression|none|
|compression|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|humio_hec|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|crowdstrike_next_gen_siem|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dl_s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|security_lake|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|cribl_lake|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|auto_rpc|
|awsAuthenticationMethod|manual|
|format|json|
|format|parquet|
|format|ddss|
|type|disk_spool|
|compress|none|
|compress|gzip|
|type|click_house|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|sslUserCertificate|
|authType|token|
|authType|textSecret|
|authType|oauth|
|format|json-compact-each-row-with-names|
|format|json-each-row|
|mappingType|automatic|
|mappingType|custom|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|xsiam|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|token|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|netflow|
|type|dynatrace_http|
|method|POST|
|method|PUT|
|method|PATCH|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|token|
|authType|textSecret|
|format|json_array|
|format|plaintext|
|endpoint|cloud|
|endpoint|activeGate|
|endpoint|manual|
|telemetryType|logs|
|telemetryType|metrics|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dynatrace_otlp|
|protocol|http|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|endpointType|saas|
|endpointType|ag|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel_one_ai_siem|
|region|US|
|region|CA|
|region|EMEA|
|region|AP|
|region|APS|
|region|AU|
|region|Custom|
|endpoint|/services/collector/event|
|endpoint|/services/collector/raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|chronicle|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|databricks|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|microsoft_fabric|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudflare_r2|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|secret|
|awsAuthenticationMethod|manual|
|signatureVersion|v2|
|signatureVersion|v4|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<aside class="success">
This operation does not require authentication
</aside>

## Update a Destination

<a id="opIdupdateOutputById"></a>

> Code samples

`PATCH /system/outputs/{id}`

Update the specified Destination.</br></br>Provide a complete representation of the Destination that you want to update in the request body. This endpoint does not support partial updates. Cribl removes any omitted fields when updating the Destination.</br></br>Confirm that the configuration in your request body is correct before sending the request. If the configuration is incorrect, the updated Destination might not function as expected.

> Body parameter

```json
{
  "id": "string",
  "type": "default",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "defaultId": "string"
}
```

<h3 id="update-a-destination-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Destination to update.|
|body|body|[Output](#schemaoutput)|true|Output object|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "default",
      "pipeline": "string",
      "systemFields": [
        "cribl_pipe"
      ],
      "environment": "string",
      "streamtags": [],
      "defaultId": "string"
    }
  ]
}
```

<h3 id="update-a-destination-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="update-a-destination-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDefault](#schemaoutputdefault)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» defaultId|string|true|none|ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWebhook](#schemaoutputwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» format|string|false|none|How to format events before sending out|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Authentication method to use for the HTTP request|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» description|string|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» url|string|false|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinel](#schemaoutputsentinel)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» loginUrl|string|true|none|URL for OAuth|
|»»» secret|string|true|none|Secret parameter value to pass in request body|
|»»» client_id|string|true|none|JavaScript expression to compute the Client ID for the Azure application. Can be a constant.|
|»»» scope|string|false|none|Scope to pass in the OAuth request|
|»»» endpointURLConfiguration|string|true|none|Enter the data collection endpoint URL or the individual ID|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» format|any|false|none|none|
|»»» customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|»»» customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|»»» customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|»»» customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|»»» customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|»»» advancedContentType|string|false|none|HTTP content-type header value|
|»»» formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|
|»»» dcrID|string|false|none|Immutable ID for the Data Collection Rule (DCR)|
|»»» dceEndpoint|string|false|none|Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`|
|»»» streamName|string|false|none|The name of the stream (Sentinel table) in which to store the events|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDevnull](#schemaoutputdevnull)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSyslog](#schemaoutputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|The network protocol to use for sending out syslog messages|
|»»» facility|integer|false|none|Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.|
|»»» severity|integer|false|none|Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.|
|»»» appName|string|false|none|Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.|
|»»» messageFormat|string|false|none|The syslog message format depending on the receiver's support|
|»»» timestampFormat|string|false|none|Timestamp format to use when serializing event's time field|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» octetCountFraming|boolean|false|none|Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» description|string|false|none|none|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» maxRecordSize|number|false|none|Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.|
|»»» udpDnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.|
|»»» enableIpSpoofing|boolean|false|none|Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunk](#schemaoutputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» host|string|true|none|The hostname of the receiver|
|»»» port|number|true|none|The port to connect to on the provided host|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkLb](#schemaoutputsplunklb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|»»» enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» indexerDiscovery|boolean|false|none|Automatically discover indexers in indexer clustering environment.|
|»»» senderUnhealthyTimeAllowance|number|false|none|How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|»»» compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|»»» indexerDiscoveryConfigs|object|false|none|List of configurations to set up indexer discovery in Splunk Indexer clustering environment.|
|»»»» site|string|true|none|Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.|
|»»»» masterUri|string|true|none|Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089|
|»»»» refreshIntervalSec|number|true|none|Time interval, in seconds, between two consecutive indexer list fetches from cluster manager|
|»»»» rejectUnauthorized|boolean|false|none|During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.|
|»»»» authTokens|[object]|false|none|Tokens required to authenticate to cluster manager for indexer discovery|
|»»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|true|none|Set of Splunk indexers to load-balance data to.|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSplunkHec](#schemaoutputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» nextQueue|string|false|none|In the Splunk app, define which Splunk processing queue to send the events after HEC processing.|
|»»» tcpRouting|string|false|none|In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|Splunk HEC authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputTcpjson](#schemaoutputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» sendHeader|boolean|false|none|Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» authToken|string|false|none|Optional authentication token to include as part of the connection header|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputWavefront](#schemaoutputwavefront)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» domain|string|true|none|WaveFront domain name, e.g. "longboard"|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSignalfx](#schemaoutputsignalfx)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» realm|string|true|none|SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputFilesystem](#schemaoutputfilesystem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|true|none|Final destination for the output files|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputS3](#schemaoutputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureBlob](#schemaoutputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» containerName|string|true|none|The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» createContainer|boolean|false|none|Create the configured container in Azure Blob Storage if it does not already exist|
|»»» destPath|string|false|none|Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» authType|string|false|none|none|
|»»» storageClass|string|false|none|none|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureDataExplorer](#schemaoutputazuredataexplorer)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» clusterUrl|string|true|none|The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.|
|»»» database|string|true|none|Name of the database containing the table where data will be ingested|
|»»» table|string|true|none|Name of the table to ingest data into|
|»»» validateDatabaseSettings|boolean|false|none|When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.|
|»»» ingestMode|string|false|none|none|
|»»» oauthEndpoint|string|true|none|Endpoint used to acquire authentication tokens from Azure|
|»»» tenantId|string|true|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»» clientId|string|true|none|client_id to pass in the OAuth request parameter|
|»»» scope|string|true|none|Scope to pass in the OAuth request parameter|
|»»» oauthType|string|true|none|The type of OAuth 2.0 client credentials grant flow to use|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret that you generated for your app in the Azure portal|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|false|none|The certificate you registered as credentials for your app in the Azure portal|
|»»» format|string|false|none|Format of the output data|
|»»» compress|string|true|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» isMappingObj|boolean|false|none|Send a JSON mapping object instead of specifying an existing named data mapping|
|»»» mappingObj|string|false|none|Enter a JSON object that defines your desired data mapping|
|»»» mappingRef|string|false|none|Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.|
|»»» ingestUrl|string|false|none|The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushImmediately|boolean|false|none|Bypass the data management service's aggregation mechanism|
|»»» retainBlobOnSuccess|boolean|false|none|Prevent blob deletion after ingestion is complete|
|»»» extentTags|[object]|false|none|Strings or tags associated with the extent (ingested data shard)|
|»»»» prefix|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» ingestIfNotExists|[object]|false|none|Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists|
|»»»» value|string|true|none|none|
|»»» reportLevel|string|false|none|Level of ingestion status reporting. Defaults to FailuresOnly.|
|»»» reportMethod|string|false|none|Target of the ingestion status reporting. Defaults to Queue.|
|»»» additionalProperties|[object]|false|none|Optionally, enter additional configuration properties to send to the ingestion service|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureLogs](#schemaoutputazurelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logType|string|true|none|The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.|
|»»» resourceId|string|false|none|Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» apiUrl|string|false|none|The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter workspace ID and workspace key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» workspaceId|string|false|none|Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.|
|»»» workspaceKey|string|false|none|Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.|
|»»» keypairSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKinesis](#schemaoutputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» streamName|string|true|none|Kinesis stream name to send events to.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» concurrency|number|false|none|Maximum number of ongoing put requests before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» compression|string|false|none|Compression type to use for records|
|»»» useListShards|boolean|false|none|Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.|
|»»» asNdjson|boolean|false|none|Batch events into a single record as NDJSON|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» maxEventsPerFlush|number|false|none|Maximum number of records to send in a single request|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHoneycomb](#schemaoutputhoneycomb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» dataset|string|true|none|Name of the dataset to send events to – e.g., observability|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» team|string|false|none|Team API key where the dataset belongs|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputAzureEventhub](#schemaoutputazureeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topic|string|true|none|The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleChronicle](#schemaoutputgooglechronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» logFormatType|string|true|none|none|
|»»» region|string|false|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» extraLogTypes|[object]|false|none|Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.|
|»»»» logType|string|true|none|none|
|»»»» description|string|false|none|none|
|»»» logType|string|false|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» customerId|string|false|none|A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every batch|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» udmType|string|false|none|Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.|
|»»» apiKey|string|false|none|Organization's API key in Google SecOps|
|»»» apiKeySecret|string|false|none|Select or create a stored text secret|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudStorage](#schemaoutputgooglecloudstorage)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|»»» awsApiKey|string|false|none|HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGoogleCloudLogging](#schemaoutputgooglecloudlogging)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logLocationType|string|true|none|none|
|»»» logNameExpression|string|true|none|JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» sanitizeLogNames|boolean|false|none|none|
|»»» payloadFormat|string|false|none|Format to use when sending payload. Defaults to Text.|
|»»» logLabels|[object]|false|none|Labels to apply to the log entry|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» resourceTypeExpression|string|false|none|JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".|
|»»» resourceTypeLabels|[object]|false|none|Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.|
|»»»» label|string|true|none|Label name|
|»»»» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|»»» severityExpression|string|false|none|JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".|
|»»» insertIdExpression|string|false|none|JavaScript expression to compute the value of the insert ID field.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body.|
|»»» maxPayloadEvents|number|false|none|Max number of events to include in the request body. Default is 0 (unlimited).|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second.|
|»»» requestMethodExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestUrlExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» requestSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» statusExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» responseSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» userAgentExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» remoteIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» serverIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» refererExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» latencyExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheLookupExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheHitExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheValidatedExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» cacheFillBytesExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» protocolExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|»»» idExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» producerExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» firstExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» lastExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|»»» fileExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» lineExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» functionExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|»»» uidExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» indexExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» totalSplitsExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|»»» traceExpression|string|false|none|A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» spanIdExpression|string|false|none|A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» traceSampledExpression|string|false|none|A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» logLocationExpression|string|true|none|JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|»»» payloadExpression|string|false|none|JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGooglePubsub](#schemaoutputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicName|string|true|none|ID of the topic to send events to.|
|»»» createTopic|boolean|false|none|If enabled, create topic if it does not exist.|
|»»» orderedDelivery|boolean|false|none|If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.|
|»»» region|string|false|none|Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» batchSize|number|false|none|The maximum number of items the Google API should batch before it sends them to the topic.|
|»»» batchTimeout|number|false|none|The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send.|
|»»» flushPeriod|number|false|none|Maximum time to wait before sending a batch (when batch size limit is not reached)|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputExabeam](#schemaoutputexabeam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.|
|»»» region|string|true|none|Region where the bucket is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» endpoint|string|true|none|Google Cloud Storage service endpoint|
|»»» signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» encodedConfiguration|string|false|none|Enter an encoded string containing Exabeam configurations|
|»»» collectorInstanceId|string|true|none|ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888|
|»»» siteName|string|false|none|Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.|
|»»» siteId|string|false|none|Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.|
|»»» timezoneOffset|string|false|none|none|
|»»» awsApiKey|string|false|none|HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|»»» awsSecretKey|string|false|none|HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputKafka](#schemaoutputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputConfluentCloud](#schemaoutputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMsk](#schemaoutputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|»»» topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|»»» ack|integer|false|none|Control the number of required acknowledgments.|
|»»» format|string|false|none|Format to use to serialize events before writing to Kafka.|
|»»» compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|»»» flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»»» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|»»»» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|»»» protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElastic](#schemaoutputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» index|string|true|none|Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» docType|string|false|none|Document type to use for events. Can be overwritten by an event's __type field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» extraParams|[object]|false|none|none|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticVersion|string|false|none|Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.|
|»»» elasticPipeline|string|false|none|Optional Elasticsearch destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» writeAction|string|false|none|Action to use when writing events. Must be set to `Create` when writing to a data stream.|
|»»» retryPartialErrors|boolean|false|none|Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputElasticCloud](#schemaoutputelasticcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Enter Cloud ID of the Elastic Cloud environment to send events to|
|»»» index|string|true|none|Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» extraParams|[object]|false|none|Extra parameters to use in HTTP requests|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» auth|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» manualAPIKey|string|false|none|Enter API key directly|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» elasticPipeline|string|false|none|Optional Elastic Cloud Destination pipeline|
|»»» includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelic](#schemaoutputnewrelic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» logType|string|false|none|Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.|
|»»» messageField|string|false|none|Name of field to send as log message value. If not present, event will be serialized and sent as JSON.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNewrelicEvents](#schemaoutputnewrelicevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|false|none|Which New Relic region endpoint to use.|
|»»» accountId|string|true|none|New Relic account ID|
|»»» eventType|string|true|none|Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputInfluxdb](#schemaoutputinfluxdb)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write|
|»»» useV2API|boolean|false|none|The v2 API can be enabled with InfluxDB versions 1.8 and later.|
|»»» timestampPrecision|string|false|none|Sets the precision for the supplied Unix time values. Defaults to milliseconds.|
|»»» dynamicValueFieldName|boolean|false|none|Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.|
|»»» valueFieldName|string|false|none|Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|InfluxDB authentication type|
|»»» description|string|false|none|none|
|»»» database|string|false|none|Database to write to.|
|»»» bucket|string|false|none|Bucket to write to.|
|»»» org|string|false|none|Organization ID for this bucket.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudwatch](#schemaoutputcloudwatch)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» logGroupName|string|true|none|CloudWatch log group to associate events with|
|»»» logStreamName|string|true|none|Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the CloudWatchLogs is located|
|»»» endpoint|string|false|none|CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access CloudWatchLogs|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMinio](#schemaoutputminio)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|MinIO service url (e.g. http://minioHost:9000)|
|»»» bucket|string|true|none|Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|string|false|none|Region where the MinIO service/cluster is located|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsd](#schemaoutputstatsd)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputStatsdExt](#schemaoutputstatsdext)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGraphite](#schemaoutputgraphite)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Protocol to use when communicating with the destination.|
|»»» host|string|true|none|The hostname of the destination.|
|»»» port|number|true|none|Destination port.|
|»»» mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|»»» flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|»»» description|string|false|none|none|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRouter](#schemaoutputrouter)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» rules|[object]|true|none|Event routing rules|
|»»»» filter|string|true|none|JavaScript expression to select events to send to output|
|»»»» output|string|true|none|Output to send matching events to|
|»»»» description|string|false|none|Description of this rule's purpose|
|»»»» final|boolean|false|none|Flag to control whether to stop the event from being checked against other rules|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSns](#schemaoutputsns)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topicArn|string|true|none|The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`|
|»»» messageGroupId|string|true|none|Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» maxRetries|number|false|none|Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the SNS is located|
|»»» endpoint|string|false|none|SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SNS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SNS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSqs](#schemaoutputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created). Defaults to Standard.|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» messageGroupId|string|false|none|This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|»»» maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSnmp](#schemaoutputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more SNMP destinations to forward traps to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 162|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSumoLogic](#schemaoutputsumologic)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|Sumo Logic HTTP collector URL to which events should be sent|
|»»» customSource|string|false|none|Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.|
|»»» customCategory|string|false|none|Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.|
|»»» format|string|false|none|Preserve the raw event format instead of JSONifying it|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatadog](#schemaoutputdatadog)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» contentType|string|false|none|The content type to use when sending logs|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» source|string|false|none|Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.|
|»»» host|string|false|none|Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.|
|»»» service|string|false|none|Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.|
|»»» tags|[string]|false|none|List of tags to send with logs, such as 'env:prod' and 'env_staging:east'|
|»»» batchByTags|boolean|false|none|Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.|
|»»» allowApiKeyFromEvents|boolean|false|none|Allow API key to be set from the event's '__agent_api_key' field|
|»»» severity|string|false|none|Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.|
|»»» site|string|false|none|Datadog site to which events should be sent|
|»»» sendCountersAsCount|boolean|false|none|If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|Organization's API key in Datadog|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputGrafanaCloud](#schemaoutputgrafanacloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» lokiUrl|string|false|none|The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net|
|»»» prometheusUrl|string|false|none|The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» username|string|false|none|Username for authentication|
|»»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputLoki](#schemaoutputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send logs to|
|»»» message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|»»» labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» authType|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» enableDynamicHeaders|boolean|false|none|Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» username|string|false|none|Username for authentication|
|»»» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputPrometheus](#schemaoutputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|The endpoint to send metrics to|
|»»» metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|»»» sendMetadata|boolean|false|none|Generate and send metadata (`type` and `metricFamilyName`) requests|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» description|string|false|none|none|
|»»» metricsFlushPeriodSec|number|false|none|How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputRing](#schemaoutputring)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» format|string|false|none|Format of the output data.|
|»»» partitionExpr|string|false|none|JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.|
|»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»» compress|string|false|none|none|
|»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputOpenTelemetry](#schemaoutputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|false|none|Select a transport option for OpenTelemetry|
|»»» endpoint|string|true|none|The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputServiceNow](#schemaoutputservicenow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» protocol|string|true|none|Select a transport option for OpenTelemetry|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDataset](#schemaoutputdataset)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» messageField|string|false|none|Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.|
|»»» serverHostField|string|false|none|Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.|
|»»» timestampField|string|false|none|Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.|
|»»» defaultSeverity|string|false|none|Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» site|string|false|none|DataSet site to which events should be sent|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|Enter API key directly, or select a stored secret|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» customUrl|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» apiKey|string|false|none|A 'Log Write Access' API key for the DataSet account|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblTcp](#schemaoutputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Use load-balanced destinations|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» host|string|false|none|The hostname of the receiver|
|»»» port|number|false|none|The port to connect to on the provided host|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» hosts|[object]|false|none|Set of hosts to load-balance data to|
|»»»» host|string|true|none|The hostname of the receiver|
|»»»» port|number|true|none|The port to connect to on the provided host|
|»»»» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|»»»» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblHttp](#schemaoutputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.|
|»»» excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|»»» compression|string|false|none|Codec to use to compress the data before sending|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» url|string|false|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|string|true|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputHumioHec](#schemaoutputhumiohec)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|CrowdStrike Falcon LogScale authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCrowdstrikeNextGenSiem](#schemaoutputcrowdstrikenextgensiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL provided from a CrowdStrike data connector. <br>Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|none|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDlS3](#schemaoutputdls3)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» partitioningFields|[string]|false|none|List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSecurityLake](#schemaoutputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|true|none|Region where the Amazon Security Lake is located.|
|»»» awsSecretKey|string|false|none|none|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» endpoint|string|false|none|Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Amazon Security Lake requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|true|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» accountId|string|true|none|ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.|
|»»» customSource|string|true|none|Name of the custom source configured in Amazon Security Lake|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCriblLake](#schemaoutputcribllake)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» bucket|string|false|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» region|string|false|none|Region where the S3 bucket is located|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Lake dataset to send the data to.|
|»»» objectACL|string|false|none|Object ACL to assign to uploaded objects|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|none|
|»»» kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|»»» awsAuthenticationMethod|string|false|none|none|
|»»» format|string|false|none|none|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDiskSpool](#schemaoutputdiskspool)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputClickHouse](#schemaoutputclickhouse)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» url|string|true|none|URL of the ClickHouse instance. Example: http://localhost:8123/|
|»»» authType|string|false|none|none|
|»»» database|string|true|none|none|
|»»» tableName|string|true|none|Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".|
|»»» format|string|false|none|Data format to use when sending data to ClickHouse. Defaults to JSON Compact.|
|»»» mappingType|string|false|none|How event fields are mapped to ClickHouse columns.|
|»»» asyncInserts|boolean|false|none|Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» dumpFormatErrorsToDisk|boolean|false|none|Log the most recent event that fails to match the table schema|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» sqlUsername|string|false|none|Username for certificate authentication|
|»»» waitForAsyncInserts|boolean|false|none|Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.|
|»»» excludeMappingFields|[string]|false|none|Fields to exclude from sending to ClickHouse|
|»»» describeTable|string|false|none|Retrieves the table schema from ClickHouse and populates the Column Mapping table|
|»»» columnMappings|[object]|false|none|none|
|»»»» columnName|string|true|none|Name of the column in ClickHouse that will store field value|
|»»»» columnType|string|false|none|Type of the column in the ClickHouse database|
|»»»» columnValueExpression|string|true|none|JavaScript expression to compute value to be inserted into ClickHouse table|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputXsiam](#schemaoutputxsiam)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Enter a token directly, or provide a secret referencing a token|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» url|string|false|none|XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|»»» urls|[object]|false|none|none|
|»»»» url|any|true|none|none|
|»»»» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|»»» dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|»»» loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|»»» token|string|false|none|XSIAM authentication token|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputNetflow](#schemaoutputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» hosts|[object]|true|none|One or more NetFlow destinations to forward events to|
|»»»» host|string|true|none|Destination host|
|»»»» port|number|true|none|Destination port, default is 2055|
|»»» dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceHttp](#schemaoutputdynatracehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» method|string|false|none|The method to use when sending events|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» authType|string|false|none|none|
|»»» format|string|true|none|How to format events before sending. Defaults to JSON. Plaintext is not currently supported.|
|»»» endpoint|string|true|none|none|
|»»» telemetryType|string|true|none|none|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» environmentId|string|false|none|ID of the environment to send to|
|»»» activeGateDomain|string|false|none|ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.|
|»»» url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDynatraceOtlp](#schemaoutputdynatraceotlp)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» protocol|string|true|none|Select a transport option for Dynatrace|
|»»» endpoint|string|true|none|The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|»»» otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|»»» compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|»»» httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|»»» metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|»»» keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|»»» keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|»»» endpointType|string|true|none|Select the type of Dynatrace endpoint configured|
|»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»» authTokenName|string|false|none|none|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputSentinelOneAiSiem](#schemaoutputsentineloneaisiem)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» region|string|true|none|The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.|
|»»» endpoint|string|true|none|Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» description|string|false|none|none|
|»»» token|string|false|none|In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» baseUrl|string|false|none|Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.|
|»»» hostExpression|string|false|none|Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').|
|»»» sourceExpression|string|false|none|Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').|
|»»» sourceTypeExpression|string|false|none|Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.|
|»»» dataSourceCategoryExpression|string|false|none|Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.|
|»»» dataSourceNameExpression|string|false|none|Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').|
|»»» dataSourceVendorExpression|string|false|none|Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').|
|»»» eventTypeExpression|string|false|none|Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').|
|»»» host|string|false|none|Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').|
|»»» source|string|false|none|Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.|
|»»» sourceType|string|false|none|Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.|
|»»» dataSourceCategory|string|false|none|Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.|
|»»» dataSourceName|string|false|none|Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.|
|»»» dataSourceVendor|string|false|none|Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.|
|»»» eventType|string|false|none|Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputChronicle](#schemaoutputchronicle)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» apiVersion|string|false|none|none|
|»»» authenticationMethod|string|false|none|none|
|»»» responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|»»»» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» timeoutRetrySettings|object|false|none|none|
|»»»» timeoutRetry|boolean|true|none|none|
|»»»» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|»»»» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|»»»» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|»»» responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|»»» region|string|true|none|Regional endpoint to send events to|
|»»» concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|»»» maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|»»» maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|»»» compress|boolean|false|none|Compress the payload body before sending|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|»»» safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|»»» useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|»»» ingestionMethod|string|false|none|none|
|»»» namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|»»» logType|string|true|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|»»» logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|»»» gcpProjectId|string|true|none|The Google Cloud Platform (GCP) project ID to send events to|
|»»» gcpInstance|string|true|none|The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.|
|»»» customLabels|[object]|false|none|Custom labels to be added to every event|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»»» rbacEnabled|boolean|false|none|Designate this label for role-based access control and filtering|
|»»» description|string|false|none|none|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputDatabricks](#schemaoutputdatabricks)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» destPath|string|false|none|Optional path to prepend to files before uploading.|
|»»» stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» workspaceId|string|true|none|Databricks workspace ID|
|»»» scope|string|true|none|OAuth scope for Unity Catalog authentication|
|»»» clientId|string|true|none|OAuth client ID for Unity Catalog authentication|
|»»» catalog|string|true|none|Name of the catalog to use for the output|
|»»» schema|string|true|none|Name of the catalog schema to use for the output|
|»»» eventsVolumeName|string|true|none|Name of the events volume in Databricks|
|»»» clientTextSecret|string|true|none|OAuth client secret for Unity Catalog authentication|
|»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputMicrosoftFabric](#schemaoutputmicrosoftfabric)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» topic|string|true|none|Topic name from Fabric Eventstream's endpoint|
|»»» ack|integer|false|none|Control the number of required acknowledgments|
|»»» format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|»»» maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|»»» flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|»»» flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. This should always be $ConnectionString.|
|»»»» textSecret|string|false|none|Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» bootstrap_server|string|true|none|Bootstrap server from Fabric Eventstream's endpoint|
|»»» description|string|false|none|none|
|»»» pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|»»» pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|»»» pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|»»» pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|»»» pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|»»» pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|»»» pqCompress|string|false|none|Codec to use to compress the persisted data|
|»»» pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|»»» pqControls|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[OutputCloudflareR2](#schemaoutputcloudflarer2)|false|none|none|
|»»» id|string|false|none|Unique ID for this output|
|»»» type|string|true|none|none|
|»»» pipeline|string|false|none|Pipeline to process data before sending out to this output|
|»»» systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» endpoint|string|true|none|Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)|
|»»» bucket|string|true|none|Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|»»» region|any|false|none|none|
|»»» stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|»»» addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|»»» destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|»»» objectACL|any|false|none|none|
|»»» storageClass|string|false|none|Storage class to select for uploaded objects|
|»»» serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|»»» verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|»»» removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|»»» partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|»»» format|string|false|none|Format of the output data|
|»»» baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|»»» fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|»»» maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|»»» maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|»»» headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|»»» writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|»»» onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|»»» deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|»»» onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|»»» forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|»»» maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|»»» maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|»»» maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|»»» compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|»»» automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|»»» parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|»»» parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|»»» parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|»»» parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|»»» parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|»»» shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|»»» keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|»»»» key|string|true|none|none|
|»»»» value|string|true|none|none|
|»»» enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|»»» enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|»»» enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|»»» emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|»»» directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|»»» deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|»»» maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|default|
|type|webhook|
|method|POST|
|method|PUT|
|method|PATCH|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|oauth|
|endpointURLConfiguration|url|
|endpointURLConfiguration|ID|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|devnull|
|type|syslog|
|protocol|tcp|
|protocol|udp|
|facility|0|
|facility|1|
|facility|2|
|facility|3|
|facility|4|
|facility|5|
|facility|6|
|facility|7|
|facility|8|
|facility|9|
|facility|10|
|facility|11|
|facility|12|
|facility|13|
|facility|14|
|facility|15|
|facility|16|
|facility|17|
|facility|18|
|facility|19|
|facility|20|
|facility|21|
|severity|0|
|severity|1|
|severity|2|
|severity|3|
|severity|4|
|severity|5|
|severity|6|
|severity|7|
|messageFormat|rfc3164|
|messageFormat|rfc5424|
|timestampFormat|syslog|
|timestampFormat|iso8601|
|tls|inherit|
|tls|off|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_lb|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|authType|manual|
|authType|secret|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|splunk_hec|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|tcpjson|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|wavefront|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|signalfx|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|filesystem|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_blob|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|storageClass|Inferred|
|storageClass|Hot|
|storageClass|Cool|
|storageClass|Cold|
|storageClass|Archive|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|azure_data_explorer|
|ingestMode|batching|
|ingestMode|streaming|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|oauthType|clientSecret|
|oauthType|clientTextSecret|
|oauthType|certificate|
|format|json|
|format|raw|
|format|parquet|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|prefix|dropBy|
|prefix|ingestBy|
|reportLevel|failuresOnly|
|reportLevel|doNotReport|
|reportLevel|failuresAndSuccesses|
|reportMethod|queue|
|reportMethod|table|
|reportMethod|queueAndTable|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_logs|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|kinesis|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|compression|none|
|compression|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|honeycomb|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|azure_eventhub|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_chronicle|
|apiVersion|v1|
|apiVersion|v2|
|authenticationMethod|manual|
|authenticationMethod|secret|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|logFormatType|unstructured|
|logFormatType|udm|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|udmType|entities|
|udmType|logs|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_cloud_storage|
|signatureVersion|v2|
|signatureVersion|v4|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|google_cloud_logging|
|logLocationType|project|
|logLocationType|organization|
|logLocationType|billingAccount|
|logLocationType|folder|
|payloadFormat|text|
|payloadFormat|json|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|google_pubsub|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|exabeam|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|type|kafka|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|confluent_cloud|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|msk|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|elasticVersion|auto|
|elasticVersion|6|
|elasticVersion|7|
|writeAction|index|
|writeAction|create|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|elastic_cloud|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic|
|region|US|
|region|EU|
|region|Custom|
|name|service|
|name|hostname|
|name|timestamp|
|name|auditId|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|newrelic_events|
|region|US|
|region|EU|
|region|Custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|influxdb|
|timestampPrecision|ns|
|timestampPrecision|u|
|timestampPrecision|ms|
|timestampPrecision|s|
|timestampPrecision|m|
|timestampPrecision|h|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudwatch|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|minio|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|statsd|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|statsd_ext|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|graphite|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|router|
|type|sns|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sqs|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|snmp|
|type|sumo_logic|
|format|json|
|format|raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|datadog|
|contentType|text|
|contentType|json|
|severity|emergency|
|severity|alert|
|severity|critical|
|severity|error|
|severity|warning|
|severity|notice|
|severity|info|
|severity|debug|
|site|us|
|site|us3|
|site|us5|
|site|eu|
|site|fed1|
|site|ap1|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|grafana_cloud|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|loki|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|prometheus|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|ring|
|format|json|
|format|raw|
|compress|none|
|compress|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|type|open_telemetry|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|service_now|
|otlpVersion|1.3.1|
|protocol|grpc|
|protocol|http|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dataset|
|defaultSeverity|finest|
|defaultSeverity|finer|
|defaultSeverity|fine|
|defaultSeverity|info|
|defaultSeverity|warning|
|defaultSeverity|error|
|defaultSeverity|fatal|
|site|us|
|site|eu|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_tcp|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cribl_http|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|compression|none|
|compression|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|humio_hec|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|crowdstrike_next_gen_siem|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dl_s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|security_lake|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|cribl_lake|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|auto_rpc|
|awsAuthenticationMethod|manual|
|format|json|
|format|parquet|
|format|ddss|
|type|disk_spool|
|compress|none|
|compress|gzip|
|type|click_house|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|sslUserCertificate|
|authType|token|
|authType|textSecret|
|authType|oauth|
|format|json-compact-each-row-with-names|
|format|json-each-row|
|mappingType|automatic|
|mappingType|custom|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|xsiam|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|token|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|netflow|
|type|dynatrace_http|
|method|POST|
|method|PUT|
|method|PATCH|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|token|
|authType|textSecret|
|format|json_array|
|format|plaintext|
|endpoint|cloud|
|endpoint|activeGate|
|endpoint|manual|
|telemetryType|logs|
|telemetryType|metrics|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|dynatrace_otlp|
|protocol|http|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|endpointType|saas|
|endpointType|ag|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|sentinel_one_ai_siem|
|region|US|
|region|CA|
|region|EMEA|
|region|AP|
|region|APS|
|region|AU|
|region|Custom|
|endpoint|/services/collector/event|
|endpoint|/services/collector/raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|chronicle|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|databricks|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|type|microsoft_fabric|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|
|type|cloudflare_r2|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|secret|
|awsAuthenticationMethod|manual|
|signatureVersion|v2|
|signatureVersion|v4|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<aside class="success">
This operation does not require authentication
</aside>

## Lists destinations associated with this project.

<a id="opIdgetProjectDestinationsByProjectId"></a>

> Code samples

`GET /system/projects/{projectId}/destinations`

Lists destinations associated with this project.

<h3 id="lists-destinations-associated-with-this-project.-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|projectId|path|string|true|Project Id|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "string"
    }
  ]
}
```

<h3 id="lists-destinations-associated-with-this-project.-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of ProjectDestination objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="lists-destinations-associated-with-this-project.-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[ProjectDestination](#schemaprojectdestination)]|false|none|none|
|»» id|string|true|none|none|
|»» type|string|true|none|none|

<aside class="success">
This operation does not require authentication
</aside>

## Get a list of Destination Status objects

<a id="opIdlistOutputStatus"></a>

> Code samples

`GET /system/status/outputs`

Get a list of Destination Status objects

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "status": {
        "error": {
          "details": {},
          "message": "string"
        },
        "health": "Green",
        "healthCounts": {
          "Green": 0,
          "Yellow": 0,
          "Red": 0,
          "Unknown": 0
        },
        "metrics": {},
        "pq": {
          "error": {
            "details": {},
            "message": "string"
          },
          "health": "Green",
          "healthCounts": {
            "Green": 0,
            "Yellow": 0,
            "Red": 0,
            "Unknown": 0
          },
          "timestamp": 0
        },
        "timestamp": 0
      }
    }
  ]
}
```

<h3 id="get-a-list-of-destination-status-objects-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination Status objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-a-list-of-destination-status-objects-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[OutputStatus](#schemaoutputstatus)]|false|none|none|
|»» id|string|true|none|none|
|»» status|object|true|none|none|
|»»» error|object|false|none|none|
|»»»» details|object|false|none|none|
|»»»» message|string|true|none|none|
|»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» Green|number|true|none|none|
|»»»» Yellow|number|true|none|none|
|»»»» Red|number|true|none|none|
|»»»» Unknown|number|true|none|none|
|»»» metrics|object|true|none|none|
|»»» pq|object|false|none|none|
|»»»» error|object|false|none|none|
|»»»»» details|object|false|none|none|
|»»»»» message|string|true|none|none|
|»»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» timestamp|number|true|none|none|
|»»» timestamp|number|true|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|

<aside class="success">
This operation does not require authentication
</aside>

## Get Destination Status by ID

<a id="opIdgetOutputStatusById"></a>

> Code samples

`GET /system/status/outputs/{id}`

Get Destination Status by ID

<h3 id="get-destination-status-by-id-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|Unique ID to GET|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "status": {
        "error": {
          "details": {},
          "message": "string"
        },
        "health": "Green",
        "healthCounts": {
          "Green": 0,
          "Yellow": 0,
          "Red": 0,
          "Unknown": 0
        },
        "metrics": {},
        "pq": {
          "error": {
            "details": {},
            "message": "string"
          },
          "health": "Green",
          "healthCounts": {
            "Green": 0,
            "Yellow": 0,
            "Red": 0,
            "Unknown": 0
          },
          "timestamp": 0
        },
        "timestamp": 0
      }
    }
  ]
}
```

<h3 id="get-destination-status-by-id-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Destination Status objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-destination-status-by-id-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[OutputStatus](#schemaoutputstatus)]|false|none|none|
|»» id|string|true|none|none|
|»» status|object|true|none|none|
|»»» error|object|false|none|none|
|»»»» details|object|false|none|none|
|»»»» message|string|true|none|none|
|»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» Green|number|true|none|none|
|»»»» Yellow|number|true|none|none|
|»»»» Red|number|true|none|none|
|»»»» Unknown|number|true|none|none|
|»»» metrics|object|true|none|none|
|»»» pq|object|false|none|none|
|»»»» error|object|false|none|none|
|»»»»» details|object|false|none|none|
|»»»»» message|string|true|none|none|
|»»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» timestamp|number|true|none|none|
|»»» timestamp|number|true|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|

<aside class="success">
This operation does not require authentication
</aside>

## Get sample event data for a Destination

<a id="opIdgetOutputSamplesById"></a>

> Code samples

`GET /system/outputs/{id}/samples`

Get sample event data for the specified Destination to validate the configuration or test connectivity.

<h3 id="get-sample-event-data-for-a-destination-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Destination to get sample event data for.|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "events": [
        {}
      ]
    }
  ]
}
```

<h3 id="get-sample-event-data-for-a-destination-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of OutputSamplesResponse objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-sample-event-data-for-a-destination-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[OutputSamplesResponse](#schemaoutputsamplesresponse)]|false|none|none|
|»» events|[object]|true|none|none|

<aside class="success">
This operation does not require authentication
</aside>

## Send sample event data to a Destination

<a id="opIdcreateOutputTestById"></a>

> Code samples

`POST /system/outputs/{id}/test`

Send sample event data to the specified Destination to validate the configuration or test connectivity.

> Body parameter

```json
{
  "events": [
    {
      "_raw": "string"
    }
  ]
}
```

<h3 id="send-sample-event-data-to-a-destination-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Destination to send sample event data to.|
|body|body|[OutputTestRequest](#schemaoutputtestrequest)|true|OutputTestRequest object|
|» events|body|[[CriblEvent](#schemacriblevent)]|true|none|
|»» _raw|body|string|true|none|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "details": {},
      "error": "string",
      "outputId": "string",
      "success": true,
      "successDetail": "string"
    }
  ]
}
```

<h3 id="send-sample-event-data-to-a-destination-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of OutputTestResponse objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="send-sample-event-data-to-a-destination-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[OutputTestResponse](#schemaoutputtestresponse)]|false|none|none|
|»» details|object|false|none|none|
|»» error|string|false|none|none|
|»» outputId|string|true|none|none|
|»» success|boolean|true|none|none|
|»» successDetail|string|false|none|none|

<aside class="success">
This operation does not require authentication
</aside>

# Schemas

<h2 id="tocS_Output">Output</h2>
<!-- backwards compatibility -->
<a id="schemaoutput"></a>
<a id="schema_Output"></a>
<a id="tocSoutput"></a>
<a id="tocsoutput"></a>

```json
{
  "id": "string",
  "type": "default",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "defaultId": "string"
}

```

### Properties

oneOf

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDefault](#schemaoutputdefault)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputWebhook](#schemaoutputwebhook)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSentinel](#schemaoutputsentinel)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDevnull](#schemaoutputdevnull)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSyslog](#schemaoutputsyslog)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSplunk](#schemaoutputsplunk)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSplunkLb](#schemaoutputsplunklb)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSplunkHec](#schemaoutputsplunkhec)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputTcpjson](#schemaoutputtcpjson)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputWavefront](#schemaoutputwavefront)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSignalfx](#schemaoutputsignalfx)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputFilesystem](#schemaoutputfilesystem)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputS3](#schemaoutputs3)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputAzureBlob](#schemaoutputazureblob)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputAzureDataExplorer](#schemaoutputazuredataexplorer)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputAzureLogs](#schemaoutputazurelogs)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputKinesis](#schemaoutputkinesis)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputHoneycomb](#schemaoutputhoneycomb)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputAzureEventhub](#schemaoutputazureeventhub)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputGoogleChronicle](#schemaoutputgooglechronicle)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputGoogleCloudStorage](#schemaoutputgooglecloudstorage)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputGoogleCloudLogging](#schemaoutputgooglecloudlogging)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputGooglePubsub](#schemaoutputgooglepubsub)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputExabeam](#schemaoutputexabeam)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputKafka](#schemaoutputkafka)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputConfluentCloud](#schemaoutputconfluentcloud)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputMsk](#schemaoutputmsk)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputElastic](#schemaoutputelastic)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputElasticCloud](#schemaoutputelasticcloud)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputNewrelic](#schemaoutputnewrelic)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputNewrelicEvents](#schemaoutputnewrelicevents)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputInfluxdb](#schemaoutputinfluxdb)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputCloudwatch](#schemaoutputcloudwatch)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputMinio](#schemaoutputminio)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputStatsd](#schemaoutputstatsd)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputStatsdExt](#schemaoutputstatsdext)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputGraphite](#schemaoutputgraphite)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputRouter](#schemaoutputrouter)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSns](#schemaoutputsns)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSqs](#schemaoutputsqs)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSnmp](#schemaoutputsnmp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSumoLogic](#schemaoutputsumologic)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDatadog](#schemaoutputdatadog)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputGrafanaCloud](#schemaoutputgrafanacloud)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputLoki](#schemaoutputloki)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputPrometheus](#schemaoutputprometheus)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputRing](#schemaoutputring)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputOpenTelemetry](#schemaoutputopentelemetry)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputServiceNow](#schemaoutputservicenow)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDataset](#schemaoutputdataset)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputCriblTcp](#schemaoutputcribltcp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputCriblHttp](#schemaoutputcriblhttp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputHumioHec](#schemaoutputhumiohec)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputCrowdstrikeNextGenSiem](#schemaoutputcrowdstrikenextgensiem)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDlS3](#schemaoutputdls3)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSecurityLake](#schemaoutputsecuritylake)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputCriblLake](#schemaoutputcribllake)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDiskSpool](#schemaoutputdiskspool)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputClickHouse](#schemaoutputclickhouse)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputXsiam](#schemaoutputxsiam)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputNetflow](#schemaoutputnetflow)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDynatraceHttp](#schemaoutputdynatracehttp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDynatraceOtlp](#schemaoutputdynatraceotlp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputSentinelOneAiSiem](#schemaoutputsentineloneaisiem)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputChronicle](#schemaoutputchronicle)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputDatabricks](#schemaoutputdatabricks)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputMicrosoftFabric](#schemaoutputmicrosoftfabric)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[OutputCloudflareR2](#schemaoutputcloudflarer2)|false|none|none|

<h2 id="tocS_Error">Error</h2>
<!-- backwards compatibility -->
<a id="schemaerror"></a>
<a id="schema_Error"></a>
<a id="tocSerror"></a>
<a id="tocserror"></a>

```json
{
  "message": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|message|string|false|none|Error message|

<h2 id="tocS_ProjectDestination">ProjectDestination</h2>
<!-- backwards compatibility -->
<a id="schemaprojectdestination"></a>
<a id="schema_ProjectDestination"></a>
<a id="tocSprojectdestination"></a>
<a id="tocsprojectdestination"></a>

```json
{
  "id": "string",
  "type": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|true|none|none|
|type|string|true|none|none|

<h2 id="tocS_OutputStatus">OutputStatus</h2>
<!-- backwards compatibility -->
<a id="schemaoutputstatus"></a>
<a id="schema_OutputStatus"></a>
<a id="tocSoutputstatus"></a>
<a id="tocsoutputstatus"></a>

```json
{
  "id": "string",
  "status": {
    "error": {
      "details": {},
      "message": "string"
    },
    "health": "Green",
    "healthCounts": {
      "Green": 0,
      "Yellow": 0,
      "Red": 0,
      "Unknown": 0
    },
    "metrics": {},
    "pq": {
      "error": {
        "details": {},
        "message": "string"
      },
      "health": "Green",
      "healthCounts": {
        "Green": 0,
        "Yellow": 0,
        "Red": 0,
        "Unknown": 0
      },
      "timestamp": 0
    },
    "timestamp": 0
  }
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|true|none|none|
|status|object|true|none|none|
|» error|object|false|none|none|
|»» details|object|false|none|none|
|»» message|string|true|none|none|
|» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|» metrics|object|true|none|none|
|» pq|object|false|none|none|
|»» error|object|false|none|none|
|»»» details|object|false|none|none|
|»»» message|string|true|none|none|
|»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»» timestamp|number|true|none|none|
|» timestamp|number|true|none|none|

<h2 id="tocS_OutputSamplesResponse">OutputSamplesResponse</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsamplesresponse"></a>
<a id="schema_OutputSamplesResponse"></a>
<a id="tocSoutputsamplesresponse"></a>
<a id="tocsoutputsamplesresponse"></a>

```json
{
  "events": [
    {}
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|events|[object]|true|none|none|

<h2 id="tocS_OutputTestResponse">OutputTestResponse</h2>
<!-- backwards compatibility -->
<a id="schemaoutputtestresponse"></a>
<a id="schema_OutputTestResponse"></a>
<a id="tocSoutputtestresponse"></a>
<a id="tocsoutputtestresponse"></a>

```json
{
  "details": {},
  "error": "string",
  "outputId": "string",
  "success": true,
  "successDetail": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|details|object|false|none|none|
|error|string|false|none|none|
|outputId|string|true|none|none|
|success|boolean|true|none|none|
|successDetail|string|false|none|none|

<h2 id="tocS_OutputTestRequest">OutputTestRequest</h2>
<!-- backwards compatibility -->
<a id="schemaoutputtestrequest"></a>
<a id="schema_OutputTestRequest"></a>
<a id="tocSoutputtestrequest"></a>
<a id="tocsoutputtestrequest"></a>

```json
{
  "events": [
    {
      "_raw": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|events|[[CriblEvent](#schemacriblevent)]|true|none|none|

<h2 id="tocS_OutputDefault">OutputDefault</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdefault"></a>
<a id="schema_OutputDefault"></a>
<a id="tocSoutputdefault"></a>
<a id="tocsoutputdefault"></a>

```json
{
  "id": "string",
  "type": "default",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "defaultId": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|defaultId|string|true|none|ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|default|

<h2 id="tocS_OutputWebhook">OutputWebhook</h2>
<!-- backwards compatibility -->
<a id="schemaoutputwebhook"></a>
<a id="schema_OutputWebhook"></a>
<a id="tocSoutputwebhook"></a>
<a id="tocsoutputwebhook"></a>

```json
{
  "id": "string",
  "type": "webhook",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "method": "POST",
  "format": "ndjson",
  "keepAlive": true,
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "onBackpressure": "block",
  "authType": "none",
  "tls": {
    "disabled": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "totalMemoryLimitKB": 0,
  "loadBalanced": false,
  "description": "string",
  "customSourceExpression": "__httpOut",
  "customDropWhenNull": false,
  "customEventDelimiter": "\\n",
  "customContentType": "application/x-ndjson",
  "customPayloadExpression": "`${events}`",
  "advancedContentType": "application/json",
  "formatEventCode": "string",
  "formatPayloadCode": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "url": "string",
  "excludeSelf": false,
  "urls": [
    {
      "url": "string",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|method|string|false|none|The method to use when sending events|
|format|string|false|none|How to format events before sending out|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Authentication method to use for the HTTP request|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|description|string|false|none|none|
|customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|advancedContentType|string|false|none|HTTP content-type header value|
|formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|
|url|string|false|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|urls|[object]|false|none|none|
|» url|string|true|none|URL of a webhook endpoint to send events to, such as http://localhost:10200|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|

#### Enumerated Values

|Property|Value|
|---|---|
|type|webhook|
|method|POST|
|method|PUT|
|method|PATCH|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSentinel">OutputSentinel</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsentinel"></a>
<a id="schema_OutputSentinel"></a>
<a id="tocSoutputsentinel"></a>
<a id="tocsoutputsentinel"></a>

```json
{
  "id": "string",
  "type": "sentinel",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "keepAlive": true,
  "concurrency": 5,
  "maxPayloadSizeKB": 1000,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "onBackpressure": "block",
  "authType": "oauth",
  "loginUrl": "string",
  "secret": "string",
  "client_id": "string",
  "scope": "https://monitor.azure.com/.default",
  "endpointURLConfiguration": "url",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "format": "ndjson",
  "customSourceExpression": "__httpOut",
  "customDropWhenNull": false,
  "customEventDelimiter": "\\n",
  "customContentType": "application/x-ndjson",
  "customPayloadExpression": "`${events}`",
  "advancedContentType": "application/json",
  "formatEventCode": "string",
  "formatPayloadCode": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "url": "string",
  "dcrID": "string",
  "dceEndpoint": "string",
  "streamName": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|none|
|loginUrl|string|true|none|URL for OAuth|
|secret|string|true|none|Secret parameter value to pass in request body|
|client_id|string|true|none|JavaScript expression to compute the Client ID for the Azure application. Can be a constant.|
|scope|string|false|none|Scope to pass in the OAuth request|
|endpointURLConfiguration|string|true|none|Enter the data collection endpoint URL or the individual ID|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|format|any|false|none|none|
|customSourceExpression|string|false|none|Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.|
|customDropWhenNull|boolean|false|none|Whether to drop events when the source expression evaluates to null|
|customEventDelimiter|string|false|none|Delimiter string to insert between individual events. Defaults to newline character.|
|customContentType|string|false|none|Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.|
|customPayloadExpression|string|false|none|Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.|
|advancedContentType|string|false|none|HTTP content-type header value|
|formatEventCode|string|false|none|Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|formatPayloadCode|string|false|none|Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|
|dcrID|string|false|none|Immutable ID for the Data Collection Rule (DCR)|
|dceEndpoint|string|false|none|Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`|
|streamName|string|false|none|The name of the stream (Sentinel table) in which to store the events|

#### Enumerated Values

|Property|Value|
|---|---|
|type|sentinel|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|oauth|
|endpointURLConfiguration|url|
|endpointURLConfiguration|ID|
|format|ndjson|
|format|json_array|
|format|custom|
|format|advanced|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputDevnull">OutputDevnull</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdevnull"></a>
<a id="schema_OutputDevnull"></a>
<a id="tocSoutputdevnull"></a>
<a id="tocsoutputdevnull"></a>

```json
{
  "id": "string",
  "type": "devnull",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": []
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|

#### Enumerated Values

|Property|Value|
|---|---|
|type|devnull|

<h2 id="tocS_OutputSyslog">OutputSyslog</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsyslog"></a>
<a id="schema_OutputSyslog"></a>
<a id="tocSoutputsyslog"></a>
<a id="tocsoutputsyslog"></a>

```json
{
  "id": "string",
  "type": "syslog",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "protocol": "tcp",
  "facility": 0,
  "severity": 0,
  "appName": "Cribl",
  "messageFormat": "rfc3164",
  "timestampFormat": "syslog",
  "throttleRatePerSec": "0",
  "octetCountFraming": true,
  "logFailedRequests": false,
  "description": "string",
  "loadBalanced": true,
  "host": "string",
  "port": 65535,
  "excludeSelf": false,
  "hosts": [
    {
      "host": "string",
      "port": 65535,
      "tls": "inherit",
      "servername": "string",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "maxConcurrentSenders": 0,
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "onBackpressure": "block",
  "maxRecordSize": 1500,
  "udpDnsResolvePeriodSec": 0,
  "enableIpSpoofing": false,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|protocol|string|false|none|The network protocol to use for sending out syslog messages|
|facility|integer|false|none|Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.|
|severity|integer|false|none|Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.|
|appName|string|false|none|Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.|
|messageFormat|string|false|none|The syslog message format depending on the receiver's support|
|timestampFormat|string|false|none|Timestamp format to use when serializing event's time field|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|octetCountFraming|boolean|false|none|Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.|
|logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|description|string|false|none|none|
|loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.|
|host|string|false|none|The hostname of the receiver|
|port|number|false|none|The port to connect to on the provided host|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|hosts|[object]|false|none|Set of hosts to load-balance data to|
|» host|string|true|none|The hostname of the receiver|
|» port|number|true|none|The port to connect to on the provided host|
|» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|maxRecordSize|number|false|none|Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.|
|udpDnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.|
|enableIpSpoofing|boolean|false|none|Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|syslog|
|protocol|tcp|
|protocol|udp|
|facility|0|
|facility|1|
|facility|2|
|facility|3|
|facility|4|
|facility|5|
|facility|6|
|facility|7|
|facility|8|
|facility|9|
|facility|10|
|facility|11|
|facility|12|
|facility|13|
|facility|14|
|facility|15|
|facility|16|
|facility|17|
|facility|18|
|facility|19|
|facility|20|
|facility|21|
|severity|0|
|severity|1|
|severity|2|
|severity|3|
|severity|4|
|severity|5|
|severity|6|
|severity|7|
|messageFormat|rfc3164|
|messageFormat|rfc5424|
|timestampFormat|syslog|
|timestampFormat|iso8601|
|tls|inherit|
|tls|off|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSplunk">OutputSplunk</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsplunk"></a>
<a id="schema_OutputSplunk"></a>
<a id="tocSoutputsplunk"></a>
<a id="tocsoutputsplunk"></a>

```json
{
  "id": "string",
  "type": "splunk",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "host": "string",
  "port": 9997,
  "nestedFields": "json",
  "throttleRatePerSec": "0",
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "enableMultiMetrics": false,
  "enableACK": true,
  "logFailedRequests": false,
  "maxS2Sversion": "v3",
  "onBackpressure": "block",
  "authType": "manual",
  "description": "string",
  "maxFailedHealthChecks": 1,
  "compress": "disabled",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "authToken": "",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|host|string|true|none|The hostname of the receiver|
|port|number|true|none|The port to connect to on the provided host|
|nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|description|string|false|none|none|
|maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSplunkLb">OutputSplunkLb</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsplunklb"></a>
<a id="schema_OutputSplunkLb"></a>
<a id="tocSoutputsplunklb"></a>
<a id="tocsoutputsplunklb"></a>

```json
{
  "id": "string",
  "type": "splunk_lb",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "maxConcurrentSenders": 0,
  "nestedFields": "json",
  "throttleRatePerSec": "0",
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "enableMultiMetrics": false,
  "enableACK": true,
  "logFailedRequests": false,
  "maxS2Sversion": "v3",
  "onBackpressure": "block",
  "indexerDiscovery": false,
  "senderUnhealthyTimeAllowance": 100,
  "authType": "manual",
  "description": "string",
  "maxFailedHealthChecks": 1,
  "compress": "disabled",
  "indexerDiscoveryConfigs": {
    "site": "default",
    "masterUri": "string",
    "refreshIntervalSec": 300,
    "rejectUnauthorized": false,
    "authTokens": [
      {
        "authType": "manual"
      }
    ],
    "authType": "manual",
    "authToken": "",
    "textSecret": "string"
  },
  "excludeSelf": false,
  "hosts": [
    {
      "host": "string",
      "port": 9997,
      "tls": "inherit",
      "servername": "string",
      "weight": 1
    }
  ],
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "authToken": "",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|nestedFields|string|false|none|How to serialize nested fields into index-time fields|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.|
|enableACK|boolean|false|none|Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.|
|logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|indexerDiscovery|boolean|false|none|Automatically discover indexers in indexer clustering environment.|
|senderUnhealthyTimeAllowance|number|false|none|How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|description|string|false|none|none|
|maxFailedHealthChecks|number|false|none|Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.|
|compress|string|false|none|Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.|
|indexerDiscoveryConfigs|object|false|none|List of configurations to set up indexer discovery in Splunk Indexer clustering environment.|
|» site|string|true|none|Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.|
|» masterUri|string|true|none|Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089|
|» refreshIntervalSec|number|true|none|Time interval, in seconds, between two consecutive indexer list fetches from cluster manager|
|» rejectUnauthorized|boolean|false|none|During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.|
|» authTokens|[object]|false|none|Tokens required to authenticate to cluster manager for indexer discovery|
|»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|» textSecret|string|false|none|Select or create a stored text secret|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|hosts|[object]|true|none|Set of Splunk indexers to load-balance data to.|
|» host|string|true|none|The hostname of the receiver|
|» port|number|true|none|The port to connect to on the provided host|
|» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|authToken|string|false|none|Shared secret token to use when establishing a connection to a Splunk indexer.|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk_lb|
|nestedFields|json|
|nestedFields|none|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|compress|disabled|
|compress|auto|
|compress|always|
|authType|manual|
|authType|secret|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSplunkHec">OutputSplunkHec</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsplunkhec"></a>
<a id="schema_OutputSplunkHec"></a>
<a id="tocSoutputsplunkhec"></a>
<a id="tocsoutputsplunkhec"></a>

```json
{
  "id": "string",
  "type": "splunk_hec",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "loadBalanced": true,
  "nextQueue": "indexQueue",
  "tcpRouting": "nowhere",
  "tls": {
    "disabled": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "enableMultiMetrics": false,
  "authType": "manual",
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "url": "http://localhost:8088/services/collector/event",
  "useRoundRobinDns": false,
  "excludeSelf": false,
  "urls": [
    {
      "url": "http://localhost:8088/services/collector/event",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "token": "string",
  "textSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|nextQueue|string|false|none|In the Splunk app, define which Splunk processing queue to send the events after HEC processing.|
|tcpRouting|string|false|none|In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|enableMultiMetrics|boolean|false|none|Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|url|string|false|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|urls|[object]|false|none|none|
|» url|string|true|none|URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|token|string|false|none|Splunk HEC authentication token|
|textSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk_hec|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputTcpjson">OutputTcpjson</h2>
<!-- backwards compatibility -->
<a id="schemaoutputtcpjson"></a>
<a id="schema_OutputTcpjson"></a>
<a id="tocSoutputtcpjson"></a>
<a id="tocsoutputtcpjson"></a>

```json
{
  "id": "string",
  "type": "tcpjson",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "loadBalanced": true,
  "compression": "none",
  "logFailedRequests": false,
  "throttleRatePerSec": "0",
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "tokenTTLMinutes": 60,
  "sendHeader": true,
  "onBackpressure": "block",
  "authType": "manual",
  "description": "string",
  "host": "string",
  "port": 65535,
  "excludeSelf": false,
  "hosts": [
    {
      "host": "string",
      "port": 65535,
      "tls": "inherit",
      "servername": "string",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "maxConcurrentSenders": 0,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "authToken": "",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|loadBalanced|boolean|false|none|Use load-balanced destinations|
|compression|string|false|none|Codec to use to compress the data before sending|
|logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|sendHeader|boolean|false|none|Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|description|string|false|none|none|
|host|string|false|none|The hostname of the receiver|
|port|number|false|none|The port to connect to on the provided host|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|hosts|[object]|false|none|Set of hosts to load-balance data to|
|» host|string|true|none|The hostname of the receiver|
|» port|number|true|none|The port to connect to on the provided host|
|» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|authToken|string|false|none|Optional authentication token to include as part of the connection header|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|tcpjson|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputWavefront">OutputWavefront</h2>
<!-- backwards compatibility -->
<a id="schemaoutputwavefront"></a>
<a id="schema_OutputWavefront"></a>
<a id="tocSoutputwavefront"></a>
<a id="tocsoutputwavefront"></a>

```json
{
  "id": "string",
  "type": "wavefront",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "authType": "manual",
  "domain": "longboard",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "token": "string",
  "textSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|domain|string|true|none|WaveFront domain name, e.g. "longboard"|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|token|string|false|none|WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))|
|textSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|wavefront|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSignalfx">OutputSignalfx</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsignalfx"></a>
<a id="schema_OutputSignalfx"></a>
<a id="tocSoutputsignalfx"></a>
<a id="tocsoutputsignalfx"></a>

```json
{
  "id": "string",
  "type": "signalfx",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "authType": "manual",
  "realm": "us0",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "token": "string",
  "textSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|realm|string|true|none|SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|token|string|false|none|SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))|
|textSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|signalfx|
|authType|manual|
|authType|secret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputFilesystem">OutputFilesystem</h2>
<!-- backwards compatibility -->
<a id="schemaoutputfilesystem"></a>
<a id="schema_OutputFilesystem"></a>
<a id="tocSoutputfilesystem"></a>
<a id="tocsoutputfilesystem"></a>

```json
{
  "id": "string",
  "type": "filesystem",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "destPath": "string",
  "stagePath": "string",
  "addIdToStagePath": true,
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "description": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|destPath|string|true|none|Final destination for the output files|
|stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|description|string|false|none|none|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|filesystem|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputS3">OutputS3</h2>
<!-- backwards compatibility -->
<a id="schemaoutputs3"></a>
<a id="schema_OutputS3"></a>
<a id="tocSoutputs3"></a>
<a id="tocsoutputs3"></a>

```json
{
  "id": "string",
  "type": "s3",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "bucket": "string",
  "region": "string",
  "awsSecretKey": "string",
  "awsAuthenticationMethod": "auto",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "destPath": "",
  "objectACL": "private",
  "storageClass": "STANDARD",
  "serverSideEncryption": "AES256",
  "kmsKeyId": "string",
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxConcurrentFileParts": 4,
  "verifyPermissions": true,
  "maxClosingFilesToBackpressure": 100,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|region|string|false|none|Region where the S3 bucket is located|
|awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|serverSideEncryption|string|false|none|none|
|kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|description|string|false|none|none|
|awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputAzureBlob">OutputAzureBlob</h2>
<!-- backwards compatibility -->
<a id="schemaoutputazureblob"></a>
<a id="schema_OutputAzureBlob"></a>
<a id="tocSoutputazureblob"></a>
<a id="tocsoutputazureblob"></a>

```json
{
  "id": "string",
  "type": "azure_blob",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "containerName": "string",
  "createContainer": false,
  "destPath": "string",
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "maxConcurrentFileParts": 1,
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "authType": "manual",
  "storageClass": "Inferred",
  "description": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20,
  "connectionString": "string",
  "textSecret": "string",
  "storageAccountName": "string",
  "tenantId": "string",
  "clientId": "string",
  "azureCloud": "string",
  "endpointSuffix": "string",
  "clientTextSecret": "string",
  "certificate": {
    "certificateName": "string"
  }
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|containerName|string|true|none|The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.|
|createContainer|boolean|false|none|Create the configured container in Azure Blob Storage if it does not already exist|
|destPath|string|false|none|Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.|
|stagePath|string|true|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|authType|string|false|none|none|
|storageClass|string|false|none|none|
|description|string|false|none|none|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|textSecret|string|false|none|Select or create a stored text secret|
|storageAccountName|string|false|none|The name of your Azure storage account|
|tenantId|string|false|none|The service principal's tenant ID|
|clientId|string|false|none|The service principal's client ID|
|azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|clientTextSecret|string|false|none|Select or create a stored text secret|
|certificate|object|false|none|none|
|» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

#### Enumerated Values

|Property|Value|
|---|---|
|type|azure_blob|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|storageClass|Inferred|
|storageClass|Hot|
|storageClass|Cool|
|storageClass|Cold|
|storageClass|Archive|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputAzureDataExplorer">OutputAzureDataExplorer</h2>
<!-- backwards compatibility -->
<a id="schemaoutputazuredataexplorer"></a>
<a id="schema_OutputAzureDataExplorer"></a>
<a id="tocSoutputazuredataexplorer"></a>
<a id="tocsoutputazuredataexplorer"></a>

```json
{
  "id": "string",
  "type": "azure_data_explorer",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "clusterUrl": "string",
  "database": "string",
  "table": "string",
  "validateDatabaseSettings": true,
  "ingestMode": "batching",
  "oauthEndpoint": "https://login.microsoftonline.com",
  "tenantId": "string",
  "clientId": "string",
  "scope": "string",
  "oauthType": "clientSecret",
  "description": "string",
  "clientSecret": "string",
  "textSecret": "string",
  "certificate": {
    "certificateName": "string"
  },
  "format": "json",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "removeEmptyDirs": true,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterEnabled": false,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20,
  "isMappingObj": false,
  "mappingObj": "string",
  "mappingRef": "string",
  "ingestUrl": "string",
  "onBackpressure": "block",
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxOpenFiles": 100,
  "maxConcurrentFileParts": 1,
  "onDiskFullBackpressure": "block",
  "addIdToStagePath": true,
  "timeoutSec": 30,
  "flushImmediately": false,
  "retainBlobOnSuccess": false,
  "extentTags": [
    {
      "prefix": "dropBy",
      "value": "string"
    }
  ],
  "ingestIfNotExists": [
    {
      "value": "string"
    }
  ],
  "reportLevel": "failuresOnly",
  "reportMethod": "queue",
  "additionalProperties": [
    {
      "key": "string",
      "value": "string"
    }
  ],
  "responseRetrySettings": [
    {
      "httpStatus": 429,
      "initialBackoff": 1000,
      "backoffRate": 2,
      "maxBackoff": 10000
    }
  ],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "flushPeriodSec": 1,
  "rejectUnauthorized": true,
  "useRoundRobinDns": false,
  "keepAlive": true,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|clusterUrl|string|true|none|The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.|
|database|string|true|none|Name of the database containing the table where data will be ingested|
|table|string|true|none|Name of the table to ingest data into|
|validateDatabaseSettings|boolean|false|none|When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.|
|ingestMode|string|false|none|none|
|oauthEndpoint|string|true|none|Endpoint used to acquire authentication tokens from Azure|
|tenantId|string|true|none|Directory ID (tenant identifier) in Azure Active Directory|
|clientId|string|true|none|client_id to pass in the OAuth request parameter|
|scope|string|true|none|Scope to pass in the OAuth request parameter|
|oauthType|string|true|none|The type of OAuth 2.0 client credentials grant flow to use|
|description|string|false|none|none|
|clientSecret|string|false|none|The client secret that you generated for your app in the Azure portal|
|textSecret|string|false|none|Select or create a stored text secret|
|certificate|object|false|none|none|
|» certificateName|string|false|none|The certificate you registered as credentials for your app in the Azure portal|
|format|string|false|none|Format of the output data|
|compress|string|true|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|isMappingObj|boolean|false|none|Send a JSON mapping object instead of specifying an existing named data mapping|
|mappingObj|string|false|none|Enter a JSON object that defines your desired data mapping|
|mappingRef|string|false|none|Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.|
|ingestUrl|string|false|none|The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushImmediately|boolean|false|none|Bypass the data management service's aggregation mechanism|
|retainBlobOnSuccess|boolean|false|none|Prevent blob deletion after ingestion is complete|
|extentTags|[object]|false|none|Strings or tags associated with the extent (ingested data shard)|
|» prefix|string|false|none|none|
|» value|string|true|none|none|
|ingestIfNotExists|[object]|false|none|Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists|
|» value|string|true|none|none|
|reportLevel|string|false|none|Level of ingestion status reporting. Defaults to FailuresOnly.|
|reportMethod|string|false|none|Target of the ingestion status reporting. Defaults to Queue.|
|additionalProperties|[object]|false|none|Optionally, enter additional configuration properties to send to the ingestion service|
|» key|string|true|none|none|
|» value|string|true|none|none|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|azure_data_explorer|
|ingestMode|batching|
|ingestMode|streaming|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|oauthType|clientSecret|
|oauthType|clientTextSecret|
|oauthType|certificate|
|format|json|
|format|raw|
|format|parquet|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|prefix|dropBy|
|prefix|ingestBy|
|reportLevel|failuresOnly|
|reportLevel|doNotReport|
|reportLevel|failuresAndSuccesses|
|reportMethod|queue|
|reportMethod|table|
|reportMethod|queueAndTable|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputAzureLogs">OutputAzureLogs</h2>
<!-- backwards compatibility -->
<a id="schemaoutputazurelogs"></a>
<a id="schema_OutputAzureLogs"></a>
<a id="tocSoutputazurelogs"></a>
<a id="tocsoutputazurelogs"></a>

```json
{
  "id": "string",
  "type": "azure_logs",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "logType": "Cribl",
  "resourceId": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 1024,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "apiUrl": ".ods.opinsights.azure.com",
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "authType": "manual",
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "workspaceId": "string",
  "workspaceKey": "string",
  "keypairSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|logType|string|true|none|The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.|
|resourceId|string|false|none|Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|none|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|apiUrl|string|false|none|The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Enter workspace ID and workspace key directly, or select a stored secret|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|workspaceId|string|false|none|Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.|
|workspaceKey|string|false|none|Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.|
|keypairSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

#### Enumerated Values

|Property|Value|
|---|---|
|type|azure_logs|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputKinesis">OutputKinesis</h2>
<!-- backwards compatibility -->
<a id="schemaoutputkinesis"></a>
<a id="schema_OutputKinesis"></a>
<a id="tocSoutputkinesis"></a>
<a id="tocsoutputkinesis"></a>

```json
{
  "id": "string",
  "type": "kinesis",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "streamName": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "concurrency": 5,
  "maxRecordSizeKB": 1024,
  "flushPeriodSec": 1,
  "compression": "none",
  "useListShards": false,
  "asNdjson": true,
  "onBackpressure": "block",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "maxEventsPerFlush": 500,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|streamName|string|true|none|Kinesis stream name to send events to.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|true|none|Region where the Kinesis stream is located|
|endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|concurrency|number|false|none|Maximum number of ongoing put requests before blocking.|
|maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|compression|string|false|none|Compression type to use for records|
|useListShards|boolean|false|none|Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details.|
|asNdjson|boolean|false|none|Batch events into a single record as NDJSON|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|maxEventsPerFlush|number|false|none|Maximum number of records to send in a single request|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kinesis|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|compression|none|
|compression|gzip|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputHoneycomb">OutputHoneycomb</h2>
<!-- backwards compatibility -->
<a id="schemaoutputhoneycomb"></a>
<a id="schema_OutputHoneycomb"></a>
<a id="tocSoutputhoneycomb"></a>
<a id="tocsoutputhoneycomb"></a>

```json
{
  "id": "string",
  "type": "honeycomb",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "dataset": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "authType": "manual",
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "team": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|dataset|string|true|none|Name of the dataset to send events to – e.g., observability|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Enter API key directly, or select a stored secret|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|team|string|false|none|Team API key where the dataset belongs|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|honeycomb|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputAzureEventhub">OutputAzureEventhub</h2>
<!-- backwards compatibility -->
<a id="schemaoutputazureeventhub"></a>
<a id="schema_OutputAzureEventhub"></a>
<a id="tocSoutputazureeventhub"></a>
<a id="tocsoutputazureeventhub"></a>

```json
{
  "id": "string",
  "type": "azure_eventhub",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "brokers": [
    "string"
  ],
  "topic": "string",
  "ack": 1,
  "format": "json",
  "maxRecordSizeKB": 768,
  "flushEventCount": 1000,
  "flushPeriodSec": 1,
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": false,
    "authType": "manual",
    "password": "string",
    "textSecret": "string",
    "mechanism": "plain",
    "username": "$ConnectionString",
    "clientSecretAuthType": "manual",
    "clientSecret": "string",
    "clientTextSecret": "string",
    "certificateName": "string",
    "certPath": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "oauthEndpoint": "https://login.microsoftonline.com",
    "clientId": "string",
    "tenantId": "string",
    "scope": "string"
  },
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true
  },
  "onBackpressure": "block",
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to, eg. yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|topic|string|true|none|The name of the Event Hub (Kafka Topic) to publish events. Can be overwritten using field __topicOut.|
|ack|integer|false|none|Control the number of required acknowledgments|
|format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» authType|string|false|none|Enter password directly, or select a stored secret|
|» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|» textSecret|string|false|none|Select or create a stored text secret|
|» mechanism|string|false|none|none|
|» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|» clientSecretAuthType|string|false|none|none|
|» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» certificateName|string|false|none|Select or create a stored certificate|
|» certPath|string|false|none|none|
|» privKeyPath|string|false|none|none|
|» passphrase|string|false|none|none|
|» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|» scope|string|false|none|Scope to pass in the OAuth request parameter|
|tls|object|false|none|none|
|» disabled|boolean|true|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|azure_eventhub|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputGoogleChronicle">OutputGoogleChronicle</h2>
<!-- backwards compatibility -->
<a id="schemaoutputgooglechronicle"></a>
<a id="schema_OutputGoogleChronicle"></a>
<a id="tocSoutputgooglechronicle"></a>
<a id="tocsoutputgooglechronicle"></a>

```json
{
  "id": "string",
  "type": "google_chronicle",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "apiVersion": "v1",
  "authenticationMethod": "manual",
  "responseRetrySettings": [
    {
      "httpStatus": 429,
      "initialBackoff": 30000,
      "backoffRate": 1,
      "maxBackoff": 30000
    },
    {
      "httpStatus": 500,
      "initialBackoff": 30000,
      "backoffRate": 1,
      "maxBackoff": 30000
    },
    {
      "httpStatus": 503,
      "initialBackoff": 30000,
      "backoffRate": 1,
      "maxBackoff": 30000
    }
  ],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "logFormatType": "unstructured",
  "region": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 1024,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 90,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "useRoundRobinDns": false,
  "onBackpressure": "block",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "extraLogTypes": [
    {
      "logType": "string",
      "description": "string"
    }
  ],
  "logType": "string",
  "logTextField": "string",
  "customerId": "string",
  "namespace": "string",
  "customLabels": [
    {
      "key": "string",
      "value": "string"
    }
  ],
  "udmType": "entities",
  "apiKey": "string",
  "apiKeySecret": "string",
  "serviceAccountCredentials": "string",
  "serviceAccountCredentialsSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|apiVersion|string|false|none|none|
|authenticationMethod|string|false|none|none|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|logFormatType|string|true|none|none|
|region|string|false|none|Regional endpoint to send events to|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|extraLogTypes|[object]|false|none|Custom log types. If the value "Custom" is selected in the setting "Default log type" above, the first custom log type in this table will be automatically selected as default log type.|
|» logType|string|true|none|none|
|» description|string|false|none|none|
|logType|string|false|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|customerId|string|false|none|A unique identifier (UUID) for your Google SecOps instance. This is provided by your Google representative and is required for API V2 authentication.|
|namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. Use namespace as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|customLabels|[object]|false|none|Custom labels to be added to every batch|
|» key|string|true|none|none|
|» value|string|true|none|none|
|udmType|string|false|none|Defines the specific format for UDM events sent to Google SecOps. This must match the type of UDM data being sent.|
|apiKey|string|false|none|Organization's API key in Google SecOps|
|apiKeySecret|string|false|none|Select or create a stored text secret|
|serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|google_chronicle|
|apiVersion|v1|
|apiVersion|v2|
|authenticationMethod|manual|
|authenticationMethod|secret|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|logFormatType|unstructured|
|logFormatType|udm|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|udmType|entities|
|udmType|logs|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputGoogleCloudStorage">OutputGoogleCloudStorage</h2>
<!-- backwards compatibility -->
<a id="schemaoutputgooglecloudstorage"></a>
<a id="schema_OutputGoogleCloudStorage"></a>
<a id="tocSoutputgooglecloudstorage"></a>
<a id="tocsoutputgooglecloudstorage"></a>

```json
{
  "id": "string",
  "type": "google_cloud_storage",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "bucket": "string",
  "region": "string",
  "endpoint": "https://storage.googleapis.com",
  "signatureVersion": "v2",
  "awsAuthenticationMethod": "auto",
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "destPath": "",
  "verifyPermissions": true,
  "objectACL": "private",
  "storageClass": "STANDARD",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "addIdToStagePath": true,
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "description": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20,
  "awsApiKey": "string",
  "awsSecretKey": "string",
  "awsSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|bucket|string|true|none|Name of the destination bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a Global Variable: `myBucket-${C.vars.myVar}`.|
|region|string|true|none|Region where the bucket is located|
|endpoint|string|true|none|Google Cloud Storage service endpoint|
|signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|awsAuthenticationMethod|string|false|none|none|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|description|string|false|none|none|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|
|awsApiKey|string|false|none|HMAC access key. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|awsSecretKey|string|false|none|HMAC secret. This value can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

#### Enumerated Values

|Property|Value|
|---|---|
|type|google_cloud_storage|
|signatureVersion|v2|
|signatureVersion|v4|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputGoogleCloudLogging">OutputGoogleCloudLogging</h2>
<!-- backwards compatibility -->
<a id="schemaoutputgooglecloudlogging"></a>
<a id="schema_OutputGoogleCloudLogging"></a>
<a id="tocSoutputgooglecloudlogging"></a>
<a id="tocsoutputgooglecloudlogging"></a>

```json
{
  "id": "string",
  "type": "google_cloud_logging",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "logLocationType": "project",
  "logNameExpression": "string",
  "sanitizeLogNames": false,
  "payloadFormat": "text",
  "logLabels": [
    {
      "label": "string",
      "valueExpression": "string"
    }
  ],
  "resourceTypeExpression": "string",
  "resourceTypeLabels": [
    {
      "label": "string",
      "valueExpression": "string"
    }
  ],
  "severityExpression": "string",
  "insertIdExpression": "string",
  "googleAuthMethod": "auto",
  "serviceAccountCredentials": "string",
  "secret": "string",
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "flushPeriodSec": 1,
  "concurrency": 5,
  "connectionTimeout": 10000,
  "timeoutSec": 30,
  "throttleRateReqPerSec": 2000,
  "requestMethodExpression": "string",
  "requestUrlExpression": "string",
  "requestSizeExpression": "string",
  "statusExpression": "string",
  "responseSizeExpression": "string",
  "userAgentExpression": "string",
  "remoteIpExpression": "string",
  "serverIpExpression": "string",
  "refererExpression": "string",
  "latencyExpression": "string",
  "cacheLookupExpression": "string",
  "cacheHitExpression": "string",
  "cacheValidatedExpression": "string",
  "cacheFillBytesExpression": "string",
  "protocolExpression": "string",
  "idExpression": "string",
  "producerExpression": "string",
  "firstExpression": "string",
  "lastExpression": "string",
  "fileExpression": "string",
  "lineExpression": "string",
  "functionExpression": "string",
  "uidExpression": "string",
  "indexExpression": "string",
  "totalSplitsExpression": "string",
  "traceExpression": "string",
  "spanIdExpression": "string",
  "traceSampledExpression": "string",
  "onBackpressure": "block",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "logLocationExpression": "string",
  "payloadExpression": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|logLocationType|string|true|none|none|
|logNameExpression|string|true|none|JavaScript expression to compute the value of the log name. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|sanitizeLogNames|boolean|false|none|none|
|payloadFormat|string|false|none|Format to use when sending payload. Defaults to Text.|
|logLabels|[object]|false|none|Labels to apply to the log entry|
|» label|string|true|none|Label name|
|» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|resourceTypeExpression|string|false|none|JavaScript expression to compute the value of the managed resource type field. Must evaluate to one of the valid values [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types). Defaults to "global".|
|resourceTypeLabels|[object]|false|none|Labels to apply to the managed resource. These must correspond to the valid labels for the specified resource type (see [here](https://cloud.google.com/logging/docs/api/v2/resource-list#resource-types)). Otherwise, they will be dropped by Google Cloud Logging.|
|» label|string|true|none|Label name|
|» valueExpression|string|true|none|JavaScript expression to compute the label's value.|
|severityExpression|string|false|none|JavaScript expression to compute the value of the severity field. Must evaluate to one of the severity values supported by Google Cloud Logging [here](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) (case insensitive). Defaults to "DEFAULT".|
|insertIdExpression|string|false|none|JavaScript expression to compute the value of the insert ID field.|
|googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|secret|string|false|none|Select or create a stored text secret|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body.|
|maxPayloadEvents|number|false|none|Max number of events to include in the request body. Default is 0 (unlimited).|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it.|
|throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second.|
|requestMethodExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|requestUrlExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request URL as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|requestSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|statusExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request method as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|responseSizeExpression|string|false|none|A JavaScript expression that evaluates to the HTTP response size as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|userAgentExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request user agent as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|remoteIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request remote IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|serverIpExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request server IP as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|refererExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request referer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|latencyExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request latency, formatted as <seconds>.<nanoseconds>s (for example, 1.23s). See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|cacheLookupExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache lookup as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|cacheHitExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache hit as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|cacheValidatedExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache validated with origin server as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|cacheFillBytesExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request cache fill bytes as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|protocolExpression|string|false|none|A JavaScript expression that evaluates to the HTTP request protocol as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#httprequest) for details.|
|idExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation ID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|producerExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation producer as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|firstExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation first flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|lastExpression|string|false|none|A JavaScript expression that evaluates to the log entry operation last flag as a boolean. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentryoperation) for details.|
|fileExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location file as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|lineExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location line as a string, in int64 format. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|functionExpression|string|false|none|A JavaScript expression that evaluates to the log entry source location function as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation) for details.|
|uidExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split UID as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|indexExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split index as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|totalSplitsExpression|string|false|none|A JavaScript expression that evaluates to the log entry log split total splits as a number. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logsplit) for details.|
|traceExpression|string|false|none|A JavaScript expression that evaluates to the REST resource name of the trace being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|spanIdExpression|string|false|none|A JavaScript expression that evaluates to the ID of the cloud trace span associated with the current operation in which the log is being written as a string. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|traceSampledExpression|string|false|none|A JavaScript expression that evaluates to the the sampling decision of the span associated with the log entry. See the [documentation](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry) for details.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|logLocationExpression|string|true|none|JavaScript expression to compute the value of the folder ID with which log entries should be associated. If Validate and correct log name is enabled, invalid characters (characters other than alphanumerics, forward-slashes, underscores, hyphens, and periods) will be replaced with an underscore.|
|payloadExpression|string|false|none|JavaScript expression to compute the value of the payload. Must evaluate to a JavaScript object value. If an invalid value is encountered it will result in the default value instead. Defaults to the entire event.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|google_cloud_logging|
|logLocationType|project|
|logLocationType|organization|
|logLocationType|billingAccount|
|logLocationType|folder|
|payloadFormat|text|
|payloadFormat|json|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputGooglePubsub">OutputGooglePubsub</h2>
<!-- backwards compatibility -->
<a id="schemaoutputgooglepubsub"></a>
<a id="schema_OutputGooglePubsub"></a>
<a id="tocSoutputgooglepubsub"></a>
<a id="tocsoutputgooglepubsub"></a>

```json
{
  "id": "string",
  "type": "google_pubsub",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "topicName": "string",
  "createTopic": false,
  "orderedDelivery": false,
  "region": "string",
  "googleAuthMethod": "auto",
  "serviceAccountCredentials": "string",
  "secret": "string",
  "batchSize": 1000,
  "batchTimeout": 100,
  "maxQueueSize": 100,
  "maxRecordSizeKB": 256,
  "flushPeriod": 1,
  "maxInProgress": 10,
  "onBackpressure": "block",
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|topicName|string|true|none|ID of the topic to send events to.|
|createTopic|boolean|false|none|If enabled, create topic if it does not exist.|
|orderedDelivery|boolean|false|none|If enabled, send events in the order they were added to the queue. For this to work correctly, the process receiving events must have ordering enabled.|
|region|string|false|none|Region to publish messages to. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|secret|string|false|none|Select or create a stored text secret|
|batchSize|number|false|none|The maximum number of items the Google API should batch before it sends them to the topic.|
|batchTimeout|number|false|none|The maximum amount of time, in milliseconds, that the Google API should wait to send a batch (if the Batch size is not reached).|
|maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send.|
|flushPeriod|number|false|none|Maximum time to wait before sending a batch (when batch size limit is not reached)|
|maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|google_pubsub|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputExabeam">OutputExabeam</h2>
<!-- backwards compatibility -->
<a id="schemaoutputexabeam"></a>
<a id="schema_OutputExabeam"></a>
<a id="tocSoutputexabeam"></a>
<a id="tocsoutputexabeam"></a>

```json
{
  "id": "string",
  "type": "exabeam",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "bucket": "string",
  "region": "string",
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "endpoint": "https://storage.googleapis.com",
  "signatureVersion": "v2",
  "objectACL": "private",
  "storageClass": "STANDARD",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "addIdToStagePath": true,
  "removeEmptyDirs": true,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxOpenFiles": 100,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "maxFileSizeMB": 10,
  "encodedConfiguration": "string",
  "collectorInstanceId": "string",
  "siteName": "string",
  "siteId": "string",
  "timezoneOffset": "string",
  "awsApiKey": "string",
  "awsSecretKey": "string",
  "description": "string",
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|bucket|string|true|none|Name of the destination bucket. A constant or a JavaScript expression that can only be evaluated at init time. Example of referencing a JavaScript Global Variable: `myBucket-${C.vars.myVar}`.|
|region|string|true|none|Region where the bucket is located|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|endpoint|string|true|none|Google Cloud Storage service endpoint|
|signatureVersion|string|false|none|Signature version to use for signing Google Cloud Storage requests|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|encodedConfiguration|string|false|none|Enter an encoded string containing Exabeam configurations|
|collectorInstanceId|string|true|none|ID of the Exabeam Collector where data should be sent. Example: 11112222-3333-4444-5555-666677778888|
|siteName|string|false|none|Constant or JavaScript expression to create an Exabeam site name. Values that aren't successfully evaluated will be treated as string constants.|
|siteId|string|false|none|Exabeam site ID. If left blank, @{product} will use the value of the Exabeam site name.|
|timezoneOffset|string|false|none|none|
|awsApiKey|string|false|none|HMAC access key. Can be a constant or a JavaScript expression, such as `${C.env.GCS_ACCESS_KEY}`.|
|awsSecretKey|string|false|none|HMAC secret. Can be a constant or a JavaScript expression, such as `${C.env.GCS_SECRET}`.|
|description|string|false|none|none|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|exabeam|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|objectACL|project-private|
|objectACL|authenticated-read|
|objectACL|public-read|
|storageClass|STANDARD|
|storageClass|NEARLINE|
|storageClass|COLDLINE|
|storageClass|ARCHIVE|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|

<h2 id="tocS_OutputKafka">OutputKafka</h2>
<!-- backwards compatibility -->
<a id="schemaoutputkafka"></a>
<a id="schema_OutputKafka"></a>
<a id="tocSoutputkafka"></a>
<a id="tocsoutputkafka"></a>

```json
{
  "id": "string",
  "type": "kafka",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "brokers": [
    "string"
  ],
  "topic": "string",
  "ack": 1,
  "format": "json",
  "compression": "none",
  "maxRecordSizeKB": 768,
  "flushEventCount": 1000,
  "flushPeriodSec": 1,
  "kafkaSchemaRegistry": {
    "disabled": true,
    "schemaRegistryURL": "http://localhost:8081",
    "connectionTimeout": 30000,
    "requestTimeout": 30000,
    "maxRetries": 1,
    "auth": {
      "disabled": true,
      "credentialsSecret": "string"
    },
    "tls": {
      "disabled": true,
      "rejectUnauthorized": true,
      "servername": "string",
      "certificateName": "string",
      "caPath": "string",
      "privKeyPath": "string",
      "certPath": "string",
      "passphrase": "string",
      "minVersion": "TLSv1",
      "maxVersion": "TLSv1"
    },
    "defaultKeySchemaId": 0,
    "defaultValueSchemaId": 0
  },
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": true,
    "username": "string",
    "password": "string",
    "authType": "manual",
    "credentialsSecret": "string",
    "mechanism": "plain",
    "keytabLocation": "string",
    "principal": "string",
    "brokerServiceClass": "string",
    "oauthEnabled": false,
    "tokenUrl": "string",
    "clientId": "string",
    "oauthSecretType": "secret",
    "clientTextSecret": "string",
    "oauthParams": [
      {
        "name": "string",
        "value": "string"
      }
    ],
    "saslExtensions": [
      {
        "name": "string",
        "value": "string"
      }
    ]
  },
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "onBackpressure": "block",
  "description": "string",
  "protobufLibraryId": "string",
  "protobufEncodingId": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|ack|integer|false|none|Control the number of required acknowledgments.|
|format|string|false|none|Format to use to serialize events before writing to Kafka.|
|compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|kafkaSchemaRegistry|object|false|none|none|
|» disabled|boolean|true|none|none|
|» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»» disabled|boolean|true|none|none|
|»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» tls|object|false|none|none|
|»» disabled|boolean|false|none|none|
|»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»» certificateName|string|false|none|The name of the predefined certificate|
|»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»» minVersion|string|false|none|none|
|»» maxVersion|string|false|none|none|
|» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» mechanism|string|false|none|none|
|» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|» clientId|string|false|none|Client ID to use for OAuth authentication|
|» oauthSecretType|string|false|none|none|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kafka|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputConfluentCloud">OutputConfluentCloud</h2>
<!-- backwards compatibility -->
<a id="schemaoutputconfluentcloud"></a>
<a id="schema_OutputConfluentCloud"></a>
<a id="tocSoutputconfluentcloud"></a>
<a id="tocsoutputconfluentcloud"></a>

```json
{
  "id": "string",
  "type": "confluent_cloud",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "brokers": [
    "string"
  ],
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "topic": "string",
  "ack": 1,
  "format": "json",
  "compression": "none",
  "maxRecordSizeKB": 768,
  "flushEventCount": 1000,
  "flushPeriodSec": 1,
  "kafkaSchemaRegistry": {
    "disabled": true,
    "schemaRegistryURL": "http://localhost:8081",
    "connectionTimeout": 30000,
    "requestTimeout": 30000,
    "maxRetries": 1,
    "auth": {
      "disabled": true,
      "credentialsSecret": "string"
    },
    "tls": {
      "disabled": true,
      "rejectUnauthorized": true,
      "servername": "string",
      "certificateName": "string",
      "caPath": "string",
      "privKeyPath": "string",
      "certPath": "string",
      "passphrase": "string",
      "minVersion": "TLSv1",
      "maxVersion": "TLSv1"
    },
    "defaultKeySchemaId": 0,
    "defaultValueSchemaId": 0
  },
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": true,
    "username": "string",
    "password": "string",
    "authType": "manual",
    "credentialsSecret": "string",
    "mechanism": "plain",
    "keytabLocation": "string",
    "principal": "string",
    "brokerServiceClass": "string",
    "oauthEnabled": false,
    "tokenUrl": "string",
    "clientId": "string",
    "oauthSecretType": "secret",
    "clientTextSecret": "string",
    "oauthParams": [
      {
        "name": "string",
        "value": "string"
      }
    ],
    "saslExtensions": [
      {
        "name": "string",
        "value": "string"
      }
    ]
  },
  "onBackpressure": "block",
  "description": "string",
  "protobufLibraryId": "string",
  "protobufEncodingId": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|ack|integer|false|none|Control the number of required acknowledgments.|
|format|string|false|none|Format to use to serialize events before writing to Kafka.|
|compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|kafkaSchemaRegistry|object|false|none|none|
|» disabled|boolean|true|none|none|
|» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»» disabled|boolean|true|none|none|
|»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» tls|object|false|none|none|
|»» disabled|boolean|false|none|none|
|»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»» certificateName|string|false|none|The name of the predefined certificate|
|»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»» minVersion|string|false|none|none|
|»» maxVersion|string|false|none|none|
|» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» mechanism|string|false|none|none|
|» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|» clientId|string|false|none|Client ID to use for OAuth authentication|
|» oauthSecretType|string|false|none|none|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|confluent_cloud|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputMsk">OutputMsk</h2>
<!-- backwards compatibility -->
<a id="schemaoutputmsk"></a>
<a id="schema_OutputMsk"></a>
<a id="tocSoutputmsk"></a>
<a id="tocsoutputmsk"></a>

```json
{
  "id": "string",
  "type": "msk",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "brokers": [
    "string"
  ],
  "topic": "string",
  "ack": 1,
  "format": "json",
  "compression": "none",
  "maxRecordSizeKB": 768,
  "flushEventCount": 1000,
  "flushPeriodSec": 1,
  "kafkaSchemaRegistry": {
    "disabled": true,
    "schemaRegistryURL": "http://localhost:8081",
    "connectionTimeout": 30000,
    "requestTimeout": 30000,
    "maxRetries": 1,
    "auth": {
      "disabled": true,
      "credentialsSecret": "string"
    },
    "tls": {
      "disabled": true,
      "rejectUnauthorized": true,
      "servername": "string",
      "certificateName": "string",
      "caPath": "string",
      "privKeyPath": "string",
      "certPath": "string",
      "passphrase": "string",
      "minVersion": "TLSv1",
      "maxVersion": "TLSv1"
    },
    "defaultKeySchemaId": 0,
    "defaultValueSchemaId": 0
  },
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "onBackpressure": "block",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "protobufLibraryId": "string",
  "protobufEncodingId": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify hostname and port, e.g., mykafkabroker:9092, or just hostname, in which case @{product} will assign port 9092.|
|topic|string|true|none|The topic to publish events to. Can be overridden using the __topicOut field.|
|ack|integer|false|none|Control the number of required acknowledgments.|
|format|string|false|none|Format to use to serialize events before writing to Kafka.|
|compression|string|false|none|Codec to use to compress the data before sending to Kafka|
|maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. The value must not exceed the Kafka brokers' message.max.bytes setting.|
|flushEventCount|number|false|none|The maximum number of events you want the Destination to allow in a batch before forcing a flush|
|flushPeriodSec|number|false|none|The maximum amount of time you want the Destination to wait before forcing a flush. Shorter intervals tend to result in smaller batches being sent.|
|kafkaSchemaRegistry|object|false|none|none|
|» disabled|boolean|true|none|none|
|» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»» disabled|boolean|true|none|none|
|»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» tls|object|false|none|none|
|»» disabled|boolean|false|none|none|
|»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»» certificateName|string|false|none|The name of the predefined certificate|
|»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»» minVersion|string|false|none|none|
|»» maxVersion|string|false|none|none|
|» defaultKeySchemaId|number|false|none|Used when __keySchemaIdOut is not present, to transform key values, leave blank if key transformation is not required by default.|
|» defaultValueSchemaId|number|false|none|Used when __valueSchemaIdOut is not present, to transform _raw, leave blank if value transformation is not required by default.|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|true|none|Region where the MSK cluster is located|
|endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|protobufLibraryId|string|false|none|Select a set of Protobuf definitions for the events you want to send|
|protobufEncodingId|string|false|none|Select the type of object you want the Protobuf definitions to use for event encoding|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|msk|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|format|protobuf|
|compression|none|
|compression|gzip|
|compression|snappy|
|compression|lz4|
|compression|zstd|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputElastic">OutputElastic</h2>
<!-- backwards compatibility -->
<a id="schemaoutputelastic"></a>
<a id="schema_OutputElastic"></a>
<a id="tocSoutputelastic"></a>
<a id="tocsoutputelastic"></a>

```json
{
  "id": "string",
  "type": "elastic",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "loadBalanced": true,
  "index": "string",
  "docType": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "extraParams": [
    {
      "name": "filter_path",
      "value": "errors,items.*.error,items.*._index,items.*.status"
    }
  ],
  "auth": {
    "disabled": true,
    "username": "string",
    "password": "string",
    "authType": "manual",
    "credentialsSecret": "string",
    "manualAPIKey": "string",
    "textSecret": "string"
  },
  "elasticVersion": "auto",
  "elasticPipeline": "string",
  "includeDocId": false,
  "writeAction": "index",
  "retryPartialErrors": false,
  "onBackpressure": "block",
  "description": "string",
  "url": "string",
  "useRoundRobinDns": false,
  "excludeSelf": false,
  "urls": [
    {
      "url": "string",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|index|string|true|none|Index or data stream to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|docType|string|false|none|Document type to use for events. Can be overwritten by an event's __type field.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|extraParams|[object]|false|none|none|
|» name|string|true|none|none|
|» value|string|true|none|none|
|auth|object|false|none|none|
|» disabled|boolean|true|none|none|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» manualAPIKey|string|false|none|Enter API key directly|
|» textSecret|string|false|none|Select or create a stored text secret|
|elasticVersion|string|false|none|Optional Elasticsearch version, used to format events. If not specified, will auto-discover version.|
|elasticPipeline|string|false|none|Optional Elasticsearch destination pipeline|
|includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|writeAction|string|false|none|Action to use when writing events. Must be set to `Create` when writing to a data stream.|
|retryPartialErrors|boolean|false|none|Retry failed events when a bulk request to Elastic is successful, but the response body returns an error for one or more events in the batch|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|url|string|false|none|The Cloud ID or URL to an Elastic cluster to send events to. Example: http://elastic:9200/_bulk|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|urls|[object]|false|none|none|
|» url|string|true|none|The URL to an Elastic node to send events to. Example: http://elastic:9200/_bulk|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|elastic|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|elasticVersion|auto|
|elasticVersion|6|
|elasticVersion|7|
|writeAction|index|
|writeAction|create|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputElasticCloud">OutputElasticCloud</h2>
<!-- backwards compatibility -->
<a id="schemaoutputelasticcloud"></a>
<a id="schema_OutputElasticCloud"></a>
<a id="tocSoutputelasticcloud"></a>
<a id="tocsoutputelasticcloud"></a>

```json
{
  "id": "string",
  "type": "elastic_cloud",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "index": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "extraParams": [
    {
      "name": "filter_path",
      "value": "errors,items.*.error,items.*._index,items.*.status"
    }
  ],
  "auth": {
    "disabled": false,
    "username": "string",
    "password": "string",
    "authType": "manual",
    "credentialsSecret": "string",
    "manualAPIKey": "string",
    "textSecret": "string"
  },
  "elasticPipeline": "string",
  "includeDocId": true,
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|Enter Cloud ID of the Elastic Cloud environment to send events to|
|index|string|true|none|Data stream or index to send events to. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be overwritten by an event's __index field.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|extraParams|[object]|false|none|Extra parameters to use in HTTP requests|
|» name|string|true|none|none|
|» value|string|true|none|none|
|auth|object|false|none|none|
|» disabled|boolean|true|none|none|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» manualAPIKey|string|false|none|Enter API key directly|
|» textSecret|string|false|none|Select or create a stored text secret|
|elasticPipeline|string|false|none|Optional Elastic Cloud Destination pipeline|
|includeDocId|boolean|false|none|Include the `document_id` field when sending events to an Elastic TSDS (time series data stream)|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|elastic_cloud|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|authType|manualAPIKey|
|authType|textSecret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputNewrelic">OutputNewrelic</h2>
<!-- backwards compatibility -->
<a id="schemaoutputnewrelic"></a>
<a id="schema_OutputNewrelic"></a>
<a id="tocSoutputnewrelic"></a>
<a id="tocsoutputnewrelic"></a>

```json
{
  "id": "string",
  "type": "newrelic",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "region": "US",
  "logType": "",
  "messageField": "",
  "metadata": [
    {
      "name": "service",
      "value": "string"
    }
  ],
  "concurrency": 5,
  "maxPayloadSizeKB": 1024,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "onBackpressure": "block",
  "authType": "manual",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "customUrl": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "apiKey": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|region|string|false|none|Which New Relic region endpoint to use.|
|logType|string|false|none|Name of the logtype to send with events, e.g.: observability, access_log. The event's 'sourcetype' field (if set) will override this value.|
|messageField|string|false|none|Name of field to send as log message value. If not present, event will be serialized and sent as JSON.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Enter API key directly, or select a stored secret|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|customUrl|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|newrelic|
|region|US|
|region|EU|
|region|Custom|
|name|service|
|name|hostname|
|name|timestamp|
|name|auditId|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputNewrelicEvents">OutputNewrelicEvents</h2>
<!-- backwards compatibility -->
<a id="schemaoutputnewrelicevents"></a>
<a id="schema_OutputNewrelicEvents"></a>
<a id="tocSoutputnewrelicevents"></a>
<a id="tocsoutputnewrelicevents"></a>

```json
{
  "id": "string",
  "type": "newrelic_events",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "region": "US",
  "accountId": "string",
  "eventType": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 1024,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "authType": "manual",
  "description": "string",
  "customUrl": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "apiKey": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|region|string|false|none|Which New Relic region endpoint to use.|
|accountId|string|true|none|New Relic account ID|
|eventType|string|true|none|Default eventType to use when not present in an event. For more information, see [here](https://docs.newrelic.com/docs/telemetry-data-platform/custom-data/custom-events/data-requirements-limits-custom-event-data/#reserved-words).|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Enter API key directly, or select a stored secret|
|description|string|false|none|none|
|customUrl|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|apiKey|string|false|none|New Relic API key. Can be overridden using __newRelic_apiKey field.|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|newrelic_events|
|region|US|
|region|EU|
|region|Custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputInfluxdb">OutputInfluxdb</h2>
<!-- backwards compatibility -->
<a id="schemaoutputinfluxdb"></a>
<a id="schema_OutputInfluxdb"></a>
<a id="tocSoutputinfluxdb"></a>
<a id="tocsoutputinfluxdb"></a>

```json
{
  "id": "string",
  "type": "influxdb",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "useV2API": false,
  "timestampPrecision": "ns",
  "dynamicValueFieldName": true,
  "valueFieldName": "value",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "authType": "none",
  "description": "string",
  "database": "string",
  "bucket": "string",
  "org": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|URL of an InfluxDB cluster to send events to, e.g., http://localhost:8086/write|
|useV2API|boolean|false|none|The v2 API can be enabled with InfluxDB versions 1.8 and later.|
|timestampPrecision|string|false|none|Sets the precision for the supplied Unix time values. Defaults to milliseconds.|
|dynamicValueFieldName|boolean|false|none|Enabling this will pull the value field from the metric name. E,g, 'db.query.user' will use 'db.query' as the measurement and 'user' as the value field.|
|valueFieldName|string|false|none|Name of the field in which to store the metric when sending to InfluxDB. If dynamic generation is enabled and fails, this will be used as a fallback.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|InfluxDB authentication type|
|description|string|false|none|none|
|database|string|false|none|Database to write to.|
|bucket|string|false|none|Bucket to write to.|
|org|string|false|none|Organization ID for this bucket.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|

#### Enumerated Values

|Property|Value|
|---|---|
|type|influxdb|
|timestampPrecision|ns|
|timestampPrecision|u|
|timestampPrecision|ms|
|timestampPrecision|s|
|timestampPrecision|m|
|timestampPrecision|h|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputCloudwatch">OutputCloudwatch</h2>
<!-- backwards compatibility -->
<a id="schemaoutputcloudwatch"></a>
<a id="schema_OutputCloudwatch"></a>
<a id="tocSoutputcloudwatch"></a>
<a id="tocsoutputcloudwatch"></a>

```json
{
  "id": "string",
  "type": "cloudwatch",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "logGroupName": "string",
  "logStreamName": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "maxQueueSize": 5,
  "maxRecordSizeKB": 1024,
  "flushPeriodSec": 1,
  "onBackpressure": "block",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|logGroupName|string|true|none|CloudWatch log group to associate events with|
|logStreamName|string|true|none|Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|true|none|Region where the CloudWatchLogs is located|
|endpoint|string|false|none|CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint.|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access CloudWatchLogs|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|maxQueueSize|number|false|none|Maximum number of queued batches before blocking|
|maxRecordSizeKB|number|false|none|Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cloudwatch|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputMinio">OutputMinio</h2>
<!-- backwards compatibility -->
<a id="schemaoutputminio"></a>
<a id="schema_OutputMinio"></a>
<a id="tocSoutputminio"></a>
<a id="tocsoutputminio"></a>

```json
{
  "id": "string",
  "type": "minio",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "endpoint": "string",
  "bucket": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "destPath": "string",
  "signatureVersion": "v2",
  "objectACL": "private",
  "storageClass": "STANDARD",
  "serverSideEncryption": "AES256",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "verifyPermissions": true,
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxConcurrentFileParts": 4,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|endpoint|string|true|none|MinIO service url (e.g. http://minioHost:9000)|
|bucket|string|true|none|Name of the destination MinIO bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|region|string|false|none|Region where the MinIO service/cluster is located|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|description|string|false|none|none|
|awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|minio|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputStatsd">OutputStatsd</h2>
<!-- backwards compatibility -->
<a id="schemaoutputstatsd"></a>
<a id="schema_OutputStatsd"></a>
<a id="tocSoutputstatsd"></a>
<a id="tocsoutputstatsd"></a>

```json
{
  "id": "string",
  "type": "statsd",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "protocol": "udp",
  "host": "string",
  "port": 8125,
  "mtu": 512,
  "flushPeriodSec": 1,
  "dnsResolvePeriodSec": 0,
  "description": "string",
  "throttleRatePerSec": "0",
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "onBackpressure": "block",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|protocol|string|true|none|Protocol to use when communicating with the destination.|
|host|string|true|none|The hostname of the destination.|
|port|number|true|none|Destination port.|
|mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|description|string|false|none|none|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|statsd|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputStatsdExt">OutputStatsdExt</h2>
<!-- backwards compatibility -->
<a id="schemaoutputstatsdext"></a>
<a id="schema_OutputStatsdExt"></a>
<a id="tocSoutputstatsdext"></a>
<a id="tocsoutputstatsdext"></a>

```json
{
  "id": "string",
  "type": "statsd_ext",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "protocol": "udp",
  "host": "string",
  "port": 8125,
  "mtu": 512,
  "flushPeriodSec": 1,
  "dnsResolvePeriodSec": 0,
  "description": "string",
  "throttleRatePerSec": "0",
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "onBackpressure": "block",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|protocol|string|true|none|Protocol to use when communicating with the destination.|
|host|string|true|none|The hostname of the destination.|
|port|number|true|none|Destination port.|
|mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|description|string|false|none|none|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|statsd_ext|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputGraphite">OutputGraphite</h2>
<!-- backwards compatibility -->
<a id="schemaoutputgraphite"></a>
<a id="schema_OutputGraphite"></a>
<a id="tocSoutputgraphite"></a>
<a id="tocsoutputgraphite"></a>

```json
{
  "id": "string",
  "type": "graphite",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "protocol": "udp",
  "host": "string",
  "port": 8125,
  "mtu": 512,
  "flushPeriodSec": 1,
  "dnsResolvePeriodSec": 0,
  "description": "string",
  "throttleRatePerSec": "0",
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "onBackpressure": "block",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|protocol|string|true|none|Protocol to use when communicating with the destination.|
|host|string|true|none|The hostname of the destination.|
|port|number|true|none|Destination port.|
|mtu|number|false|none|When protocol is UDP, specifies the maximum size of packets sent to the destination. Also known as the MTU for the network path to the destination system.|
|flushPeriodSec|number|false|none|When protocol is TCP, specifies how often buffers should be flushed, resulting in records sent to the destination.|
|dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every batch sent will incur a DNS lookup.|
|description|string|false|none|none|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|graphite|
|protocol|udp|
|protocol|tcp|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputRouter">OutputRouter</h2>
<!-- backwards compatibility -->
<a id="schemaoutputrouter"></a>
<a id="schema_OutputRouter"></a>
<a id="tocSoutputrouter"></a>
<a id="tocsoutputrouter"></a>

```json
{
  "id": "string",
  "type": "router",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "rules": [
    {
      "filter": "string",
      "output": "string",
      "description": "string",
      "final": true
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|rules|[object]|true|none|Event routing rules|
|» filter|string|true|none|JavaScript expression to select events to send to output|
|» output|string|true|none|Output to send matching events to|
|» description|string|false|none|Description of this rule's purpose|
|» final|boolean|false|none|Flag to control whether to stop the event from being checked against other rules|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|router|

<h2 id="tocS_OutputSns">OutputSns</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsns"></a>
<a id="schema_OutputSns"></a>
<a id="tocSoutputsns"></a>
<a id="tocsoutputsns"></a>

```json
{
  "id": "string",
  "type": "sns",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "topicArn": "string",
  "messageGroupId": "string",
  "maxRetries": 0,
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "onBackpressure": "block",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|topicArn|string|true|none|The ARN of the SNS topic to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. E.g., 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`|
|messageGroupId|string|true|none|Messages in the same group are processed in a FIFO manner. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|maxRetries|number|false|none|Maximum number of retries before the output returns an error. Note that not all errors are retryable. The retries use an exponential backoff policy.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|Region where the SNS is located|
|endpoint|string|false|none|SNS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SNS-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing SNS requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SNS|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|sns|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSqs">OutputSqs</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsqs"></a>
<a id="schema_OutputSqs"></a>
<a id="tocSoutputsqs"></a>
<a id="tocsoutputsqs"></a>

```json
{
  "id": "string",
  "type": "sqs",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "queueName": "string",
  "queueType": "standard",
  "awsAccountId": "string",
  "messageGroupId": "cribl",
  "createQueue": true,
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "maxQueueSize": 100,
  "maxRecordSizeKB": 256,
  "flushPeriodSec": 1,
  "maxInProgress": 10,
  "onBackpressure": "block",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|queueName|string|true|none|The name, URL, or ARN of the SQS queue to send events to. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|queueType|string|true|none|The queue type used (or created). Defaults to Standard.|
|awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|messageGroupId|string|false|none|This parameter applies only to FIFO queues. The tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are processed in a FIFO manner. Use event field __messageGroupId to override this value.|
|createQueue|boolean|false|none|Create queue if it does not exist.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|maxQueueSize|number|false|none|Maximum number of queued batches before blocking.|
|maxRecordSizeKB|number|false|none|Maximum size (KB) of batches to send. Per the SQS spec, the max allowed value is 256 KB.|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|maxInProgress|number|false|none|The maximum number of in-progress API requests before backpressure is applied.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|sqs|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSnmp">OutputSnmp</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsnmp"></a>
<a id="schema_OutputSnmp"></a>
<a id="tocSoutputsnmp"></a>
<a id="tocsoutputsnmp"></a>

```json
{
  "id": "string",
  "type": "snmp",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "hosts": [
    {
      "host": "string",
      "port": 162
    }
  ],
  "dnsResolvePeriodSec": 0,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|hosts|[object]|true|none|One or more SNMP destinations to forward traps to|
|» host|string|true|none|Destination host|
|» port|number|true|none|Destination port, default is 162|
|dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every trap sent will incur a DNS lookup.|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|snmp|

<h2 id="tocS_OutputSumoLogic">OutputSumoLogic</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsumologic"></a>
<a id="schema_OutputSumoLogic"></a>
<a id="tocSoutputsumologic"></a>
<a id="tocsoutputsumologic"></a>

```json
{
  "id": "string",
  "type": "sumo_logic",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "customSource": "string",
  "customCategory": "string",
  "format": "json",
  "concurrency": 5,
  "maxPayloadSizeKB": 1024,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "onBackpressure": "block",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|Sumo Logic HTTP collector URL to which events should be sent|
|customSource|string|false|none|Override the source name configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceName field.|
|customCategory|string|false|none|Override the source category configured on the Sumo Logic HTTP collector. This can also be overridden at the event level with the __sourceCategory field.|
|format|string|false|none|Preserve the raw event format instead of JSONifying it|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|sumo_logic|
|format|json|
|format|raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputDatadog">OutputDatadog</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdatadog"></a>
<a id="schema_OutputDatadog"></a>
<a id="tocSoutputdatadog"></a>
<a id="tocsoutputdatadog"></a>

```json
{
  "id": "string",
  "type": "datadog",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "contentType": "text",
  "message": "string",
  "source": "string",
  "host": "string",
  "service": "string",
  "tags": [],
  "batchByTags": true,
  "allowApiKeyFromEvents": false,
  "severity": "emergency",
  "site": "us",
  "sendCountersAsCount": false,
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "onBackpressure": "block",
  "authType": "manual",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "customUrl": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "apiKey": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|contentType|string|false|none|The content type to use when sending logs|
|message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|source|string|false|none|Name of the source to send with logs. When you send logs as JSON objects, the event's 'source' field (if set) will override this value.|
|host|string|false|none|Name of the host to send with logs. When you send logs as JSON objects, the event's 'host' field (if set) will override this value.|
|service|string|false|none|Name of the service to send with logs. When you send logs as JSON objects, the event's '__service' field (if set) will override this value.|
|tags|[string]|false|none|List of tags to send with logs, such as 'env:prod' and 'env_staging:east'|
|batchByTags|boolean|false|none|Batch events by API key and the ddtags field on the event. When disabled, batches events only by API key. If incoming events have high cardinality in the ddtags field, disabling this setting may improve Destination performance.|
|allowApiKeyFromEvents|boolean|false|none|Allow API key to be set from the event's '__agent_api_key' field|
|severity|string|false|none|Default value for message severity. When you send logs as JSON objects, the event's '__severity' field (if set) will override this value.|
|site|string|false|none|Datadog site to which events should be sent|
|sendCountersAsCount|boolean|false|none|If not enabled, Datadog will transform 'counter' metrics to 'gauge'. [Learn more about Datadog metrics types.](https://docs.datadoghq.com/metrics/types/?tab=count)|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Enter API key directly, or select a stored secret|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|customUrl|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|apiKey|string|false|none|Organization's API key in Datadog|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|datadog|
|contentType|text|
|contentType|json|
|severity|emergency|
|severity|alert|
|severity|critical|
|severity|error|
|severity|warning|
|severity|notice|
|severity|info|
|severity|debug|
|site|us|
|site|us3|
|site|us5|
|site|eu|
|site|fed1|
|site|ap1|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputGrafanaCloud">OutputGrafanaCloud</h2>
<!-- backwards compatibility -->
<a id="schemaoutputgrafanacloud"></a>
<a id="schema_OutputGrafanaCloud"></a>
<a id="tocSoutputgrafanacloud"></a>
<a id="tocsoutputgrafanacloud"></a>

```json
{
  "id": "string",
  "type": "grafana_cloud",
  "pipeline": "string",
  "systemFields": [
    "cribl_host",
    "cribl_wp"
  ],
  "environment": "string",
  "streamtags": [],
  "lokiUrl": "string",
  "prometheusUrl": "string",
  "message": "string",
  "messageFormat": "protobuf",
  "labels": [
    {
      "name": "",
      "value": "string"
    }
  ],
  "metricRenameExpr": "name.replace(/[^a-zA-Z0-9_]/g, '_')",
  "prometheusAuth": {
    "authType": "none",
    "token": "string",
    "textSecret": "string",
    "username": "string",
    "password": "string",
    "credentialsSecret": "string"
  },
  "lokiAuth": {
    "authType": "none",
    "token": "string",
    "textSecret": "string",
    "username": "string",
    "password": "string",
    "credentialsSecret": "string"
  },
  "concurrency": 1,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 15,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "compress": true,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|lokiUrl|string|false|none|The endpoint to send logs to, such as https://logs-prod-us-central1.grafana.net|
|prometheusUrl|string|false|none|The remote_write endpoint to send Prometheus metrics to, such as https://prometheus-blocks-prod-us-central1.grafana.net/api/prom/push|
|message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|» name|string|true|none|none|
|» value|string|true|none|none|
|metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|prometheusAuth|object|false|none|none|
|» authType|string|false|none|none|
|» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|» textSecret|string|false|none|Select or create a stored text secret|
|» username|string|false|none|Username for authentication|
|» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|lokiAuth|object|false|none|none|
|» authType|string|false|none|none|
|» token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|» textSecret|string|false|none|Select or create a stored text secret|
|» username|string|false|none|Username for authentication|
|» password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki and Prometheus to complain about entries being delivered out of order.|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki and Prometheus to complain about entries being delivered out of order.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|compress|boolean|false|none|Compress the payload body before sending. Applies only to JSON payloads; the Protobuf variant for both Prometheus and Loki are snappy-compressed by default.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

anyOf

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|object|false|none|none|

or

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|grafana_cloud|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputLoki">OutputLoki</h2>
<!-- backwards compatibility -->
<a id="schemaoutputloki"></a>
<a id="schema_OutputLoki"></a>
<a id="tocSoutputloki"></a>
<a id="tocsoutputloki"></a>

```json
{
  "id": "string",
  "type": "loki",
  "pipeline": "string",
  "systemFields": [
    "cribl_host",
    "cribl_wp"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "message": "string",
  "messageFormat": "protobuf",
  "labels": [
    {
      "name": "",
      "value": "string"
    }
  ],
  "authType": "none",
  "concurrency": 1,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 15,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "enableDynamicHeaders": false,
  "onBackpressure": "block",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "compress": true,
  "token": "string",
  "textSecret": "string",
  "username": "string",
  "password": "string",
  "credentialsSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as labels to generated logs.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|The endpoint to send logs to|
|message|string|false|none|Name of the event field that contains the message to send. If not specified, Stream sends a JSON representation of the whole event.|
|messageFormat|string|false|none|Format to use when sending logs to Loki (Protobuf or JSON)|
|labels|[object]|false|none|List of labels to send with logs. Labels define Loki streams, so use static labels to avoid proliferating label value combinations and streams. Can be merged and/or overridden by the event's __labels field. Example: '__labels: {host: "cribl.io", level: "error"}'|
|» name|string|true|none|none|
|» value|string|true|none|none|
|authType|string|false|none|none|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking. Warning: Setting this value > 1 can cause Loki to complain about entries being delivered out of order.|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Defaults to 0 (unlimited). Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Maximum time between requests. Small values can reduce the payload size below the configured 'Max record size' and 'Max events per request'. Warning: Setting this too low can increase the number of ongoing requests (depending on the value of 'Request concurrency'); this can cause Loki to complain about entries being delivered out of order.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|enableDynamicHeaders|boolean|false|none|Add per-event HTTP headers from the __headers field to outgoing requests. Events with different headers are batched and sent separately.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|compress|boolean|false|none|Compress the payload body before sending|
|token|string|false|none|Bearer token to include in the authorization header. In Grafana Cloud, this is generally built by concatenating the username and the API key, separated by a colon. Example: <your-username>:<your-api-key>|
|textSecret|string|false|none|Select or create a stored text secret|
|username|string|false|none|Username for authentication|
|password|string|false|none|Password (API key in Grafana Cloud domain) for authentication|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|loki|
|messageFormat|protobuf|
|messageFormat|json|
|authType|none|
|authType|token|
|authType|textSecret|
|authType|basic|
|authType|credentialsSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputPrometheus">OutputPrometheus</h2>
<!-- backwards compatibility -->
<a id="schemaoutputprometheus"></a>
<a id="schema_OutputPrometheus"></a>
<a id="tocSoutputprometheus"></a>
<a id="tocsoutputprometheus"></a>

```json
{
  "id": "string",
  "type": "prometheus",
  "pipeline": "string",
  "systemFields": [
    "cribl_host",
    "cribl_wp"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "metricRenameExpr": "name.replace(/[^a-zA-Z0-9_]/g, '_')",
  "sendMetadata": true,
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "authType": "none",
  "description": "string",
  "metricsFlushPeriodSec": 60,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions to generated metrics.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|The endpoint to send metrics to|
|metricRenameExpr|string|false|none|JavaScript expression that can be used to rename metrics. For example, name.replace(/\./g, '_') will replace all '.' characters in a metric's name with the supported '_' character. Use the 'name' global variable to access the metric's name. You can access event fields' values via __e.<fieldName>.|
|sendMetadata|boolean|false|none|Generate and send metadata (`type` and `metricFamilyName`) requests|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Remote Write authentication type|
|description|string|false|none|none|
|metricsFlushPeriodSec|number|false|none|How frequently metrics metadata is sent out. Value cannot be smaller than the base Flush period set above.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|

#### Enumerated Values

|Property|Value|
|---|---|
|type|prometheus|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputRing">OutputRing</h2>
<!-- backwards compatibility -->
<a id="schemaoutputring"></a>
<a id="schema_OutputRing"></a>
<a id="tocSoutputring"></a>
<a id="tocsoutputring"></a>

```json
{
  "id": "string",
  "type": "ring",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "format": "json",
  "partitionExpr": "string",
  "maxDataSize": "1GB",
  "maxDataTime": "24h",
  "compress": "none",
  "destPath": "string",
  "onBackpressure": "block",
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|format|string|false|none|Format of the output data.|
|partitionExpr|string|false|none|JS expression to define how files are partitioned and organized. If left blank, Cribl Stream will fallback on event.__partition.|
|maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|compress|string|false|none|none|
|destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|ring|
|format|json|
|format|raw|
|compress|none|
|compress|gzip|
|onBackpressure|block|
|onBackpressure|drop|

<h2 id="tocS_OutputOpenTelemetry">OutputOpenTelemetry</h2>
<!-- backwards compatibility -->
<a id="schemaoutputopentelemetry"></a>
<a id="schema_OutputOpenTelemetry"></a>
<a id="tocSoutputopentelemetry"></a>
<a id="tocsoutputopentelemetry"></a>

```json
{
  "id": "string",
  "type": "open_telemetry",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "protocol": "grpc",
  "endpoint": "string",
  "otlpVersion": "0.10.0",
  "compress": "none",
  "httpCompress": "none",
  "authType": "none",
  "httpTracesEndpointOverride": "string",
  "httpMetricsEndpointOverride": "string",
  "httpLogsEndpointOverride": "string",
  "metadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "failedRequestLoggingMode": "payload",
  "connectionTimeout": 10000,
  "keepAliveTime": 30,
  "keepAlive": true,
  "onBackpressure": "block",
  "description": "string",
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "rejectUnauthorized": true,
  "useRoundRobinDns": false,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|protocol|string|false|none|Select a transport option for OpenTelemetry|
|endpoint|string|true|none|The endpoint where OTel events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets). Unspecified ports will default to 4317, unless the endpoint is an HTTPS-based URL or TLS is enabled, in which case 443 will be used.|
|otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|authType|string|false|none|OpenTelemetry authentication type|
|httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|» key|string|true|none|none|
|» value|string|true|none|none|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|open_telemetry|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputServiceNow">OutputServiceNow</h2>
<!-- backwards compatibility -->
<a id="schemaoutputservicenow"></a>
<a id="schema_OutputServiceNow"></a>
<a id="tocSoutputservicenow"></a>
<a id="tocsoutputservicenow"></a>

```json
{
  "id": "string",
  "type": "service_now",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "endpoint": "ingest.lightstep.com:443",
  "tokenSecret": "string",
  "authTokenName": "lightstep-access-token",
  "otlpVersion": "1.3.1",
  "maxPayloadSizeKB": 2048,
  "protocol": "grpc",
  "compress": "none",
  "httpCompress": "none",
  "httpTracesEndpointOverride": "string",
  "httpMetricsEndpointOverride": "string",
  "httpLogsEndpointOverride": "string",
  "metadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "concurrency": 5,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "failedRequestLoggingMode": "payload",
  "connectionTimeout": 10000,
  "keepAliveTime": 30,
  "keepAlive": true,
  "onBackpressure": "block",
  "description": "string",
  "rejectUnauthorized": true,
  "useRoundRobinDns": false,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|endpoint|string|true|none|The endpoint where ServiceNow events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|tokenSecret|string|true|none|Select or create a stored text secret|
|authTokenName|string|false|none|none|
|otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|protocol|string|true|none|Select a transport option for OpenTelemetry|
|compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|» key|string|true|none|none|
|» value|string|true|none|none|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|service_now|
|otlpVersion|1.3.1|
|protocol|grpc|
|protocol|http|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputDataset">OutputDataset</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdataset"></a>
<a id="schema_OutputDataset"></a>
<a id="tocSoutputdataset"></a>
<a id="tocsoutputdataset"></a>

```json
{
  "id": "string",
  "type": "dataset",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "messageField": "string",
  "excludeFields": [
    "sev",
    "_time",
    "ts",
    "thread"
  ],
  "serverHostField": "string",
  "timestampField": "string",
  "defaultSeverity": "finest",
  "responseRetrySettings": [
    {
      "httpStatus": 429,
      "initialBackoff": 1000,
      "backoffRate": 2,
      "maxBackoff": 10000
    }
  ],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": false,
  "site": "us",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "onBackpressure": "block",
  "authType": "manual",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "customUrl": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "apiKey": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|messageField|string|false|none|Name of the event field that contains the message or attributes to send. If not specified, all of the event's non-internal fields will be sent as attributes.|
|excludeFields|[string]|false|none|Fields to exclude from the event if the Message field is either unspecified or refers to an object. Ignored if the Message field is a string. If empty, we send all non-internal fields.|
|serverHostField|string|false|none|Name of the event field that contains the `serverHost` identifier. If not specified, defaults to `cribl_<outputId>`.|
|timestampField|string|false|none|Name of the event field that contains the timestamp. If not specified, defaults to `ts`, `_time`, or `Date.now()`, in that order.|
|defaultSeverity|string|false|none|Default value for event severity. If the `sev` or `__severity` fields are set on an event, the first one matching will override this value.|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|site|string|false|none|DataSet site to which events should be sent|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|Enter API key directly, or select a stored secret|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|customUrl|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|apiKey|string|false|none|A 'Log Write Access' API key for the DataSet account|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|dataset|
|defaultSeverity|finest|
|defaultSeverity|finer|
|defaultSeverity|fine|
|defaultSeverity|info|
|defaultSeverity|warning|
|defaultSeverity|error|
|defaultSeverity|fatal|
|site|us|
|site|eu|
|site|custom|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|manual|
|authType|secret|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputCriblTcp">OutputCriblTcp</h2>
<!-- backwards compatibility -->
<a id="schemaoutputcribltcp"></a>
<a id="schema_OutputCriblTcp"></a>
<a id="tocSoutputcribltcp"></a>
<a id="tocsoutputcribltcp"></a>

```json
{
  "id": "string",
  "type": "cribl_tcp",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "loadBalanced": true,
  "compression": "none",
  "logFailedRequests": false,
  "throttleRatePerSec": "0",
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "connectionTimeout": 10000,
  "writeTimeout": 60000,
  "tokenTTLMinutes": 60,
  "authTokens": [
    {
      "tokenSecret": "string",
      "enabled": true,
      "description": "string"
    }
  ],
  "excludeFields": [
    "__kube_*",
    "__metadata",
    "__winEvent"
  ],
  "onBackpressure": "block",
  "description": "string",
  "host": "string",
  "port": 10300,
  "excludeSelf": false,
  "hosts": [
    {
      "host": "string",
      "port": 10300,
      "tls": "inherit",
      "servername": "string",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "maxConcurrentSenders": 0,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|loadBalanced|boolean|false|none|Use load-balanced destinations|
|compression|string|false|none|Codec to use to compress the data before sending|
|logFailedRequests|boolean|false|none|Use to troubleshoot issues with sending data|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|writeTimeout|number|false|none|Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead|
|tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires, valid values between 1 and 60|
|authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl TCP Source in Cribl.Cloud.|
|» tokenSecret|string|true|none|Select or create a stored text secret|
|» enabled|boolean|false|none|none|
|» description|string|false|none|Optional token description|
|excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|host|string|false|none|The hostname of the receiver|
|port|number|false|none|The port to connect to on the provided host|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|hosts|[object]|false|none|Set of hosts to load-balance data to|
|» host|string|true|none|The hostname of the receiver|
|» port|number|true|none|The port to connect to on the provided host|
|» tls|string|false|none|Whether to inherit TLS configs from group setting or disable TLS|
|» servername|string|false|none|Servername to use if establishing a TLS connection. If not specified, defaults to connection host (if not an IP); otherwise, uses the global TLS settings.|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|maxConcurrentSenders|number|false|none|Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl_tcp|
|compression|none|
|compression|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|tls|inherit|
|tls|off|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputCriblHttp">OutputCriblHttp</h2>
<!-- backwards compatibility -->
<a id="schemaoutputcriblhttp"></a>
<a id="schema_OutputCriblHttp"></a>
<a id="tocSoutputcriblhttp"></a>
<a id="tocsoutputcriblhttp"></a>

```json
{
  "id": "string",
  "type": "cribl_http",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "loadBalanced": true,
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "tokenTTLMinutes": 60,
  "excludeFields": [
    "__kube_*",
    "__metadata",
    "__winEvent"
  ],
  "compression": "none",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [
    {
      "httpStatus": 401,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 408,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 429,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 500,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 502,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 503,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 504,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    },
    {
      "httpStatus": 509,
      "initialBackoff": 30000,
      "backoffRate": 2,
      "maxBackoff": 180000
    }
  ],
  "timeoutRetrySettings": {
    "timeoutRetry": true,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "authTokens": [
    {
      "tokenSecret": "string",
      "enabled": true,
      "description": "string"
    }
  ],
  "onBackpressure": "block",
  "description": "string",
  "url": "string",
  "useRoundRobinDns": false,
  "excludeSelf": false,
  "urls": [
    {
      "url": "string",
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|loadBalanced|boolean|false|none|For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs. If this setting is disabled, consider enabling round-robin DNS.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|tokenTTLMinutes|number|false|none|The number of minutes before the internally generated authentication token expires. Valid values are between 1 and 60.|
|excludeFields|[string]|false|none|Fields to exclude from the event. By default, all internal fields except `__output` are sent. Example: `cribl_pipe`, `c*`. Wildcards supported.|
|compression|string|false|none|Codec to use to compress the data before sending|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should also be installed in Cribl HTTP Source in Cribl.Cloud.|
|» tokenSecret|string|true|none|Select or create a stored text secret|
|» enabled|boolean|false|none|none|
|» description|string|false|none|none|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|url|string|false|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|urls|[object]|false|none|none|
|» url|string|true|none|URL of a Cribl Worker to send events to, such as http://localhost:10200|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl_http|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|compression|none|
|compression|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputHumioHec">OutputHumioHec</h2>
<!-- backwards compatibility -->
<a id="schemaoutputhumiohec"></a>
<a id="schema_OutputHumioHec"></a>
<a id="tocSoutputhumiohec"></a>
<a id="tocsoutputhumiohec"></a>

```json
{
  "id": "string",
  "type": "humio_hec",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "https://cloud.us.humio.com/api/v1/ingest/hec",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": true,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "format": "JSON",
  "authType": "manual",
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "token": "string",
  "textSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|URL to a CrowdStrike Falcon LogScale endpoint to send events to. Examples: https://cloud.us.humio.com/api/v1/ingest/hec for JSON and https://cloud.us.humio.com/api/v1/ingest/hec/raw for raw|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|token|string|false|none|CrowdStrike Falcon LogScale authentication token|
|textSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|humio_hec|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputCrowdstrikeNextGenSiem">OutputCrowdstrikeNextGenSiem</h2>
<!-- backwards compatibility -->
<a id="schemaoutputcrowdstrikenextgensiem"></a>
<a id="schema_OutputCrowdstrikeNextGenSiem"></a>
<a id="tocSoutputcrowdstrikenextgensiem"></a>
<a id="tocsoutputcrowdstrikenextgensiem"></a>

```json
{
  "id": "string",
  "type": "crowdstrike_next_gen_siem",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": true,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "format": "JSON",
  "authType": "manual",
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "token": "string",
  "textSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|URL provided from a CrowdStrike data connector. <br>Example: https://ingest.<region>.crowdstrike.com/api/ingest/hec/<connection-id>/v1/services/collector|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|format|string|true|none|When set to JSON, the event is automatically formatted with required fields before sending. When set to Raw, only the event's `_raw` value is sent.|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|token|string|false|none|none|
|textSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|crowdstrike_next_gen_siem|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|format|JSON|
|format|raw|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputDlS3">OutputDlS3</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdls3"></a>
<a id="schema_OutputDlS3"></a>
<a id="tocSoutputdls3"></a>
<a id="tocsoutputdls3"></a>

```json
{
  "id": "string",
  "type": "dl_s3",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "bucket": "string",
  "region": "string",
  "awsSecretKey": "string",
  "awsAuthenticationMethod": "auto",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "destPath": "",
  "objectACL": "private",
  "storageClass": "STANDARD",
  "serverSideEncryption": "AES256",
  "kmsKeyId": "string",
  "removeEmptyDirs": true,
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxConcurrentFileParts": 4,
  "verifyPermissions": true,
  "maxClosingFilesToBackpressure": 100,
  "partitioningFields": [],
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|region|string|false|none|Region where the S3 bucket is located|
|awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|destPath|string|false|none|Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|serverSideEncryption|string|false|none|none|
|kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|partitioningFields|[string]|false|none|List of fields to partition the path by, in addition to time, which is included automatically. The effective partition will be YYYY/MM/DD/HH/<list/of/fields>.|
|description|string|false|none|none|
|awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|dl_s3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputSecurityLake">OutputSecurityLake</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsecuritylake"></a>
<a id="schema_OutputSecurityLake"></a>
<a id="tocSoutputsecuritylake"></a>
<a id="tocsoutputsecuritylake"></a>

```json
{
  "id": "string",
  "type": "security_lake",
  "pipeline": "string",
  "systemFields": [],
  "environment": "string",
  "streamtags": [],
  "bucket": "string",
  "region": "string",
  "awsSecretKey": "string",
  "awsAuthenticationMethod": "auto",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "objectACL": "private",
  "storageClass": "STANDARD",
  "serverSideEncryption": "AES256",
  "kmsKeyId": "string",
  "removeEmptyDirs": true,
  "baseFileName": "`CriblOut`",
  "maxFileSizeMB": 32,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxConcurrentFileParts": 4,
  "verifyPermissions": true,
  "maxClosingFilesToBackpressure": 100,
  "accountId": "string",
  "customSource": "string",
  "automaticSchema": false,
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "parquetSchema": "string",
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards. These fields are added as dimensions and labels to generated metrics and logs, respectively.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|bucket|string|true|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|region|string|true|none|Region where the Amazon Security Lake is located.|
|awsSecretKey|string|false|none|none|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|endpoint|string|false|none|Amazon Security Lake service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Amazon Security Lake-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing Amazon Security Lake requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|assumeRoleArn|string|true|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|serverSideEncryption|string|false|none|none|
|kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|accountId|string|true|none|ID of the AWS account whose data the Destination will write to Security Lake. This should have been configured when creating the Amazon Security Lake custom source.|
|customSource|string|true|none|Name of the custom source configured in Amazon Security Lake|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|description|string|false|none|none|
|awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|security_lake|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputCriblLake">OutputCriblLake</h2>
<!-- backwards compatibility -->
<a id="schemaoutputcribllake"></a>
<a id="schema_OutputCriblLake"></a>
<a id="tocSoutputcribllake"></a>
<a id="tocsoutputcribllake"></a>

```json
{
  "id": "string",
  "type": "cribl_lake",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "bucket": "string",
  "region": "string",
  "awsSecretKey": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "destPath": "string",
  "objectACL": "private",
  "storageClass": "STANDARD",
  "serverSideEncryption": "AES256",
  "kmsKeyId": "string",
  "removeEmptyDirs": true,
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 64,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 300,
  "verifyPermissions": true,
  "maxClosingFilesToBackpressure": 100,
  "awsAuthenticationMethod": "auto",
  "format": "json",
  "maxConcurrentFileParts": 1,
  "description": "string",
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|bucket|string|false|none|Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|region|string|false|none|Region where the S3 bucket is located|
|awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|stagePath|string|false|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|destPath|string|false|none|Lake dataset to send the data to.|
|objectACL|string|false|none|Object ACL to assign to uploaded objects|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|serverSideEncryption|string|false|none|none|
|kmsKeyId|string|false|none|ID or ARN of the KMS customer-managed key to use for encryption|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|maxClosingFilesToBackpressure|number|false|none|Maximum number of files that can be waiting for upload before backpressure is applied|
|awsAuthenticationMethod|string|false|none|none|
|format|string|false|none|none|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|description|string|false|none|none|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl_lake|
|signatureVersion|v2|
|signatureVersion|v4|
|objectACL|private|
|objectACL|public-read|
|objectACL|public-read-write|
|objectACL|authenticated-read|
|objectACL|aws-exec-read|
|objectACL|bucket-owner-read|
|objectACL|bucket-owner-full-control|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|storageClass|STANDARD_IA|
|storageClass|ONEZONE_IA|
|storageClass|INTELLIGENT_TIERING|
|storageClass|GLACIER|
|storageClass|GLACIER_IR|
|storageClass|DEEP_ARCHIVE|
|serverSideEncryption|AES256|
|serverSideEncryption|aws:kms|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|auto_rpc|
|awsAuthenticationMethod|manual|
|format|json|
|format|parquet|
|format|ddss|

<h2 id="tocS_OutputDiskSpool">OutputDiskSpool</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdiskspool"></a>
<a id="schema_OutputDiskSpool"></a>
<a id="tocSoutputdiskspool"></a>
<a id="tocsoutputdiskspool"></a>

```json
{
  "id": "string",
  "type": "disk_spool",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "timeWindow": "10m",
  "maxDataSize": "1GB",
  "maxDataTime": "24h",
  "compress": "none",
  "partitionExpr": "string",
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|compress|string|false|none|Data compression format. Default is gzip.|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized within the time-buckets. If blank, the event's __partition property is used and otherwise, events go directly into the time-bucket directory.|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|disk_spool|
|compress|none|
|compress|gzip|

<h2 id="tocS_OutputClickHouse">OutputClickHouse</h2>
<!-- backwards compatibility -->
<a id="schemaoutputclickhouse"></a>
<a id="schema_OutputClickHouse"></a>
<a id="tocSoutputclickhouse"></a>
<a id="tocsoutputclickhouse"></a>

```json
{
  "id": "string",
  "type": "click_house",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "url": "string",
  "authType": "none",
  "database": "string",
  "tableName": "string",
  "format": "json-compact-each-row-with-names",
  "mappingType": "automatic",
  "asyncInserts": false,
  "tls": {
    "disabled": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "dumpFormatErrorsToDisk": false,
  "onBackpressure": "block",
  "description": "string",
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "sqlUsername": "string",
  "waitForAsyncInserts": true,
  "excludeMappingFields": [],
  "describeTable": "string",
  "columnMappings": [
    {
      "columnName": "string",
      "columnType": "string",
      "columnValueExpression": "string"
    }
  ],
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|url|string|true|none|URL of the ClickHouse instance. Example: http://localhost:8123/|
|authType|string|false|none|none|
|database|string|true|none|none|
|tableName|string|true|none|Name of the ClickHouse table where data will be inserted. Name can contain letters (A-Z, a-z), numbers (0-9), and the character "_", and must start with either a letter or the character "_".|
|format|string|false|none|Data format to use when sending data to ClickHouse. Defaults to JSON Compact.|
|mappingType|string|false|none|How event fields are mapped to ClickHouse columns.|
|asyncInserts|boolean|false|none|Collect data into batches for later processing. Disable to write to a ClickHouse table immediately.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|dumpFormatErrorsToDisk|boolean|false|none|Log the most recent event that fails to match the table schema|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|
|sqlUsername|string|false|none|Username for certificate authentication|
|waitForAsyncInserts|boolean|false|none|Cribl will wait for confirmation that data has been fully inserted into the ClickHouse database before proceeding. Disabling this option can increase throughput, but Cribl won’t be able to verify data has been completely inserted.|
|excludeMappingFields|[string]|false|none|Fields to exclude from sending to ClickHouse|
|describeTable|string|false|none|Retrieves the table schema from ClickHouse and populates the Column Mapping table|
|columnMappings|[object]|false|none|none|
|» columnName|string|true|none|Name of the column in ClickHouse that will store field value|
|» columnType|string|false|none|Type of the column in the ClickHouse database|
|» columnValueExpression|string|true|none|JavaScript expression to compute value to be inserted into ClickHouse table|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|click_house|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|sslUserCertificate|
|authType|token|
|authType|textSecret|
|authType|oauth|
|format|json-compact-each-row-with-names|
|format|json-each-row|
|mappingType|automatic|
|mappingType|custom|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputXsiam">OutputXsiam</h2>
<!-- backwards compatibility -->
<a id="schemaoutputxsiam"></a>
<a id="schema_OutputXsiam"></a>
<a id="tocSoutputxsiam"></a>
<a id="tocsoutputxsiam"></a>

```json
{
  "id": "string",
  "type": "xsiam",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "loadBalanced": false,
  "concurrency": 5,
  "maxPayloadSizeKB": 10000,
  "maxPayloadEvents": 0,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "authType": "token",
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "throttleRateReqPerSec": 400,
  "onBackpressure": "block",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "url": "http://localhost:8088/logs/v1/event",
  "useRoundRobinDns": false,
  "excludeSelf": false,
  "urls": [
    {
      "url": null,
      "weight": 1
    }
  ],
  "dnsResolvePeriodSec": 600,
  "loadBalanceStatsPeriodSec": 300,
  "token": "string",
  "textSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|loadBalanced|boolean|false|none|Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|authType|string|false|none|Enter a token directly, or provide a secret referencing a token|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|throttleRateReqPerSec|integer|false|none|Maximum number of requests to limit to per second|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|url|string|false|none|XSIAM endpoint URL to send events to, such as https://api-{tenant external URL}/logs/v1/event|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|excludeSelf|boolean|false|none|Exclude all IPs of the current host from the list of any resolved hostnames|
|urls|[object]|false|none|none|
|» url|any|true|none|none|
|» weight|number|false|none|Assign a weight (>0) to each endpoint to indicate its traffic-handling capability|
|dnsResolvePeriodSec|number|false|none|The interval in which to re-resolve any hostnames and pick up destinations from A records|
|loadBalanceStatsPeriodSec|number|false|none|How far back in time to keep traffic stats for load balancing purposes|
|token|string|false|none|XSIAM authentication token|
|textSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|xsiam|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|token|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputNetflow">OutputNetflow</h2>
<!-- backwards compatibility -->
<a id="schemaoutputnetflow"></a>
<a id="schema_OutputNetflow"></a>
<a id="tocSoutputnetflow"></a>
<a id="tocsoutputnetflow"></a>

```json
{
  "id": "string",
  "type": "netflow",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "hosts": [
    {
      "host": "string",
      "port": 2055
    }
  ],
  "dnsResolvePeriodSec": 0,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|hosts|[object]|true|none|One or more NetFlow destinations to forward events to|
|» host|string|true|none|Destination host|
|» port|number|true|none|Destination port, default is 2055|
|dnsResolvePeriodSec|number|false|none|How often to resolve the destination hostname to an IP address. Ignored if all destinations are IP addresses. A value of 0 means every datagram sent will incur a DNS lookup.|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|netflow|

<h2 id="tocS_OutputDynatraceHttp">OutputDynatraceHttp</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdynatracehttp"></a>
<a id="schema_OutputDynatraceHttp"></a>
<a id="tocSoutputdynatracehttp"></a>
<a id="tocsoutputdynatracehttp"></a>

```json
{
  "id": "string",
  "type": "dynatrace_http",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "method": "POST",
  "keepAlive": true,
  "concurrency": 5,
  "maxPayloadSizeKB": 4096,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "useRoundRobinDns": false,
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "authType": "token",
  "format": "json_array",
  "endpoint": "cloud",
  "telemetryType": "logs",
  "totalMemoryLimitKB": 0,
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {},
  "token": "string",
  "textSecret": "string",
  "environmentId": "string",
  "activeGateDomain": "string",
  "url": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|method|string|false|none|The method to use when sending events|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).|
|» name|string|false|none|none|
|» value|string|true|none|none|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|authType|string|false|none|none|
|format|string|true|none|How to format events before sending. Defaults to JSON. Plaintext is not currently supported.|
|endpoint|string|true|none|none|
|telemetryType|string|true|none|none|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|textSecret|string|false|none|Select or create a stored text secret|
|environmentId|string|false|none|ID of the environment to send to|
|activeGateDomain|string|false|none|ActiveGate domain with Log analytics collector module enabled. For example https://{activeGate-domain}:9999/e/{environment-id}/api/v2/logs/ingest.|
|url|string|false|none|URL to send events to. Can be overwritten by an event's __url field.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|dynatrace_http|
|method|POST|
|method|PUT|
|method|PATCH|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|authType|token|
|authType|textSecret|
|format|json_array|
|format|plaintext|
|endpoint|cloud|
|endpoint|activeGate|
|endpoint|manual|
|telemetryType|logs|
|telemetryType|metrics|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputDynatraceOtlp">OutputDynatraceOtlp</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdynatraceotlp"></a>
<a id="schema_OutputDynatraceOtlp"></a>
<a id="tocSoutputdynatraceotlp"></a>
<a id="tocsoutputdynatraceotlp"></a>

```json
{
  "id": "string",
  "type": "dynatrace_otlp",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "protocol": "http",
  "endpoint": "https://{your-environment-id}.live.dynatrace.com/api/v2/otlp",
  "otlpVersion": "1.3.1",
  "compress": "none",
  "httpCompress": "none",
  "httpTracesEndpointOverride": "string",
  "httpMetricsEndpointOverride": "string",
  "httpLogsEndpointOverride": "string",
  "metadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "concurrency": 5,
  "maxPayloadSizeKB": 2048,
  "timeoutSec": 30,
  "flushPeriodSec": 1,
  "failedRequestLoggingMode": "payload",
  "connectionTimeout": 10000,
  "keepAliveTime": 30,
  "keepAlive": true,
  "endpointType": "saas",
  "tokenSecret": "string",
  "authTokenName": "Authorization",
  "onBackpressure": "block",
  "description": "string",
  "rejectUnauthorized": true,
  "useRoundRobinDns": false,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "safeHeaders": [],
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|protocol|string|true|none|Select a transport option for Dynatrace|
|endpoint|string|true|none|The endpoint where Dynatrace events will be sent. Enter any valid URL or an IP address (IPv4 or IPv6; enclose IPv6 addresses in square brackets)|
|otlpVersion|string|true|none|The version of OTLP Protobuf definitions to use when structuring data to send|
|compress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|httpCompress|string|false|none|Type of compression to apply to messages sent to the OpenTelemetry endpoint|
|httpTracesEndpointOverride|string|false|none|If you want to send traces to the default `{endpoint}/v1/traces` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|httpMetricsEndpointOverride|string|false|none|If you want to send metrics to the default `{endpoint}/v1/metrics` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|httpLogsEndpointOverride|string|false|none|If you want to send logs to the default `{endpoint}/v1/logs` endpoint, leave this field empty; otherwise, specify the desired endpoint|
|metadata|[object]|false|none|List of key-value pairs to send with each gRPC request. Value supports JavaScript expressions that are evaluated just once, when the destination gets started. To pass credentials as metadata, use 'C.Secret'.|
|» key|string|true|none|none|
|» value|string|true|none|none|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size (in KB) of the request body. The maximum payload size is 4 MB. If this limit is exceeded, the entire OTLP message is dropped|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|connectionTimeout|number|false|none|Amount of time (milliseconds) to wait for the connection to establish before retrying|
|keepAliveTime|number|false|none|How often the sender should ping the peer to keep the connection open|
|keepAlive|boolean|false|none|Disable to close the connection immediately after sending the outgoing request|
|endpointType|string|true|none|Select the type of Dynatrace endpoint configured|
|tokenSecret|string|true|none|Select or create a stored text secret|
|authTokenName|string|false|none|none|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|dynatrace_otlp|
|protocol|http|
|otlpVersion|1.3.1|
|compress|none|
|compress|deflate|
|compress|gzip|
|httpCompress|none|
|httpCompress|gzip|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|endpointType|saas|
|endpointType|ag|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputSentinelOneAiSiem">OutputSentinelOneAiSiem</h2>
<!-- backwards compatibility -->
<a id="schemaoutputsentineloneaisiem"></a>
<a id="schema_OutputSentinelOneAiSiem"></a>
<a id="tocSoutputsentineloneaisiem"></a>
<a id="tocsoutputsentineloneaisiem"></a>

```json
{
  "id": "string",
  "type": "sentinel_one_ai_siem",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "region": "US",
  "endpoint": "/services/collector/event",
  "concurrency": 5,
  "maxPayloadSizeKB": 5120,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 30,
  "flushPeriodSec": 5,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "authType": "manual",
  "responseRetrySettings": [],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "onBackpressure": "block",
  "description": "string",
  "token": "string",
  "textSecret": "string",
  "baseUrl": "https://<Your-S1-Tenant>.sentinelone.net",
  "hostExpression": "__e.host || C.os.hostname()",
  "sourceExpression": "__e.source || (__e.__criblMetrics ? 'metrics' : 'cribl')",
  "sourceTypeExpression": "__e.sourcetype || 'dottedJson'",
  "dataSourceCategoryExpression": "'security'",
  "dataSourceNameExpression": "__e.__dataSourceName || 'cribl'",
  "dataSourceVendorExpression": "__e.__dataSourceVendor || 'cribl'",
  "eventTypeExpression": "",
  "host": "C.os.hostname()",
  "source": "cribl",
  "sourceType": "hecRawParser",
  "dataSourceCategory": "security",
  "dataSourceName": "cribl",
  "dataSourceVendor": "cribl",
  "eventType": "",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|region|string|true|none|The SentinelOne region to send events to. In most cases you can find the region by either looking at your SentinelOne URL or knowing what geographic region your SentinelOne instance is contained in.|
|endpoint|string|true|none|Endpoint to send events to. Use /services/collector/event for structured JSON payloads with standard HEC top-level fields. Use /services/collector/raw for unstructured log lines (plain text).|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|description|string|false|none|none|
|token|string|false|none|In the SentinelOne Console select Policy & Settings then select the Singularity AI SIEM section, API Keys will be at the bottom. Under Log Access Keys select a Write token and copy it here|
|textSecret|string|false|none|Select or create a stored text secret|
|baseUrl|string|false|none|Base URL of the endpoint used to send events to, such as https://<Your-S1-Tenant>.sentinelone.net. Must begin with http:// or https://, can include a port number, and no trailing slashes. Matches pattern: ^https?://[a-zA-Z0-9.-]+(:[0-9]+)?$.|
|hostExpression|string|false|none|Define serverHost for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myServer').|
|sourceExpression|string|false|none|Define logFile for events using a JavaScript expression. You must enclose text constants in quotes (such as, 'myLogFile.txt').|
|sourceTypeExpression|string|false|none|Define the parser for events using a JavaScript expression. This value helps parse data into AI SIEM. You must enclose text constants in quotes (such as, 'dottedJson'). For custom parsers, substitute 'dottedJson' with your parser's name.|
|dataSourceCategoryExpression|string|false|none|Define the dataSource.category for events using a JavaScript expression. This value helps categorize data and helps enable extra features in SentinelOne AI SIEM. You must enclose text constants in quotes. The default value is 'security'.|
|dataSourceNameExpression|string|false|none|Define the dataSource.name for events using a JavaScript expression. This value should reflect the type of data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'networkActivity' or 'authLogs').|
|dataSourceVendorExpression|string|false|none|Define the dataSource.vendor for events using a JavaScript expression. This value should reflect the vendor of the data being inserted into AI SIEM. You must enclose text constants in quotes (such as, 'Cisco' or 'Microsoft').|
|eventTypeExpression|string|false|none|Optionally, define the event.type for events using a JavaScript expression. This value acts as a label, grouping events into meaningful categories. You must enclose text constants in quotes (such as, 'Process Creation' or 'Network Connection').|
|host|string|false|none|Define the serverHost for events using a JavaScript expression. This value will be passed to AI SIEM. You must enclose text constants in quotes (such as, 'myServerName').|
|source|string|false|none|Specify the logFile value to pass as a parameter to SentinelOne AI SIEM. Don't quote this value. The default is cribl.|
|sourceType|string|false|none|Specify the sourcetype parameter for SentinelOne AI SIEM, which determines the parser. Don't quote this value. For custom parsers, substitute hecRawParser with your parser's name. The default is hecRawParser.|
|dataSourceCategory|string|false|none|Specify the dataSource.category value to pass as a parameter to SentinelOne AI SIEM. This value helps categorize data and enables additional features. Don't quote this value. The default is security.|
|dataSourceName|string|false|none|Specify the dataSource.name value to pass as a parameter to AI SIEM. This value should reflect the type of data being inserted. Don't quote this value. The default is cribl.|
|dataSourceVendor|string|false|none|Specify the dataSource.vendorvalue to pass as a parameter to AI SIEM. This value should reflect the vendor of the data being inserted. Don't quote this value. The default is cribl.|
|eventType|string|false|none|Specify the event.type value to pass as an optional parameter to AI SIEM. This value acts as a label, grouping events into meaningful categories like Process Creation, File Modification, or Network Connection. Don't quote this value. By default, this field is empty.|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|sentinel_one_ai_siem|
|region|US|
|region|CA|
|region|EMEA|
|region|AP|
|region|APS|
|region|AU|
|region|Custom|
|endpoint|/services/collector/event|
|endpoint|/services/collector/raw|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|authType|manual|
|authType|secret|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputChronicle">OutputChronicle</h2>
<!-- backwards compatibility -->
<a id="schemaoutputchronicle"></a>
<a id="schema_OutputChronicle"></a>
<a id="tocSoutputchronicle"></a>
<a id="tocsoutputchronicle"></a>

```json
{
  "id": "string",
  "type": "chronicle",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "apiVersion": "v1alpha",
  "authenticationMethod": "serviceAccount",
  "responseRetrySettings": [
    {
      "httpStatus": 429,
      "initialBackoff": 30000,
      "backoffRate": 1,
      "maxBackoff": 30000
    },
    {
      "httpStatus": 500,
      "initialBackoff": 30000,
      "backoffRate": 1,
      "maxBackoff": 30000
    },
    {
      "httpStatus": 503,
      "initialBackoff": 30000,
      "backoffRate": 1,
      "maxBackoff": 30000
    }
  ],
  "timeoutRetrySettings": {
    "timeoutRetry": false,
    "initialBackoff": 1000,
    "backoffRate": 2,
    "maxBackoff": 10000
  },
  "responseHonorRetryAfterHeader": true,
  "region": "string",
  "concurrency": 5,
  "maxPayloadSizeKB": 1024,
  "maxPayloadEvents": 0,
  "compress": true,
  "rejectUnauthorized": true,
  "timeoutSec": 90,
  "flushPeriodSec": 1,
  "extraHttpHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "failedRequestLoggingMode": "payload",
  "safeHeaders": [],
  "useRoundRobinDns": false,
  "onBackpressure": "block",
  "totalMemoryLimitKB": 0,
  "ingestionMethod": "ImportLogs",
  "namespace": "string",
  "logType": "string",
  "logTextField": "string",
  "gcpProjectId": "string",
  "gcpInstance": "string",
  "customLabels": [
    {
      "key": "string",
      "value": "string",
      "rbacEnabled": false
    }
  ],
  "description": "string",
  "serviceAccountCredentials": "string",
  "serviceAccountCredentialsSecret": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|apiVersion|string|false|none|none|
|authenticationMethod|string|false|none|none|
|responseRetrySettings|[object]|false|none|Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)|
|» httpStatus|number|true|none|The HTTP response status code that will trigger retries|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|timeoutRetrySettings|object|false|none|none|
|» timeoutRetry|boolean|true|none|none|
|» initialBackoff|number|false|none|How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).|
|» backoffRate|number|false|none|Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.|
|» maxBackoff|number|false|none|The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).|
|responseHonorRetryAfterHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.|
|region|string|true|none|Regional endpoint to send events to|
|concurrency|number|false|none|Maximum number of ongoing requests before blocking|
|maxPayloadSizeKB|number|false|none|Maximum size, in KB, of the request body|
|maxPayloadEvents|number|false|none|Maximum number of events to include in the request body. Default is 0 (unlimited).|
|compress|boolean|false|none|Compress the payload body before sending|
|rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's). <br>        Enabled by default. When this setting is also present in TLS Settings (Client Side), <br>        that value will take precedence.|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|failedRequestLoggingMode|string|false|none|Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.|
|safeHeaders|[string]|false|none|List of headers that are safe to log in plain text|
|useRoundRobinDns|boolean|false|none|Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned.|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|totalMemoryLimitKB|number|false|none|Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.|
|ingestionMethod|string|false|none|none|
|namespace|string|false|none|User-configured environment namespace to identify the data domain the logs originated from. This namespace is used as a tag to identify the appropriate data domain for indexing and enrichment functionality. Can be overwritten by event field __namespace.|
|logType|string|true|none|Default log type value to send to SecOps. Can be overwritten by event field __logType.|
|logTextField|string|false|none|Name of the event field that contains the log text to send. If not specified, Stream sends a JSON representation of the whole event.|
|gcpProjectId|string|true|none|The Google Cloud Platform (GCP) project ID to send events to|
|gcpInstance|string|true|none|The Google Cloud Platform (GCP) instance to send events to. This is the Chronicle customer uuid.|
|customLabels|[object]|false|none|Custom labels to be added to every event|
|» key|string|true|none|none|
|» value|string|true|none|none|
|» rbacEnabled|boolean|false|none|Designate this label for role-based access control and filtering|
|description|string|false|none|none|
|serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|serviceAccountCredentialsSecret|string|false|none|Select or create a stored text secret|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|chronicle|
|authenticationMethod|serviceAccount|
|authenticationMethod|serviceAccountSecret|
|failedRequestLoggingMode|payload|
|failedRequestLoggingMode|payloadAndHeaders|
|failedRequestLoggingMode|none|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputDatabricks">OutputDatabricks</h2>
<!-- backwards compatibility -->
<a id="schemaoutputdatabricks"></a>
<a id="schema_OutputDatabricks"></a>
<a id="tocSoutputdatabricks"></a>
<a id="tocsoutputdatabricks"></a>

```json
{
  "id": "string",
  "type": "databricks",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "destPath": "",
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "workspaceId": "string",
  "scope": "all-apis",
  "clientId": "string",
  "catalog": "main",
  "schema": "external",
  "eventsVolumeName": "events",
  "clientTextSecret": "string",
  "timeoutSec": 60,
  "description": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|destPath|string|false|none|Optional path to prepend to files before uploading.|
|stagePath|string|false|none|Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|workspaceId|string|true|none|Databricks workspace ID|
|scope|string|true|none|OAuth scope for Unity Catalog authentication|
|clientId|string|true|none|OAuth client ID for Unity Catalog authentication|
|catalog|string|true|none|Name of the catalog to use for the output|
|schema|string|true|none|Name of the catalog schema to use for the output|
|eventsVolumeName|string|true|none|Name of the events volume in Databricks|
|clientTextSecret|string|true|none|OAuth client secret for Unity Catalog authentication|
|timeoutSec|number|false|none|Amount of time, in seconds, to wait for a request to complete before canceling it|
|description|string|false|none|none|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|databricks|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_OutputMicrosoftFabric">OutputMicrosoftFabric</h2>
<!-- backwards compatibility -->
<a id="schemaoutputmicrosoftfabric"></a>
<a id="schema_OutputMicrosoftFabric"></a>
<a id="tocSoutputmicrosoftfabric"></a>
<a id="tocsoutputmicrosoftfabric"></a>

```json
{
  "id": "string",
  "type": "microsoft_fabric",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "topic": "string",
  "ack": 1,
  "format": "json",
  "maxRecordSizeKB": 768,
  "flushEventCount": 1000,
  "flushPeriodSec": 1,
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": false,
    "mechanism": "plain",
    "username": "$ConnectionString",
    "textSecret": "string",
    "clientSecretAuthType": "secret",
    "clientTextSecret": "string",
    "certificateName": "string",
    "certPath": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "oauthEndpoint": "https://login.microsoftonline.com",
    "clientId": "string",
    "tenantId": "string",
    "scope": "string"
  },
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true
  },
  "onBackpressure": "block",
  "bootstrap_server": "string",
  "description": "string",
  "pqStrictOrdering": true,
  "pqRatePerSec": 0,
  "pqMode": "error",
  "pqMaxBufferSize": 42,
  "pqMaxBackpressureSec": 30,
  "pqMaxFileSize": "1 MB",
  "pqMaxSize": "5GB",
  "pqPath": "$CRIBL_HOME/state/queues",
  "pqCompress": "none",
  "pqOnBackpressure": "block",
  "pqControls": {}
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|topic|string|true|none|Topic name from Fabric Eventstream's endpoint|
|ack|integer|false|none|Control the number of required acknowledgments|
|format|string|false|none|Format to use to serialize events before writing to the Event Hubs Kafka brokers|
|maxRecordSizeKB|number|false|none|Maximum size of each record batch before compression. Setting should be < message.max.bytes settings in Event Hubs brokers.|
|flushEventCount|number|false|none|Maximum number of events in a batch before forcing a flush|
|flushPeriodSec|number|false|none|Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size.|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to bootstrap server. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» mechanism|string|false|none|none|
|» username|string|false|none|The username for authentication. This should always be $ConnectionString.|
|» textSecret|string|false|none|Select or create a stored text secret corresponding to the SASL JASS Password Primary or Password Secondary|
|» clientSecretAuthType|string|false|none|none|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» certificateName|string|false|none|Select or create a stored certificate|
|» certPath|string|false|none|none|
|» privKeyPath|string|false|none|none|
|» passphrase|string|false|none|none|
|» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|» scope|string|false|none|Scope to pass in the OAuth request parameter|
|tls|object|false|none|none|
|» disabled|boolean|true|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|bootstrap_server|string|true|none|Bootstrap server from Fabric Eventstream's endpoint|
|description|string|false|none|none|
|pqStrictOrdering|boolean|false|none|Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.|
|pqRatePerSec|number|false|none|Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.|
|pqMode|string|false|none|In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.|
|pqMaxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|pqMaxBackpressureSec|number|false|none|How long (in seconds) to wait for backpressure to resolve before engaging the queue|
|pqMaxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)|
|pqMaxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|pqPath|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.|
|pqCompress|string|false|none|Codec to use to compress the persisted data|
|pqOnBackpressure|string|false|none|How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.|
|pqControls|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|microsoft_fabric|
|ack|1|
|ack|0|
|ack|-1|
|format|json|
|format|raw|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|onBackpressure|block|
|onBackpressure|drop|
|onBackpressure|queue|
|pqMode|error|
|pqMode|always|
|pqMode|backpressure|
|pqCompress|none|
|pqCompress|gzip|
|pqOnBackpressure|block|
|pqOnBackpressure|drop|

<h2 id="tocS_OutputCloudflareR2">OutputCloudflareR2</h2>
<!-- backwards compatibility -->
<a id="schemaoutputcloudflarer2"></a>
<a id="schema_OutputCloudflareR2"></a>
<a id="tocSoutputcloudflarer2"></a>
<a id="tocsoutputcloudflarer2"></a>

```json
{
  "id": "string",
  "type": "cloudflare_r2",
  "pipeline": "string",
  "systemFields": [
    "cribl_pipe"
  ],
  "environment": "string",
  "streamtags": [],
  "endpoint": "string",
  "bucket": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": null,
  "stagePath": "$CRIBL_HOME/state/outputs/staging",
  "addIdToStagePath": true,
  "destPath": "string",
  "signatureVersion": "v2",
  "objectACL": null,
  "storageClass": "STANDARD",
  "serverSideEncryption": "AES256",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "verifyPermissions": true,
  "removeEmptyDirs": true,
  "partitionExpr": "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  "format": "json",
  "baseFileName": "`CriblOut`",
  "fileNameSuffix": "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  "maxFileSizeMB": 32,
  "maxOpenFiles": 100,
  "headerLine": "",
  "writeHighWaterMark": 64,
  "onBackpressure": "block",
  "deadletterEnabled": false,
  "onDiskFullBackpressure": "block",
  "forceCloseOnShutdown": false,
  "maxFileOpenTimeSec": 300,
  "maxFileIdleTimeSec": 30,
  "maxConcurrentFileParts": 4,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "compress": "none",
  "compressionLevel": "best_speed",
  "automaticSchema": false,
  "parquetSchema": "string",
  "parquetVersion": "PARQUET_1_0",
  "parquetDataPageVersion": "DATA_PAGE_V1",
  "parquetRowGroupLength": 10000,
  "parquetPageSize": "1MB",
  "shouldLogInvalidRows": true,
  "keyValueMetadata": [
    {
      "key": "",
      "value": "string"
    }
  ],
  "enableStatistics": true,
  "enableWritePageIndex": true,
  "enablePageChecksum": false,
  "emptyDirCleanupSec": 300,
  "directoryBatchSize": 1000,
  "deadletterPath": "$CRIBL_HOME/state/outputs/dead-letter",
  "maxRetryNum": 20
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this output|
|type|string|true|none|none|
|pipeline|string|false|none|Pipeline to process data before sending out to this output|
|systemFields|[string]|false|none|Fields to automatically add to events, such as cribl_pipe. Supports wildcards.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|endpoint|string|true|none|Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)|
|bucket|string|true|none|Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).|
|region|any|false|none|none|
|stagePath|string|true|none|Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.|
|addIdToStagePath|boolean|false|none|Add the Output ID value to staging location|
|destPath|string|false|none|Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.|
|signatureVersion|string|false|none|Signature version to use for signing MinIO requests|
|objectACL|any|false|none|none|
|storageClass|string|false|none|Storage class to select for uploaded objects|
|serverSideEncryption|string|false|none|Server-side encryption for uploaded objects|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)|
|verifyPermissions|boolean|false|none|Disable if you can access files within the bucket but not the bucket itself|
|removeEmptyDirs|boolean|false|none|Remove empty staging directories after moving files|
|partitionExpr|string|false|none|JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.|
|format|string|false|none|Format of the output data|
|baseFileName|string|false|none|JavaScript expression to define the output filename prefix (can be constant)|
|fileNameSuffix|string|false|none|JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).|
|maxFileSizeMB|number|false|none|Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.|
|maxOpenFiles|number|false|none|Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.|
|headerLine|string|false|none|If set, this line will be written to the beginning of each output file|
|writeHighWaterMark|number|false|none|Buffer size used to write to a file|
|onBackpressure|string|false|none|How to handle events when all receivers are exerting backpressure|
|deadletterEnabled|boolean|false|none|If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors|
|onDiskFullBackpressure|string|false|none|How to handle events when disk space is below the global 'Min free disk space' limit|
|forceCloseOnShutdown|boolean|false|none|Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.|
|maxFileOpenTimeSec|number|false|none|Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.|
|maxFileIdleTimeSec|number|false|none|Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.|
|maxConcurrentFileParts|number|false|none|Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.|
|description|string|false|none|none|
|awsApiKey|string|false|none|This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|compress|string|false|none|Data compression format to apply to HTTP content before it is delivered|
|compressionLevel|string|false|none|Compression level to apply before moving files to final destination|
|automaticSchema|boolean|false|none|Automatically calculate the schema based on the events of each Parquet file generated|
|parquetSchema|string|false|none|To add a new schema, navigate to Processing > Knowledge > Parquet Schemas|
|parquetVersion|string|false|none|Determines which data types are supported and how they are represented|
|parquetDataPageVersion|string|false|none|Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.|
|parquetRowGroupLength|number|false|none|The number of rows that every group will contain. The final group can contain a smaller number of rows.|
|parquetPageSize|string|false|none|Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.|
|shouldLogInvalidRows|boolean|false|none|Log up to 3 rows that @{product} skips due to data mismatch|
|keyValueMetadata|[object]|false|none|The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"|
|» key|string|true|none|none|
|» value|string|true|none|none|
|enableStatistics|boolean|false|none|Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.|
|enableWritePageIndex|boolean|false|none|One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.|
|enablePageChecksum|boolean|false|none|Parquet tools can use the checksum of a Parquet page to verify data integrity|
|emptyDirCleanupSec|number|false|none|How frequently, in seconds, to clean up empty directories|
|directoryBatchSize|number|false|none|Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.|
|deadletterPath|string|false|none|Storage location for files that fail to reach their final destination after maximum retries are exceeded|
|maxRetryNum|number|false|none|The maximum number of times a file will attempt to move to its final destination before being dead-lettered|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cloudflare_r2|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|secret|
|awsAuthenticationMethod|manual|
|signatureVersion|v2|
|signatureVersion|v4|
|storageClass|STANDARD|
|storageClass|REDUCED_REDUNDANCY|
|serverSideEncryption|AES256|
|format|json|
|format|raw|
|format|parquet|
|onBackpressure|block|
|onBackpressure|drop|
|onDiskFullBackpressure|block|
|onDiskFullBackpressure|drop|
|compress|none|
|compress|gzip|
|compressionLevel|best_speed|
|compressionLevel|normal|
|compressionLevel|best_compression|
|parquetVersion|PARQUET_1_0|
|parquetVersion|PARQUET_2_4|
|parquetVersion|PARQUET_2_6|
|parquetDataPageVersion|DATA_PAGE_V1|
|parquetDataPageVersion|DATA_PAGE_V2|

<h2 id="tocS_HealthStringType">HealthStringType</h2>
<!-- backwards compatibility -->
<a id="schemahealthstringtype"></a>
<a id="schema_HealthStringType"></a>
<a id="tocShealthstringtype"></a>
<a id="tocshealthstringtype"></a>

```json
"Green"

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|*anonymous*|Green|
|*anonymous*|Yellow|
|*anonymous*|Red|
|*anonymous*|Unknown|

<h2 id="tocS_HealthCountType">HealthCountType</h2>
<!-- backwards compatibility -->
<a id="schemahealthcounttype"></a>
<a id="schema_HealthCountType"></a>
<a id="tocShealthcounttype"></a>
<a id="tocshealthcounttype"></a>

```json
{
  "Green": 0,
  "Yellow": 0,
  "Red": 0,
  "Unknown": 0
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|Green|number|true|none|none|
|Yellow|number|true|none|none|
|Red|number|true|none|none|
|Unknown|number|true|none|none|

<h2 id="tocS_CriblEvent">CriblEvent</h2>
<!-- backwards compatibility -->
<a id="schemacriblevent"></a>
<a id="schema_CriblEvent"></a>
<a id="tocScriblevent"></a>
<a id="tocscriblevent"></a>

```json
{
  "_raw": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|_raw|string|true|none|none|

