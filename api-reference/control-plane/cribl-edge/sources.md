
<h1 id="cribl-edge-api-sources">Cribl Edge API - Sources v4.15.0-f275b803</h1>

> Scroll down for example requests and responses.

This API Reference lists available REST endpoints, along with their supported operations for accessing, creating, updating, or deleting resources. See our complementary product documentation at [docs.cribl.io](http://docs.cribl.io).

Base URLs:

* <a href="/">/</a>

Web: <a href="https://portal.support.cribl.io">Support</a> 

# Authentication

- HTTP Authentication, scheme: bearer 

<h1 id="cribl-edge-api-sources-sources">Sources</h1>

## Create a Source

<a id="opIdcreateInput"></a>

> Code samples

`POST /system/inputs`

Create a new Source.

> Body parameter

```json
{
  "id": "string",
  "type": "collection",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "throttleRatePerSec": "0",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "output": "string"
}
```

<h3 id="create-a-source-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|body|body|[Input](#schemainput)|true|Input object|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "collection",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "preprocess": {
        "disabled": true,
        "command": "string",
        "args": [
          "string"
        ]
      },
      "throttleRatePerSec": "0",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "output": "string"
    }
  ]
}
```

<h3 id="create-a-source-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="create-a-source-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCollection](#schemainputcollection)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process results|
|»»» sendToRoutes|boolean|false|none|Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» output|string|false|none|Destination to send results to|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKafka](#schemainputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMsk](#schemainputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttp](#schemainputhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunk](#schemainputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» description|string|false|none|none|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» description|string|false|none|none|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» compress|string|false|none|Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkSearch](#schemainputsplunksearch)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» searchHead|string|true|none|Search head base URL. Can be an expression. Default is https://localhost:8089.|
|»»» search|string|true|none|Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'|
|»»» earliest|string|false|none|The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'|
|»»» latest|string|false|none|The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'|
|»»» cronSchedule|string|true|none|A cron schedule on which to run this job|
|»»» endpoint|string|true|none|REST API used to create a search|
|»»» outputMode|string|true|none|Format of the returned output|
|»»» endpointParams|[object]|false|none|Optional request parameters to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» endpointHeaders|[object]|false|none|Optional request headers to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» logLevel|string|false|none|Collector runtime log level (verbosity)|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 for no timeout.|
|»»» useRoundRobinDns|boolean|false|none|When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authType|string|false|none|Splunk Search authentication type|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkHec](#schemainputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAzureBlob](#schemainputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» visibilityTimeout|number|false|none|The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» maxMessages|number|false|none|The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.|
|»»» servicePeriodSecs|number|false|none|The duration (in seconds) which pollers should be validated and restarted if exited|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» authType|string|false|none|none|
|»»» description|string|false|none|none|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputElastic](#schemainputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» elasticAPI|string|true|none|Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.|
|»»» authType|string|false|none|none|
|»»» apiVersion|string|false|none|The API version to use for communicating with the server|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» url|string|false|none|URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»»» removeHeaders|[string]|false|none|List of headers to remove from the request to proxy|
|»»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a proxy request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» authTokens|[string]|false|none|Bearer tokens to include in the authorization header|
|»»» customAPIVersion|string|false|none|Custom version information to respond to requests|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputConfluentCloud](#schemainputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGrafana](#schemainputgrafana)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|false|none|Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.|
|»»» lokiAPI|string|false|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|Remote Write authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|Loki logs authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputLoki](#schemainputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» lokiAPI|string|true|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'.|
|»»» authType|string|false|none|Loki logs authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheusRw](#schemainputprometheusrw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|true|none|Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write.|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheus](#schemainputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|false|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail.|
|»»» logLevel|string|true|none|Collector runtime Log Level|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targetList|[string]|false|none|List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.|
|»»»» Targets|string|false|none|none|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEdgePrometheus](#schemainputedgeprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|true|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in seconds to scrape targets for metrics.|
|»»» timeout|number|false|none|Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targets|[object]|false|none|none|
|»»»» protocol|string|false|none|Protocol to use when collecting metrics|
|»»»» host|string|true|none|Name of host from which to pull metrics.|
|»»»» port|number|false|none|The port number in the metrics URL for discovered targets.|
|»»»» path|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» scrapeProtocolExpr|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePortExpr|string|false|none|The port number in the metrics URL for discovered targets.|
|»»» scrapePathExpr|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» podFilter|[object]|false|none|Add rules to decide which pods to discover for metrics.<br>  Pods are searched if no rules are given or of all the rules'<br>  expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to pods objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Mgmt](#schemainputoffice365mgmt)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|true|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» publisherIdentifier|string|false|none|Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Management Activity API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» ingestionLag|number|false|none|Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Service](#schemainputoffice365service)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Services API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365MsgTrace](#schemainputoffice365msgtrace)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» url|string|true|none|URL to use when retrieving report data.|
|»»» interval|number|true|none|How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.|
|»»» startDate|string|false|none|Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.|
|»»» endDate|string|false|none|Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.|
|»»» timeout|number|false|none|HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.|
|»»» disableTimeFilter|boolean|false|none|Disables time filtering of events when a date range is specified.|
|»»» authType|string|false|none|Select authentication method.|
|»»» rescheduleDroppedTasks|boolean|false|none|Reschedule tasks that failed with non-fatal errors|
|»»» maxTaskReschedule|number|false|none|Maximum number of times a task can be rescheduled|
|»»» logLevel|string|false|none|Log Level (verbosity) for collection runtime behavior.|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» description|string|false|none|none|
|»»» username|string|false|none|Username to run Message Trace API call.|
|»»» password|string|false|none|Password to run Message Trace API call.|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials.|
|»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter.|
|»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory.|
|»»» clientId|string|false|none|client_id to pass in the OAuth request parameter.|
|»»» resource|string|false|none|Resource to pass in the OAuth request parameter.|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» textSecret|string|false|none|Select or create a secret that references your client_secret to pass in the OAuth request parameter.|
|»»» certOptions|object|false|none|none|
|»»»» certificateName|string|false|none|The name of the predefined certificate.|
|»»»» privKeyPath|string|true|none|Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt the private key.|
|»»»» certPath|string|true|none|Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEventhub](#schemainputeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topics|[string]|true|none|The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.|
|»»» groupId|string|false|none|The consumer group this instance belongs to. Default is 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Start reading from earliest available data; relevant only during initial subscription|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» sessionTimeout|number|false|none|Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.<br>      Value must be lower than rebalanceTimeout.<br>      See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» heartbeatInterval|number|false|none|Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» minimizeDuplicates|boolean|false|none|Minimize duplicate events by starting only one consumer for each topic partition|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputExec](#schemainputexec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» command|string|true|none|Command to execute; supports Bourne shell (or CMD on Windows) syntax|
|»»» retries|number|false|none|Maximum number of retry attempts in the event that the command fails|
|»»» scheduleType|string|false|none|Select a schedule type; either an interval (in seconds) or a cron-style schedule.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» interval|number|false|none|Interval between command executions in seconds.|
|»»» cronSchedule|string|false|none|Cron schedule to execute the command on.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFirehose](#schemainputfirehose)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGooglePubsub](#schemainputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» topicName|string|true|none|ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.|
|»»» subscriptionName|string|true|none|ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription|
|»»» monitorSubscription|boolean|false|none|Use when the subscription is not created by this Source and topic is not known|
|»»» createTopic|boolean|false|none|Create topic if it does not exist|
|»»» createSubscription|boolean|false|none|Create subscription if it does not exist|
|»»» region|string|false|none|Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxBacklog|number|false|none|If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events|
|»»» concurrency|number|false|none|How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.|
|»»» requestTimeout|number|false|none|Pull request timeout, in milliseconds|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» orderedDelivery|boolean|false|none|Receive events in the order they were added to the queue. The process sending events must have ordering enabled.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCribl](#schemainputcribl)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» filter|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblTcp](#schemainputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblHttp](#schemainputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblLakeHttp](#schemainputcribllakehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|none|
|»»»» token|string|true|none|none|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»»» splunkHecMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»»» elasticsearchMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcpjson](#schemainputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemMetrics](#schemainputsystemmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for system metrics|
|»»»»»» processes|boolean|false|none|Generate metrics for the numbers of processes in various states|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for disk metrics|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» inodes|boolean|false|none|Generate filesystem inode metrics|
|»»»»»» devices|[string]|false|none|Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.|
|»»»»»» mountpoints|[string]|false|none|Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.|
|»»»»»» fstypes|[string]|false|none|Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.|
|»»»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» container|object|false|none|none|
|»»»» mode|string|false|none|Select the level of detail for container metrics|
|»»»» dockerSocket|[string]|false|none|Full paths for Docker's UNIX-domain socket|
|»»»» dockerTimeout|number|false|none|Timeout, in seconds, for the Docker API|
|»»»» filters|[object]|false|none|Containers matching any of these will be included. All are included if no filters are added.|
|»»»»» expr|string|true|none|none|
|»»»» allContainers|boolean|false|none|Include stopped and paused containers|
|»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»»» detail|boolean|false|none|Generate full container metrics|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemState](#schemainputsystemstate)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» collectors|object|false|none|none|
|»»»» hostsfile|object|false|none|Creates events based on entries collected from the hosts file|
|»»»»» enable|boolean|false|none|none|
|»»»» interfaces|object|false|none|Creates events for each of the host’s network interfaces|
|»»»»» enable|boolean|false|none|none|
|»»»» disk|object|false|none|Creates events for physical disks, partitions, and file systems|
|»»»»» enable|boolean|false|none|none|
|»»»» metadata|object|false|none|Creates events based on the host system’s current state|
|»»»»» enable|boolean|false|none|none|
|»»»» routes|object|false|none|Creates events based on entries collected from the host’s network routes|
|»»»»» enable|boolean|false|none|none|
|»»»» dns|object|false|none|Creates events for DNS resolvers and search entries|
|»»»»» enable|boolean|false|none|none|
|»»»» user|object|false|none|Creates events for local users and groups|
|»»»»» enable|boolean|false|none|none|
|»»»» firewall|object|false|none|Creates events for Firewall rules entries|
|»»»»» enable|boolean|false|none|none|
|»»»» services|object|false|none|Creates events from the list of services|
|»»»»» enable|boolean|false|none|none|
|»»»» ports|object|false|none|Creates events from list of listening ports|
|»»»»» enable|boolean|false|none|none|
|»»»» loginUsers|object|false|none|Creates events from list of logged-in users|
|»»»»» enable|boolean|false|none|none|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeMetrics](#schemainputkubemetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metrics collections. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics on disk for Cribl Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeLogs](#schemainputkubelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between checks for new containers. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Pod objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» timestamps|boolean|false|none|For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeEvents](#schemainputkubeevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» rules|[object]|false|none|Filtering on event fields|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWindowsMetrics](#schemainputwindowsmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for system metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all system information|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for disk metrics|
|»»»»»» perVolume|boolean|false|none|Generate separate metrics for each volume|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» volumes|[string]|false|none|Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCrowdstrike](#schemainputcrowdstrike)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatadogAgent](#schemainputdatadogagent)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» extractMetrics|boolean|false|none|Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.|
|»»»» rejectUnauthorized|boolean|false|none|Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatagen](#schemainputdatagen)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» samples|[object]|true|none|none|
|»»»» sample|string|true|none|none|
|»»»» eventsPerSec|number|true|none|Maximum number of events to generate per second per Worker Node. Defaults to 10.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttpRaw](#schemainputhttpraw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKinesis](#schemainputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» streamName|string|true|none|Kinesis Data Stream to read data from|
|»»» serviceInterval|number|false|none|Time interval in minutes between consecutive service calls|
|»»» shardExpr|string|false|none|A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.|
|»»» shardIteratorType|string|false|none|Location at which to start reading a shard for the first time|
|»»» payloadFormat|string|false|none|Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.|
|»»» getRecordsLimit|number|false|none|Maximum number of records per getRecords call|
|»»» getRecordsLimitTotal|number|false|none|Maximum number of records, across all shards, to pull down at once per Worker Process|
|»»» loadBalancingAlgorithm|string|false|none|The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» verifyKPLCheckSums|boolean|false|none|Verify Kinesis Producer Library (KPL) event checksums|
|»»» avoidDuplicates|boolean|false|none|When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblmetrics](#schemainputcriblmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» prefix|string|false|none|A prefix that is applied to the metrics provided by Cribl Stream|
|»»» fullFidelity|boolean|false|none|Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMetrics](#schemainputmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3](#schemainputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» tagAfterProcessing|boolean|false|none|Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3Inventory](#schemainputs3inventory)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» checksumSuffix|string|false|none|Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"|
|»»» maxManifestSizeKB|integer|false|none|Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.|
|»»» validateInventoryFiles|boolean|false|none|If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSnmp](#schemainputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|UDP port to receive SNMP traps on. Defaults to 162.|
|»»» snmpV3Auth|object|false|none|Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.|
|»»»» v3AuthEnabled|boolean|true|none|none|
|»»»» allowUnmatchedTrap|boolean|false|none|Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.|
|»»»» v3Users|[object]|false|none|User credentials for receiving v3 traps|
|»»»»» name|string|true|none|none|
|»»»»» authProtocol|string|false|none|none|
|»»»»» authKey|any|false|none|none|
|»»»»» privProtocol|any|false|none|none|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» varbindsWithTypes|boolean|false|none|If enabled, parses varbinds as an array of objects that include OID, value, and type|
|»»» bestEffortParsing|boolean|false|none|If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOpenTelemetry](#schemainputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|any|false|none|none|
|»»» captureHeaders|any|false|none|none|
|»»» activityLogSampleRate|any|false|none|none|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).|
|»»» enableHealthCheck|boolean|false|none|Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» protocol|string|false|none|Select whether to leverage gRPC or HTTP for OpenTelemetry|
|»»» extractSpans|boolean|false|none|Enable to extract each incoming span to a separate event|
|»»» extractMetrics|boolean|false|none|Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when interpreting received data|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» extractLogs|boolean|false|none|Enable to extract each incoming log record to a separate event|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputModelDrivenTelemetry](#schemainputmodeldriventelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» shutdownTimeoutMs|number|false|none|Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSqs](#schemainputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created)|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSyslog](#schemainputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» timestampTimezone|string|false|none|Timezone to assign to timestamps without timezone info|
|»»» singleMsgUdpPackets|boolean|false|none|Treat UDP packet data received as full syslog message|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» keepFieldsList|[string]|false|none|Wildcard list of fields to keep from source data; * = ALL (default)|
|»»» octetCounting|boolean|false|none|Enable if incoming messages use octet counting per RFC 6587.|
|»»» inferFraming|boolean|false|none|Enable if we should infer the syslog framing of the incoming messages.|
|»»» strictlyInferOctetCounting|boolean|false|none|Enable if we should infer octet counting only if the messages comply with RFC 5424.|
|»»» allowNonStandardAppName|boolean|false|none|Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|
|»»» enableEnhancedProxyHeaderParsing|boolean|false|none|When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFile](#schemainputfile)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» mode|string|false|none|Choose how to discover files to monitor|
|»»» interval|number|false|none|Time, in seconds, between scanning for files|
|»»» filenames|[string]|false|none|The full path of discovered files are matched against this wildcard list|
|»»» filterArchivedFiles|boolean|false|none|Apply filename allowlist to file entries in archive file types, like tar or zip.|
|»»» tailOnly|boolean|false|none|Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.|
|»»» idleTimeout|number|false|none|Time, in seconds, before an idle file is closed|
|»»» minAgeDur|string|false|none|The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.|
|»»» maxAgeDur|string|false|none|The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.|
|»»» checkFileModTime|boolean|false|none|Skip files with modification times earlier than the maximum age duration|
|»»» forceText|boolean|false|none|Forces files containing binary data to be streamed as text|
|»»» hashLen|number|false|none|Length of file header bytes to use in hash for unique file identification|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» description|string|false|none|none|
|»»» path|string|false|none|Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.|
|»»» depth|number|false|none|Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.|
|»»» suppressMissingPathErrors|boolean|false|none|none|
|»»» deleteFiles|boolean|false|none|Delete files after they have been collected|
|»»» includeUnidentifiableBinary|boolean|false|none|Stream binary files as Base64-encoded chunks.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcp](#schemainputtcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableHeader|boolean|false|none|Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAppscope](#schemainputappscope)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableUnixPath|boolean|false|none|Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.|
|»»» filter|object|false|none|none|
|»»»» allow|[object]|false|none|Specify processes that AppScope should be loaded into, and the config to use.|
|»»»»» procname|string|true|none|Specify the name of a process or family of processes.|
|»»»»» arg|string|false|none|Specify a string to substring-match against process command-line.|
|»»»»» config|string|true|none|Choose a config to apply to processes that match the process name and/or argument.|
|»»»» transportURL|string|false|none|To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events and metrics on disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|false|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» unixSocketPath|string|false|none|Path to the UNIX domain socket to listen on.|
|»»» unixSocketPerms|string|false|none|Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWef](#schemainputwef)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authMethod|string|false|none|How to authenticate incoming client connections|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|Enable TLS|
|»»»» rejectUnauthorized|boolean|false|none|Required for WEF certificate authentication|
|»»»» requestCert|boolean|false|none|Required for WEF certificate authentication|
|»»»» certificateName|string|false|none|Name of the predefined certificate|
|»»»» privKeyPath|string|true|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|true|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|true|none|Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»»» ocspCheck|boolean|false|none|Enable OCSP check of certificate|
|»»»» keytab|any|false|none|none|
|»»»» principal|any|false|none|none|
|»»»» ocspCheckFailClose|boolean|false|none|If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.|
|»»» captureHeaders|boolean|false|none|Add request headers to events in the __headers field|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» caFingerprint|string|false|none|SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain|
|»»» keytab|string|false|none|Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.|
|»»» principal|string|false|none|Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>|
|»»» allowMachineIdMismatch|boolean|false|none|Allow events to be ingested even if their MachineID does not match the client certificate CN|
|»»» subscriptions|[object]|true|none|Subscriptions to events on forwarding endpoints|
|»»»» subscriptionName|string|true|none|none|
|»»»» version|string|false|none|Version UUID for this subscription. If any subscription parameters are modified, this value will change.|
|»»»» contentFormat|string|true|none|Content format in which the endpoint should deliver events|
|»»»» heartbeatInterval|number|true|none|Maximum time (in seconds) between endpoint checkins before considering it unavailable|
|»»»» batchTimeout|number|true|none|Interval (in seconds) over which the endpoint should collect events before sending them to Stream|
|»»»» readExistingEvents|boolean|false|none|Newly subscribed endpoints will send previously existing events. Disable to receive new events only.|
|»»»» sendBookmarks|boolean|false|none|Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.|
|»»»» compress|boolean|false|none|Receive compressed events from the source|
|»»»» targets|[string]|true|none|The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com|
|»»»» locale|string|false|none|The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".|
|»»»» querySelector|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events ingested under this subscription|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» logFingerprintMismatch|boolean|false|none|Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWinEventLogs](#schemainputwineventlogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» logNames|[string]|true|none|Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.|
|»»» readMode|string|false|none|Read all stored and future event logs, or only future events|
|»»» eventFormat|string|false|none|Format of individual events|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)|
|»»» interval|number|false|none|Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» batchSize|number|false|none|The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxEventBytes|number|false|none|The maximum number of bytes in an event before it is flushed to the pipelines|
|»»» description|string|false|none|none|
|»»» disableJsonRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|
|»»» disableXmlRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputRawUdp](#schemainputrawudp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» singleMsgUdpPackets|boolean|false|none|If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.|
|»»» ingestRawBytes|boolean|false|none|If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputJournalFiles](#schemainputjournalfiles)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» path|string|true|none|Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.|
|»»» interval|number|false|none|Time, in seconds, between scanning for journals.|
|»»» journals|[string]|true|none|The full path of discovered journals are matched against this wildcard list.|
|»»» rules|[object]|false|none|Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Journal objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» currentBoot|boolean|false|none|Skip log messages that are not part of the current boot session.|
|»»» maxAgeDur|string|false|none|The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWiz](#schemainputwiz)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» endpoint|string|true|none|The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql|
|»»» authUrl|string|true|none|The authentication URL to generate an OAuth token|
|»»» authAudienceOverride|string|false|none|The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.|
|»»» clientId|string|true|none|The client ID of the Wiz application|
|»»» contentConfig|[object]|true|none|none|
|»»»» contentType|string|true|none|The name of the Wiz query|
|»»»» contentDescription|string|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 to disable.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret of the Wiz application|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWizWebhook](#schemainputwizwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputNetflow](#schemainputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» enablePassThrough|boolean|false|none|Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» templateCacheMinutes|number|false|none|Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.|
|»»» v5Enabled|boolean|false|none|Accept messages in Netflow V5 format.|
|»»» v9Enabled|boolean|false|none|Accept messages in Netflow V9 format.|
|»»» ipfixEnabled|boolean|false|none|Accept messages in IPFIX format.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSecurityLake](#schemainputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputZscalerHec](#schemainputzscalerhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» hecAcks|boolean|false|none|Whether to enable Zscaler HEC acknowledgements|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCloudflareHec](#schemainputcloudflarehec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» accessControlAllowOrigin|[string]|false|none|HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|collection|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|kafka|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|msk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|splunk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|compress|disabled|
|compress|auto|
|compress|always|
|type|splunk_search|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|outputMode|csv|
|outputMode|json|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|azure_blob|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|type|elastic|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|authTokens|
|apiVersion|6.8.4|
|apiVersion|8.3.2|
|apiVersion|custom|
|authType|none|
|authType|manual|
|authType|secret|
|type|confluent_cloud|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|type|grafana|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|loki|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus_rw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|authType|manual|
|authType|secret|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|edge_prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|discoveryType|k8s-node|
|discoveryType|k8s-pods|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|kubernetes|
|protocol|http|
|protocol|https|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|office365_mgmt|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_service|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_msg_trace|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|oauth|
|authType|oauthSecret|
|authType|oauthCert|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|logLevel|silly|
|type|none|
|type|backoff|
|type|static|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|type|eventhub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|type|exec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|scheduleType|interval|
|scheduleType|cronSchedule|
|type|firehose|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|google_pubsub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|type|cribl|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|cribl_tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_lake_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|tcpjson|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|system_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|system_state|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_events|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|windows_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|crowdstrike|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|datadog_agent|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|datagen|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|http_raw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|kinesis|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|shardIteratorType|TRIM_HORIZON|
|shardIteratorType|LATEST|
|payloadFormat|cribl|
|payloadFormat|ndjson|
|payloadFormat|cloudwatch|
|payloadFormat|line|
|loadBalancingAlgorithm|ConsistentHashing|
|loadBalancingAlgorithm|RoundRobin|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|criblmetrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|s3|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|s3_inventory|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|snmp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authProtocol|none|
|authProtocol|md5|
|authProtocol|sha|
|authProtocol|sha224|
|authProtocol|sha256|
|authProtocol|sha384|
|authProtocol|sha512|
|type|open_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|model_driven_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|sqs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|syslog|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|file|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|manual|
|mode|auto|
|type|tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|appscope|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|wef|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authMethod|clientCert|
|authMethod|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|contentFormat|Raw|
|contentFormat|RenderedText|
|querySelector|simple|
|querySelector|xml|
|type|win_event_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|readMode|oldest|
|readMode|newest|
|eventFormat|json|
|eventFormat|xml|
|type|raw_udp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|journal_files|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|wiz|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|wiz_webhook|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|netflow|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|security_lake|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|zscaler_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cloudflare_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

## List all Sources

<a id="opIdlistInput"></a>

> Code samples

`GET /system/inputs`

Get a list of all Sources.

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "collection",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "preprocess": {
        "disabled": true,
        "command": "string",
        "args": [
          "string"
        ]
      },
      "throttleRatePerSec": "0",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "output": "string"
    }
  ]
}
```

<h3 id="list-all-sources-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="list-all-sources-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCollection](#schemainputcollection)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process results|
|»»» sendToRoutes|boolean|false|none|Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» output|string|false|none|Destination to send results to|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKafka](#schemainputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMsk](#schemainputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttp](#schemainputhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunk](#schemainputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» description|string|false|none|none|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» description|string|false|none|none|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» compress|string|false|none|Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkSearch](#schemainputsplunksearch)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» searchHead|string|true|none|Search head base URL. Can be an expression. Default is https://localhost:8089.|
|»»» search|string|true|none|Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'|
|»»» earliest|string|false|none|The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'|
|»»» latest|string|false|none|The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'|
|»»» cronSchedule|string|true|none|A cron schedule on which to run this job|
|»»» endpoint|string|true|none|REST API used to create a search|
|»»» outputMode|string|true|none|Format of the returned output|
|»»» endpointParams|[object]|false|none|Optional request parameters to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» endpointHeaders|[object]|false|none|Optional request headers to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» logLevel|string|false|none|Collector runtime log level (verbosity)|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 for no timeout.|
|»»» useRoundRobinDns|boolean|false|none|When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authType|string|false|none|Splunk Search authentication type|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkHec](#schemainputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAzureBlob](#schemainputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» visibilityTimeout|number|false|none|The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» maxMessages|number|false|none|The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.|
|»»» servicePeriodSecs|number|false|none|The duration (in seconds) which pollers should be validated and restarted if exited|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» authType|string|false|none|none|
|»»» description|string|false|none|none|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputElastic](#schemainputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» elasticAPI|string|true|none|Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.|
|»»» authType|string|false|none|none|
|»»» apiVersion|string|false|none|The API version to use for communicating with the server|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» url|string|false|none|URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»»» removeHeaders|[string]|false|none|List of headers to remove from the request to proxy|
|»»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a proxy request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» authTokens|[string]|false|none|Bearer tokens to include in the authorization header|
|»»» customAPIVersion|string|false|none|Custom version information to respond to requests|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputConfluentCloud](#schemainputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGrafana](#schemainputgrafana)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|false|none|Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.|
|»»» lokiAPI|string|false|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|Remote Write authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|Loki logs authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputLoki](#schemainputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» lokiAPI|string|true|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'.|
|»»» authType|string|false|none|Loki logs authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheusRw](#schemainputprometheusrw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|true|none|Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write.|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheus](#schemainputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|false|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail.|
|»»» logLevel|string|true|none|Collector runtime Log Level|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targetList|[string]|false|none|List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.|
|»»»» Targets|string|false|none|none|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEdgePrometheus](#schemainputedgeprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|true|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in seconds to scrape targets for metrics.|
|»»» timeout|number|false|none|Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targets|[object]|false|none|none|
|»»»» protocol|string|false|none|Protocol to use when collecting metrics|
|»»»» host|string|true|none|Name of host from which to pull metrics.|
|»»»» port|number|false|none|The port number in the metrics URL for discovered targets.|
|»»»» path|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» scrapeProtocolExpr|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePortExpr|string|false|none|The port number in the metrics URL for discovered targets.|
|»»» scrapePathExpr|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» podFilter|[object]|false|none|Add rules to decide which pods to discover for metrics.<br>  Pods are searched if no rules are given or of all the rules'<br>  expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to pods objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Mgmt](#schemainputoffice365mgmt)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|true|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» publisherIdentifier|string|false|none|Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Management Activity API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» ingestionLag|number|false|none|Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Service](#schemainputoffice365service)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Services API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365MsgTrace](#schemainputoffice365msgtrace)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» url|string|true|none|URL to use when retrieving report data.|
|»»» interval|number|true|none|How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.|
|»»» startDate|string|false|none|Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.|
|»»» endDate|string|false|none|Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.|
|»»» timeout|number|false|none|HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.|
|»»» disableTimeFilter|boolean|false|none|Disables time filtering of events when a date range is specified.|
|»»» authType|string|false|none|Select authentication method.|
|»»» rescheduleDroppedTasks|boolean|false|none|Reschedule tasks that failed with non-fatal errors|
|»»» maxTaskReschedule|number|false|none|Maximum number of times a task can be rescheduled|
|»»» logLevel|string|false|none|Log Level (verbosity) for collection runtime behavior.|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» description|string|false|none|none|
|»»» username|string|false|none|Username to run Message Trace API call.|
|»»» password|string|false|none|Password to run Message Trace API call.|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials.|
|»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter.|
|»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory.|
|»»» clientId|string|false|none|client_id to pass in the OAuth request parameter.|
|»»» resource|string|false|none|Resource to pass in the OAuth request parameter.|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» textSecret|string|false|none|Select or create a secret that references your client_secret to pass in the OAuth request parameter.|
|»»» certOptions|object|false|none|none|
|»»»» certificateName|string|false|none|The name of the predefined certificate.|
|»»»» privKeyPath|string|true|none|Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt the private key.|
|»»»» certPath|string|true|none|Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEventhub](#schemainputeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topics|[string]|true|none|The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.|
|»»» groupId|string|false|none|The consumer group this instance belongs to. Default is 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Start reading from earliest available data; relevant only during initial subscription|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» sessionTimeout|number|false|none|Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.<br>      Value must be lower than rebalanceTimeout.<br>      See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» heartbeatInterval|number|false|none|Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» minimizeDuplicates|boolean|false|none|Minimize duplicate events by starting only one consumer for each topic partition|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputExec](#schemainputexec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» command|string|true|none|Command to execute; supports Bourne shell (or CMD on Windows) syntax|
|»»» retries|number|false|none|Maximum number of retry attempts in the event that the command fails|
|»»» scheduleType|string|false|none|Select a schedule type; either an interval (in seconds) or a cron-style schedule.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» interval|number|false|none|Interval between command executions in seconds.|
|»»» cronSchedule|string|false|none|Cron schedule to execute the command on.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFirehose](#schemainputfirehose)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGooglePubsub](#schemainputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» topicName|string|true|none|ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.|
|»»» subscriptionName|string|true|none|ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription|
|»»» monitorSubscription|boolean|false|none|Use when the subscription is not created by this Source and topic is not known|
|»»» createTopic|boolean|false|none|Create topic if it does not exist|
|»»» createSubscription|boolean|false|none|Create subscription if it does not exist|
|»»» region|string|false|none|Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxBacklog|number|false|none|If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events|
|»»» concurrency|number|false|none|How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.|
|»»» requestTimeout|number|false|none|Pull request timeout, in milliseconds|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» orderedDelivery|boolean|false|none|Receive events in the order they were added to the queue. The process sending events must have ordering enabled.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCribl](#schemainputcribl)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» filter|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblTcp](#schemainputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblHttp](#schemainputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblLakeHttp](#schemainputcribllakehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|none|
|»»»» token|string|true|none|none|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»»» splunkHecMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»»» elasticsearchMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcpjson](#schemainputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemMetrics](#schemainputsystemmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for system metrics|
|»»»»»» processes|boolean|false|none|Generate metrics for the numbers of processes in various states|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for disk metrics|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» inodes|boolean|false|none|Generate filesystem inode metrics|
|»»»»»» devices|[string]|false|none|Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.|
|»»»»»» mountpoints|[string]|false|none|Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.|
|»»»»»» fstypes|[string]|false|none|Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.|
|»»»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» container|object|false|none|none|
|»»»» mode|string|false|none|Select the level of detail for container metrics|
|»»»» dockerSocket|[string]|false|none|Full paths for Docker's UNIX-domain socket|
|»»»» dockerTimeout|number|false|none|Timeout, in seconds, for the Docker API|
|»»»» filters|[object]|false|none|Containers matching any of these will be included. All are included if no filters are added.|
|»»»»» expr|string|true|none|none|
|»»»» allContainers|boolean|false|none|Include stopped and paused containers|
|»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»»» detail|boolean|false|none|Generate full container metrics|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemState](#schemainputsystemstate)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» collectors|object|false|none|none|
|»»»» hostsfile|object|false|none|Creates events based on entries collected from the hosts file|
|»»»»» enable|boolean|false|none|none|
|»»»» interfaces|object|false|none|Creates events for each of the host’s network interfaces|
|»»»»» enable|boolean|false|none|none|
|»»»» disk|object|false|none|Creates events for physical disks, partitions, and file systems|
|»»»»» enable|boolean|false|none|none|
|»»»» metadata|object|false|none|Creates events based on the host system’s current state|
|»»»»» enable|boolean|false|none|none|
|»»»» routes|object|false|none|Creates events based on entries collected from the host’s network routes|
|»»»»» enable|boolean|false|none|none|
|»»»» dns|object|false|none|Creates events for DNS resolvers and search entries|
|»»»»» enable|boolean|false|none|none|
|»»»» user|object|false|none|Creates events for local users and groups|
|»»»»» enable|boolean|false|none|none|
|»»»» firewall|object|false|none|Creates events for Firewall rules entries|
|»»»»» enable|boolean|false|none|none|
|»»»» services|object|false|none|Creates events from the list of services|
|»»»»» enable|boolean|false|none|none|
|»»»» ports|object|false|none|Creates events from list of listening ports|
|»»»»» enable|boolean|false|none|none|
|»»»» loginUsers|object|false|none|Creates events from list of logged-in users|
|»»»»» enable|boolean|false|none|none|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeMetrics](#schemainputkubemetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metrics collections. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics on disk for Cribl Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeLogs](#schemainputkubelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between checks for new containers. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Pod objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» timestamps|boolean|false|none|For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeEvents](#schemainputkubeevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» rules|[object]|false|none|Filtering on event fields|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWindowsMetrics](#schemainputwindowsmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for system metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all system information|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for disk metrics|
|»»»»»» perVolume|boolean|false|none|Generate separate metrics for each volume|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» volumes|[string]|false|none|Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCrowdstrike](#schemainputcrowdstrike)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatadogAgent](#schemainputdatadogagent)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» extractMetrics|boolean|false|none|Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.|
|»»»» rejectUnauthorized|boolean|false|none|Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatagen](#schemainputdatagen)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» samples|[object]|true|none|none|
|»»»» sample|string|true|none|none|
|»»»» eventsPerSec|number|true|none|Maximum number of events to generate per second per Worker Node. Defaults to 10.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttpRaw](#schemainputhttpraw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKinesis](#schemainputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» streamName|string|true|none|Kinesis Data Stream to read data from|
|»»» serviceInterval|number|false|none|Time interval in minutes between consecutive service calls|
|»»» shardExpr|string|false|none|A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.|
|»»» shardIteratorType|string|false|none|Location at which to start reading a shard for the first time|
|»»» payloadFormat|string|false|none|Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.|
|»»» getRecordsLimit|number|false|none|Maximum number of records per getRecords call|
|»»» getRecordsLimitTotal|number|false|none|Maximum number of records, across all shards, to pull down at once per Worker Process|
|»»» loadBalancingAlgorithm|string|false|none|The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» verifyKPLCheckSums|boolean|false|none|Verify Kinesis Producer Library (KPL) event checksums|
|»»» avoidDuplicates|boolean|false|none|When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblmetrics](#schemainputcriblmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» prefix|string|false|none|A prefix that is applied to the metrics provided by Cribl Stream|
|»»» fullFidelity|boolean|false|none|Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMetrics](#schemainputmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3](#schemainputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» tagAfterProcessing|boolean|false|none|Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3Inventory](#schemainputs3inventory)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» checksumSuffix|string|false|none|Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"|
|»»» maxManifestSizeKB|integer|false|none|Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.|
|»»» validateInventoryFiles|boolean|false|none|If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSnmp](#schemainputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|UDP port to receive SNMP traps on. Defaults to 162.|
|»»» snmpV3Auth|object|false|none|Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.|
|»»»» v3AuthEnabled|boolean|true|none|none|
|»»»» allowUnmatchedTrap|boolean|false|none|Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.|
|»»»» v3Users|[object]|false|none|User credentials for receiving v3 traps|
|»»»»» name|string|true|none|none|
|»»»»» authProtocol|string|false|none|none|
|»»»»» authKey|any|false|none|none|
|»»»»» privProtocol|any|false|none|none|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» varbindsWithTypes|boolean|false|none|If enabled, parses varbinds as an array of objects that include OID, value, and type|
|»»» bestEffortParsing|boolean|false|none|If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOpenTelemetry](#schemainputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|any|false|none|none|
|»»» captureHeaders|any|false|none|none|
|»»» activityLogSampleRate|any|false|none|none|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).|
|»»» enableHealthCheck|boolean|false|none|Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» protocol|string|false|none|Select whether to leverage gRPC or HTTP for OpenTelemetry|
|»»» extractSpans|boolean|false|none|Enable to extract each incoming span to a separate event|
|»»» extractMetrics|boolean|false|none|Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when interpreting received data|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» extractLogs|boolean|false|none|Enable to extract each incoming log record to a separate event|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputModelDrivenTelemetry](#schemainputmodeldriventelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» shutdownTimeoutMs|number|false|none|Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSqs](#schemainputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created)|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSyslog](#schemainputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» timestampTimezone|string|false|none|Timezone to assign to timestamps without timezone info|
|»»» singleMsgUdpPackets|boolean|false|none|Treat UDP packet data received as full syslog message|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» keepFieldsList|[string]|false|none|Wildcard list of fields to keep from source data; * = ALL (default)|
|»»» octetCounting|boolean|false|none|Enable if incoming messages use octet counting per RFC 6587.|
|»»» inferFraming|boolean|false|none|Enable if we should infer the syslog framing of the incoming messages.|
|»»» strictlyInferOctetCounting|boolean|false|none|Enable if we should infer octet counting only if the messages comply with RFC 5424.|
|»»» allowNonStandardAppName|boolean|false|none|Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|
|»»» enableEnhancedProxyHeaderParsing|boolean|false|none|When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFile](#schemainputfile)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» mode|string|false|none|Choose how to discover files to monitor|
|»»» interval|number|false|none|Time, in seconds, between scanning for files|
|»»» filenames|[string]|false|none|The full path of discovered files are matched against this wildcard list|
|»»» filterArchivedFiles|boolean|false|none|Apply filename allowlist to file entries in archive file types, like tar or zip.|
|»»» tailOnly|boolean|false|none|Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.|
|»»» idleTimeout|number|false|none|Time, in seconds, before an idle file is closed|
|»»» minAgeDur|string|false|none|The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.|
|»»» maxAgeDur|string|false|none|The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.|
|»»» checkFileModTime|boolean|false|none|Skip files with modification times earlier than the maximum age duration|
|»»» forceText|boolean|false|none|Forces files containing binary data to be streamed as text|
|»»» hashLen|number|false|none|Length of file header bytes to use in hash for unique file identification|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» description|string|false|none|none|
|»»» path|string|false|none|Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.|
|»»» depth|number|false|none|Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.|
|»»» suppressMissingPathErrors|boolean|false|none|none|
|»»» deleteFiles|boolean|false|none|Delete files after they have been collected|
|»»» includeUnidentifiableBinary|boolean|false|none|Stream binary files as Base64-encoded chunks.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcp](#schemainputtcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableHeader|boolean|false|none|Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAppscope](#schemainputappscope)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableUnixPath|boolean|false|none|Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.|
|»»» filter|object|false|none|none|
|»»»» allow|[object]|false|none|Specify processes that AppScope should be loaded into, and the config to use.|
|»»»»» procname|string|true|none|Specify the name of a process or family of processes.|
|»»»»» arg|string|false|none|Specify a string to substring-match against process command-line.|
|»»»»» config|string|true|none|Choose a config to apply to processes that match the process name and/or argument.|
|»»»» transportURL|string|false|none|To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events and metrics on disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|false|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» unixSocketPath|string|false|none|Path to the UNIX domain socket to listen on.|
|»»» unixSocketPerms|string|false|none|Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWef](#schemainputwef)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authMethod|string|false|none|How to authenticate incoming client connections|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|Enable TLS|
|»»»» rejectUnauthorized|boolean|false|none|Required for WEF certificate authentication|
|»»»» requestCert|boolean|false|none|Required for WEF certificate authentication|
|»»»» certificateName|string|false|none|Name of the predefined certificate|
|»»»» privKeyPath|string|true|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|true|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|true|none|Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»»» ocspCheck|boolean|false|none|Enable OCSP check of certificate|
|»»»» keytab|any|false|none|none|
|»»»» principal|any|false|none|none|
|»»»» ocspCheckFailClose|boolean|false|none|If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.|
|»»» captureHeaders|boolean|false|none|Add request headers to events in the __headers field|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» caFingerprint|string|false|none|SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain|
|»»» keytab|string|false|none|Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.|
|»»» principal|string|false|none|Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>|
|»»» allowMachineIdMismatch|boolean|false|none|Allow events to be ingested even if their MachineID does not match the client certificate CN|
|»»» subscriptions|[object]|true|none|Subscriptions to events on forwarding endpoints|
|»»»» subscriptionName|string|true|none|none|
|»»»» version|string|false|none|Version UUID for this subscription. If any subscription parameters are modified, this value will change.|
|»»»» contentFormat|string|true|none|Content format in which the endpoint should deliver events|
|»»»» heartbeatInterval|number|true|none|Maximum time (in seconds) between endpoint checkins before considering it unavailable|
|»»»» batchTimeout|number|true|none|Interval (in seconds) over which the endpoint should collect events before sending them to Stream|
|»»»» readExistingEvents|boolean|false|none|Newly subscribed endpoints will send previously existing events. Disable to receive new events only.|
|»»»» sendBookmarks|boolean|false|none|Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.|
|»»»» compress|boolean|false|none|Receive compressed events from the source|
|»»»» targets|[string]|true|none|The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com|
|»»»» locale|string|false|none|The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".|
|»»»» querySelector|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events ingested under this subscription|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» logFingerprintMismatch|boolean|false|none|Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWinEventLogs](#schemainputwineventlogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» logNames|[string]|true|none|Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.|
|»»» readMode|string|false|none|Read all stored and future event logs, or only future events|
|»»» eventFormat|string|false|none|Format of individual events|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)|
|»»» interval|number|false|none|Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» batchSize|number|false|none|The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxEventBytes|number|false|none|The maximum number of bytes in an event before it is flushed to the pipelines|
|»»» description|string|false|none|none|
|»»» disableJsonRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|
|»»» disableXmlRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputRawUdp](#schemainputrawudp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» singleMsgUdpPackets|boolean|false|none|If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.|
|»»» ingestRawBytes|boolean|false|none|If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputJournalFiles](#schemainputjournalfiles)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» path|string|true|none|Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.|
|»»» interval|number|false|none|Time, in seconds, between scanning for journals.|
|»»» journals|[string]|true|none|The full path of discovered journals are matched against this wildcard list.|
|»»» rules|[object]|false|none|Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Journal objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» currentBoot|boolean|false|none|Skip log messages that are not part of the current boot session.|
|»»» maxAgeDur|string|false|none|The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWiz](#schemainputwiz)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» endpoint|string|true|none|The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql|
|»»» authUrl|string|true|none|The authentication URL to generate an OAuth token|
|»»» authAudienceOverride|string|false|none|The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.|
|»»» clientId|string|true|none|The client ID of the Wiz application|
|»»» contentConfig|[object]|true|none|none|
|»»»» contentType|string|true|none|The name of the Wiz query|
|»»»» contentDescription|string|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 to disable.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret of the Wiz application|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWizWebhook](#schemainputwizwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputNetflow](#schemainputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» enablePassThrough|boolean|false|none|Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» templateCacheMinutes|number|false|none|Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.|
|»»» v5Enabled|boolean|false|none|Accept messages in Netflow V5 format.|
|»»» v9Enabled|boolean|false|none|Accept messages in Netflow V9 format.|
|»»» ipfixEnabled|boolean|false|none|Accept messages in IPFIX format.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSecurityLake](#schemainputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputZscalerHec](#schemainputzscalerhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» hecAcks|boolean|false|none|Whether to enable Zscaler HEC acknowledgements|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCloudflareHec](#schemainputcloudflarehec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» accessControlAllowOrigin|[string]|false|none|HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|collection|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|kafka|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|msk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|splunk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|compress|disabled|
|compress|auto|
|compress|always|
|type|splunk_search|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|outputMode|csv|
|outputMode|json|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|azure_blob|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|type|elastic|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|authTokens|
|apiVersion|6.8.4|
|apiVersion|8.3.2|
|apiVersion|custom|
|authType|none|
|authType|manual|
|authType|secret|
|type|confluent_cloud|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|type|grafana|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|loki|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus_rw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|authType|manual|
|authType|secret|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|edge_prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|discoveryType|k8s-node|
|discoveryType|k8s-pods|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|kubernetes|
|protocol|http|
|protocol|https|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|office365_mgmt|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_service|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_msg_trace|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|oauth|
|authType|oauthSecret|
|authType|oauthCert|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|logLevel|silly|
|type|none|
|type|backoff|
|type|static|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|type|eventhub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|type|exec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|scheduleType|interval|
|scheduleType|cronSchedule|
|type|firehose|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|google_pubsub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|type|cribl|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|cribl_tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_lake_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|tcpjson|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|system_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|system_state|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_events|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|windows_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|crowdstrike|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|datadog_agent|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|datagen|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|http_raw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|kinesis|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|shardIteratorType|TRIM_HORIZON|
|shardIteratorType|LATEST|
|payloadFormat|cribl|
|payloadFormat|ndjson|
|payloadFormat|cloudwatch|
|payloadFormat|line|
|loadBalancingAlgorithm|ConsistentHashing|
|loadBalancingAlgorithm|RoundRobin|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|criblmetrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|s3|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|s3_inventory|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|snmp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authProtocol|none|
|authProtocol|md5|
|authProtocol|sha|
|authProtocol|sha224|
|authProtocol|sha256|
|authProtocol|sha384|
|authProtocol|sha512|
|type|open_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|model_driven_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|sqs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|syslog|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|file|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|manual|
|mode|auto|
|type|tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|appscope|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|wef|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authMethod|clientCert|
|authMethod|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|contentFormat|Raw|
|contentFormat|RenderedText|
|querySelector|simple|
|querySelector|xml|
|type|win_event_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|readMode|oldest|
|readMode|newest|
|eventFormat|json|
|eventFormat|xml|
|type|raw_udp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|journal_files|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|wiz|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|wiz_webhook|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|netflow|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|security_lake|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|zscaler_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cloudflare_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

## Update a Source

<a id="opIdupdateInputById"></a>

> Code samples

`PATCH /system/inputs/{id}`

Update the specified Source.</br></br>Provide a complete representation of the Source that you want to update in the request body. This endpoint does not support partial updates. Cribl removes any omitted fields when updating the Source.</br></br>Confirm that the configuration in your request body is correct before sending the request. If the configuration is incorrect, the updated Source might not function as expected.

> Body parameter

```json
{
  "id": "string",
  "type": "collection",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "throttleRatePerSec": "0",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "output": "string"
}
```

<h3 id="update-a-source-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Source to update.|
|body|body|[Input](#schemainput)|true|Input object|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "collection",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "preprocess": {
        "disabled": true,
        "command": "string",
        "args": [
          "string"
        ]
      },
      "throttleRatePerSec": "0",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "output": "string"
    }
  ]
}
```

<h3 id="update-a-source-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="update-a-source-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCollection](#schemainputcollection)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process results|
|»»» sendToRoutes|boolean|false|none|Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» output|string|false|none|Destination to send results to|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKafka](#schemainputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMsk](#schemainputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttp](#schemainputhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunk](#schemainputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» description|string|false|none|none|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» description|string|false|none|none|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» compress|string|false|none|Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkSearch](#schemainputsplunksearch)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» searchHead|string|true|none|Search head base URL. Can be an expression. Default is https://localhost:8089.|
|»»» search|string|true|none|Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'|
|»»» earliest|string|false|none|The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'|
|»»» latest|string|false|none|The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'|
|»»» cronSchedule|string|true|none|A cron schedule on which to run this job|
|»»» endpoint|string|true|none|REST API used to create a search|
|»»» outputMode|string|true|none|Format of the returned output|
|»»» endpointParams|[object]|false|none|Optional request parameters to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» endpointHeaders|[object]|false|none|Optional request headers to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» logLevel|string|false|none|Collector runtime log level (verbosity)|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 for no timeout.|
|»»» useRoundRobinDns|boolean|false|none|When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authType|string|false|none|Splunk Search authentication type|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkHec](#schemainputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAzureBlob](#schemainputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» visibilityTimeout|number|false|none|The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» maxMessages|number|false|none|The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.|
|»»» servicePeriodSecs|number|false|none|The duration (in seconds) which pollers should be validated and restarted if exited|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» authType|string|false|none|none|
|»»» description|string|false|none|none|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputElastic](#schemainputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» elasticAPI|string|true|none|Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.|
|»»» authType|string|false|none|none|
|»»» apiVersion|string|false|none|The API version to use for communicating with the server|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» url|string|false|none|URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»»» removeHeaders|[string]|false|none|List of headers to remove from the request to proxy|
|»»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a proxy request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» authTokens|[string]|false|none|Bearer tokens to include in the authorization header|
|»»» customAPIVersion|string|false|none|Custom version information to respond to requests|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputConfluentCloud](#schemainputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGrafana](#schemainputgrafana)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|false|none|Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.|
|»»» lokiAPI|string|false|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|Remote Write authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|Loki logs authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputLoki](#schemainputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» lokiAPI|string|true|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'.|
|»»» authType|string|false|none|Loki logs authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheusRw](#schemainputprometheusrw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|true|none|Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write.|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheus](#schemainputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|false|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail.|
|»»» logLevel|string|true|none|Collector runtime Log Level|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targetList|[string]|false|none|List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.|
|»»»» Targets|string|false|none|none|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEdgePrometheus](#schemainputedgeprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|true|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in seconds to scrape targets for metrics.|
|»»» timeout|number|false|none|Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targets|[object]|false|none|none|
|»»»» protocol|string|false|none|Protocol to use when collecting metrics|
|»»»» host|string|true|none|Name of host from which to pull metrics.|
|»»»» port|number|false|none|The port number in the metrics URL for discovered targets.|
|»»»» path|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» scrapeProtocolExpr|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePortExpr|string|false|none|The port number in the metrics URL for discovered targets.|
|»»» scrapePathExpr|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» podFilter|[object]|false|none|Add rules to decide which pods to discover for metrics.<br>  Pods are searched if no rules are given or of all the rules'<br>  expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to pods objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Mgmt](#schemainputoffice365mgmt)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|true|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» publisherIdentifier|string|false|none|Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Management Activity API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» ingestionLag|number|false|none|Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Service](#schemainputoffice365service)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Services API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365MsgTrace](#schemainputoffice365msgtrace)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» url|string|true|none|URL to use when retrieving report data.|
|»»» interval|number|true|none|How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.|
|»»» startDate|string|false|none|Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.|
|»»» endDate|string|false|none|Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.|
|»»» timeout|number|false|none|HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.|
|»»» disableTimeFilter|boolean|false|none|Disables time filtering of events when a date range is specified.|
|»»» authType|string|false|none|Select authentication method.|
|»»» rescheduleDroppedTasks|boolean|false|none|Reschedule tasks that failed with non-fatal errors|
|»»» maxTaskReschedule|number|false|none|Maximum number of times a task can be rescheduled|
|»»» logLevel|string|false|none|Log Level (verbosity) for collection runtime behavior.|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» description|string|false|none|none|
|»»» username|string|false|none|Username to run Message Trace API call.|
|»»» password|string|false|none|Password to run Message Trace API call.|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials.|
|»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter.|
|»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory.|
|»»» clientId|string|false|none|client_id to pass in the OAuth request parameter.|
|»»» resource|string|false|none|Resource to pass in the OAuth request parameter.|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» textSecret|string|false|none|Select or create a secret that references your client_secret to pass in the OAuth request parameter.|
|»»» certOptions|object|false|none|none|
|»»»» certificateName|string|false|none|The name of the predefined certificate.|
|»»»» privKeyPath|string|true|none|Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt the private key.|
|»»»» certPath|string|true|none|Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEventhub](#schemainputeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topics|[string]|true|none|The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.|
|»»» groupId|string|false|none|The consumer group this instance belongs to. Default is 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Start reading from earliest available data; relevant only during initial subscription|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» sessionTimeout|number|false|none|Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.<br>      Value must be lower than rebalanceTimeout.<br>      See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» heartbeatInterval|number|false|none|Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» minimizeDuplicates|boolean|false|none|Minimize duplicate events by starting only one consumer for each topic partition|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputExec](#schemainputexec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» command|string|true|none|Command to execute; supports Bourne shell (or CMD on Windows) syntax|
|»»» retries|number|false|none|Maximum number of retry attempts in the event that the command fails|
|»»» scheduleType|string|false|none|Select a schedule type; either an interval (in seconds) or a cron-style schedule.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» interval|number|false|none|Interval between command executions in seconds.|
|»»» cronSchedule|string|false|none|Cron schedule to execute the command on.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFirehose](#schemainputfirehose)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGooglePubsub](#schemainputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» topicName|string|true|none|ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.|
|»»» subscriptionName|string|true|none|ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription|
|»»» monitorSubscription|boolean|false|none|Use when the subscription is not created by this Source and topic is not known|
|»»» createTopic|boolean|false|none|Create topic if it does not exist|
|»»» createSubscription|boolean|false|none|Create subscription if it does not exist|
|»»» region|string|false|none|Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxBacklog|number|false|none|If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events|
|»»» concurrency|number|false|none|How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.|
|»»» requestTimeout|number|false|none|Pull request timeout, in milliseconds|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» orderedDelivery|boolean|false|none|Receive events in the order they were added to the queue. The process sending events must have ordering enabled.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCribl](#schemainputcribl)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» filter|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblTcp](#schemainputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblHttp](#schemainputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblLakeHttp](#schemainputcribllakehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|none|
|»»»» token|string|true|none|none|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»»» splunkHecMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»»» elasticsearchMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcpjson](#schemainputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemMetrics](#schemainputsystemmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for system metrics|
|»»»»»» processes|boolean|false|none|Generate metrics for the numbers of processes in various states|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for disk metrics|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» inodes|boolean|false|none|Generate filesystem inode metrics|
|»»»»»» devices|[string]|false|none|Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.|
|»»»»»» mountpoints|[string]|false|none|Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.|
|»»»»»» fstypes|[string]|false|none|Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.|
|»»»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» container|object|false|none|none|
|»»»» mode|string|false|none|Select the level of detail for container metrics|
|»»»» dockerSocket|[string]|false|none|Full paths for Docker's UNIX-domain socket|
|»»»» dockerTimeout|number|false|none|Timeout, in seconds, for the Docker API|
|»»»» filters|[object]|false|none|Containers matching any of these will be included. All are included if no filters are added.|
|»»»»» expr|string|true|none|none|
|»»»» allContainers|boolean|false|none|Include stopped and paused containers|
|»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»»» detail|boolean|false|none|Generate full container metrics|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemState](#schemainputsystemstate)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» collectors|object|false|none|none|
|»»»» hostsfile|object|false|none|Creates events based on entries collected from the hosts file|
|»»»»» enable|boolean|false|none|none|
|»»»» interfaces|object|false|none|Creates events for each of the host’s network interfaces|
|»»»»» enable|boolean|false|none|none|
|»»»» disk|object|false|none|Creates events for physical disks, partitions, and file systems|
|»»»»» enable|boolean|false|none|none|
|»»»» metadata|object|false|none|Creates events based on the host system’s current state|
|»»»»» enable|boolean|false|none|none|
|»»»» routes|object|false|none|Creates events based on entries collected from the host’s network routes|
|»»»»» enable|boolean|false|none|none|
|»»»» dns|object|false|none|Creates events for DNS resolvers and search entries|
|»»»»» enable|boolean|false|none|none|
|»»»» user|object|false|none|Creates events for local users and groups|
|»»»»» enable|boolean|false|none|none|
|»»»» firewall|object|false|none|Creates events for Firewall rules entries|
|»»»»» enable|boolean|false|none|none|
|»»»» services|object|false|none|Creates events from the list of services|
|»»»»» enable|boolean|false|none|none|
|»»»» ports|object|false|none|Creates events from list of listening ports|
|»»»»» enable|boolean|false|none|none|
|»»»» loginUsers|object|false|none|Creates events from list of logged-in users|
|»»»»» enable|boolean|false|none|none|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeMetrics](#schemainputkubemetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metrics collections. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics on disk for Cribl Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeLogs](#schemainputkubelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between checks for new containers. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Pod objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» timestamps|boolean|false|none|For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeEvents](#schemainputkubeevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» rules|[object]|false|none|Filtering on event fields|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWindowsMetrics](#schemainputwindowsmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for system metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all system information|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for disk metrics|
|»»»»»» perVolume|boolean|false|none|Generate separate metrics for each volume|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» volumes|[string]|false|none|Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCrowdstrike](#schemainputcrowdstrike)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatadogAgent](#schemainputdatadogagent)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» extractMetrics|boolean|false|none|Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.|
|»»»» rejectUnauthorized|boolean|false|none|Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatagen](#schemainputdatagen)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» samples|[object]|true|none|none|
|»»»» sample|string|true|none|none|
|»»»» eventsPerSec|number|true|none|Maximum number of events to generate per second per Worker Node. Defaults to 10.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttpRaw](#schemainputhttpraw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKinesis](#schemainputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» streamName|string|true|none|Kinesis Data Stream to read data from|
|»»» serviceInterval|number|false|none|Time interval in minutes between consecutive service calls|
|»»» shardExpr|string|false|none|A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.|
|»»» shardIteratorType|string|false|none|Location at which to start reading a shard for the first time|
|»»» payloadFormat|string|false|none|Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.|
|»»» getRecordsLimit|number|false|none|Maximum number of records per getRecords call|
|»»» getRecordsLimitTotal|number|false|none|Maximum number of records, across all shards, to pull down at once per Worker Process|
|»»» loadBalancingAlgorithm|string|false|none|The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» verifyKPLCheckSums|boolean|false|none|Verify Kinesis Producer Library (KPL) event checksums|
|»»» avoidDuplicates|boolean|false|none|When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblmetrics](#schemainputcriblmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» prefix|string|false|none|A prefix that is applied to the metrics provided by Cribl Stream|
|»»» fullFidelity|boolean|false|none|Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMetrics](#schemainputmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3](#schemainputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» tagAfterProcessing|boolean|false|none|Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3Inventory](#schemainputs3inventory)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» checksumSuffix|string|false|none|Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"|
|»»» maxManifestSizeKB|integer|false|none|Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.|
|»»» validateInventoryFiles|boolean|false|none|If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSnmp](#schemainputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|UDP port to receive SNMP traps on. Defaults to 162.|
|»»» snmpV3Auth|object|false|none|Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.|
|»»»» v3AuthEnabled|boolean|true|none|none|
|»»»» allowUnmatchedTrap|boolean|false|none|Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.|
|»»»» v3Users|[object]|false|none|User credentials for receiving v3 traps|
|»»»»» name|string|true|none|none|
|»»»»» authProtocol|string|false|none|none|
|»»»»» authKey|any|false|none|none|
|»»»»» privProtocol|any|false|none|none|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» varbindsWithTypes|boolean|false|none|If enabled, parses varbinds as an array of objects that include OID, value, and type|
|»»» bestEffortParsing|boolean|false|none|If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOpenTelemetry](#schemainputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|any|false|none|none|
|»»» captureHeaders|any|false|none|none|
|»»» activityLogSampleRate|any|false|none|none|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).|
|»»» enableHealthCheck|boolean|false|none|Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» protocol|string|false|none|Select whether to leverage gRPC or HTTP for OpenTelemetry|
|»»» extractSpans|boolean|false|none|Enable to extract each incoming span to a separate event|
|»»» extractMetrics|boolean|false|none|Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when interpreting received data|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» extractLogs|boolean|false|none|Enable to extract each incoming log record to a separate event|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputModelDrivenTelemetry](#schemainputmodeldriventelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» shutdownTimeoutMs|number|false|none|Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSqs](#schemainputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created)|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSyslog](#schemainputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» timestampTimezone|string|false|none|Timezone to assign to timestamps without timezone info|
|»»» singleMsgUdpPackets|boolean|false|none|Treat UDP packet data received as full syslog message|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» keepFieldsList|[string]|false|none|Wildcard list of fields to keep from source data; * = ALL (default)|
|»»» octetCounting|boolean|false|none|Enable if incoming messages use octet counting per RFC 6587.|
|»»» inferFraming|boolean|false|none|Enable if we should infer the syslog framing of the incoming messages.|
|»»» strictlyInferOctetCounting|boolean|false|none|Enable if we should infer octet counting only if the messages comply with RFC 5424.|
|»»» allowNonStandardAppName|boolean|false|none|Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|
|»»» enableEnhancedProxyHeaderParsing|boolean|false|none|When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFile](#schemainputfile)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» mode|string|false|none|Choose how to discover files to monitor|
|»»» interval|number|false|none|Time, in seconds, between scanning for files|
|»»» filenames|[string]|false|none|The full path of discovered files are matched against this wildcard list|
|»»» filterArchivedFiles|boolean|false|none|Apply filename allowlist to file entries in archive file types, like tar or zip.|
|»»» tailOnly|boolean|false|none|Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.|
|»»» idleTimeout|number|false|none|Time, in seconds, before an idle file is closed|
|»»» minAgeDur|string|false|none|The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.|
|»»» maxAgeDur|string|false|none|The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.|
|»»» checkFileModTime|boolean|false|none|Skip files with modification times earlier than the maximum age duration|
|»»» forceText|boolean|false|none|Forces files containing binary data to be streamed as text|
|»»» hashLen|number|false|none|Length of file header bytes to use in hash for unique file identification|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» description|string|false|none|none|
|»»» path|string|false|none|Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.|
|»»» depth|number|false|none|Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.|
|»»» suppressMissingPathErrors|boolean|false|none|none|
|»»» deleteFiles|boolean|false|none|Delete files after they have been collected|
|»»» includeUnidentifiableBinary|boolean|false|none|Stream binary files as Base64-encoded chunks.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcp](#schemainputtcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableHeader|boolean|false|none|Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAppscope](#schemainputappscope)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableUnixPath|boolean|false|none|Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.|
|»»» filter|object|false|none|none|
|»»»» allow|[object]|false|none|Specify processes that AppScope should be loaded into, and the config to use.|
|»»»»» procname|string|true|none|Specify the name of a process or family of processes.|
|»»»»» arg|string|false|none|Specify a string to substring-match against process command-line.|
|»»»»» config|string|true|none|Choose a config to apply to processes that match the process name and/or argument.|
|»»»» transportURL|string|false|none|To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events and metrics on disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|false|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» unixSocketPath|string|false|none|Path to the UNIX domain socket to listen on.|
|»»» unixSocketPerms|string|false|none|Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWef](#schemainputwef)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authMethod|string|false|none|How to authenticate incoming client connections|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|Enable TLS|
|»»»» rejectUnauthorized|boolean|false|none|Required for WEF certificate authentication|
|»»»» requestCert|boolean|false|none|Required for WEF certificate authentication|
|»»»» certificateName|string|false|none|Name of the predefined certificate|
|»»»» privKeyPath|string|true|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|true|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|true|none|Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»»» ocspCheck|boolean|false|none|Enable OCSP check of certificate|
|»»»» keytab|any|false|none|none|
|»»»» principal|any|false|none|none|
|»»»» ocspCheckFailClose|boolean|false|none|If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.|
|»»» captureHeaders|boolean|false|none|Add request headers to events in the __headers field|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» caFingerprint|string|false|none|SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain|
|»»» keytab|string|false|none|Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.|
|»»» principal|string|false|none|Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>|
|»»» allowMachineIdMismatch|boolean|false|none|Allow events to be ingested even if their MachineID does not match the client certificate CN|
|»»» subscriptions|[object]|true|none|Subscriptions to events on forwarding endpoints|
|»»»» subscriptionName|string|true|none|none|
|»»»» version|string|false|none|Version UUID for this subscription. If any subscription parameters are modified, this value will change.|
|»»»» contentFormat|string|true|none|Content format in which the endpoint should deliver events|
|»»»» heartbeatInterval|number|true|none|Maximum time (in seconds) between endpoint checkins before considering it unavailable|
|»»»» batchTimeout|number|true|none|Interval (in seconds) over which the endpoint should collect events before sending them to Stream|
|»»»» readExistingEvents|boolean|false|none|Newly subscribed endpoints will send previously existing events. Disable to receive new events only.|
|»»»» sendBookmarks|boolean|false|none|Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.|
|»»»» compress|boolean|false|none|Receive compressed events from the source|
|»»»» targets|[string]|true|none|The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com|
|»»»» locale|string|false|none|The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".|
|»»»» querySelector|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events ingested under this subscription|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» logFingerprintMismatch|boolean|false|none|Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWinEventLogs](#schemainputwineventlogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» logNames|[string]|true|none|Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.|
|»»» readMode|string|false|none|Read all stored and future event logs, or only future events|
|»»» eventFormat|string|false|none|Format of individual events|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)|
|»»» interval|number|false|none|Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» batchSize|number|false|none|The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxEventBytes|number|false|none|The maximum number of bytes in an event before it is flushed to the pipelines|
|»»» description|string|false|none|none|
|»»» disableJsonRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|
|»»» disableXmlRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputRawUdp](#schemainputrawudp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» singleMsgUdpPackets|boolean|false|none|If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.|
|»»» ingestRawBytes|boolean|false|none|If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputJournalFiles](#schemainputjournalfiles)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» path|string|true|none|Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.|
|»»» interval|number|false|none|Time, in seconds, between scanning for journals.|
|»»» journals|[string]|true|none|The full path of discovered journals are matched against this wildcard list.|
|»»» rules|[object]|false|none|Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Journal objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» currentBoot|boolean|false|none|Skip log messages that are not part of the current boot session.|
|»»» maxAgeDur|string|false|none|The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWiz](#schemainputwiz)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» endpoint|string|true|none|The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql|
|»»» authUrl|string|true|none|The authentication URL to generate an OAuth token|
|»»» authAudienceOverride|string|false|none|The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.|
|»»» clientId|string|true|none|The client ID of the Wiz application|
|»»» contentConfig|[object]|true|none|none|
|»»»» contentType|string|true|none|The name of the Wiz query|
|»»»» contentDescription|string|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 to disable.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret of the Wiz application|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWizWebhook](#schemainputwizwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputNetflow](#schemainputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» enablePassThrough|boolean|false|none|Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» templateCacheMinutes|number|false|none|Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.|
|»»» v5Enabled|boolean|false|none|Accept messages in Netflow V5 format.|
|»»» v9Enabled|boolean|false|none|Accept messages in Netflow V9 format.|
|»»» ipfixEnabled|boolean|false|none|Accept messages in IPFIX format.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSecurityLake](#schemainputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputZscalerHec](#schemainputzscalerhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» hecAcks|boolean|false|none|Whether to enable Zscaler HEC acknowledgements|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCloudflareHec](#schemainputcloudflarehec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» accessControlAllowOrigin|[string]|false|none|HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|collection|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|kafka|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|msk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|splunk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|compress|disabled|
|compress|auto|
|compress|always|
|type|splunk_search|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|outputMode|csv|
|outputMode|json|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|azure_blob|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|type|elastic|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|authTokens|
|apiVersion|6.8.4|
|apiVersion|8.3.2|
|apiVersion|custom|
|authType|none|
|authType|manual|
|authType|secret|
|type|confluent_cloud|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|type|grafana|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|loki|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus_rw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|authType|manual|
|authType|secret|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|edge_prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|discoveryType|k8s-node|
|discoveryType|k8s-pods|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|kubernetes|
|protocol|http|
|protocol|https|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|office365_mgmt|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_service|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_msg_trace|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|oauth|
|authType|oauthSecret|
|authType|oauthCert|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|logLevel|silly|
|type|none|
|type|backoff|
|type|static|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|type|eventhub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|type|exec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|scheduleType|interval|
|scheduleType|cronSchedule|
|type|firehose|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|google_pubsub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|type|cribl|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|cribl_tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_lake_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|tcpjson|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|system_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|system_state|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_events|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|windows_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|crowdstrike|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|datadog_agent|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|datagen|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|http_raw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|kinesis|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|shardIteratorType|TRIM_HORIZON|
|shardIteratorType|LATEST|
|payloadFormat|cribl|
|payloadFormat|ndjson|
|payloadFormat|cloudwatch|
|payloadFormat|line|
|loadBalancingAlgorithm|ConsistentHashing|
|loadBalancingAlgorithm|RoundRobin|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|criblmetrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|s3|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|s3_inventory|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|snmp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authProtocol|none|
|authProtocol|md5|
|authProtocol|sha|
|authProtocol|sha224|
|authProtocol|sha256|
|authProtocol|sha384|
|authProtocol|sha512|
|type|open_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|model_driven_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|sqs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|syslog|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|file|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|manual|
|mode|auto|
|type|tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|appscope|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|wef|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authMethod|clientCert|
|authMethod|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|contentFormat|Raw|
|contentFormat|RenderedText|
|querySelector|simple|
|querySelector|xml|
|type|win_event_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|readMode|oldest|
|readMode|newest|
|eventFormat|json|
|eventFormat|xml|
|type|raw_udp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|journal_files|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|wiz|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|wiz_webhook|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|netflow|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|security_lake|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|zscaler_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cloudflare_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

## Get a Source

<a id="opIdgetInputById"></a>

> Code samples

`GET /system/inputs/{id}`

Get the specified Source.

<h3 id="get-a-source-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Source to get.|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "collection",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "preprocess": {
        "disabled": true,
        "command": "string",
        "args": [
          "string"
        ]
      },
      "throttleRatePerSec": "0",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "output": "string"
    }
  ]
}
```

<h3 id="get-a-source-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-a-source-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCollection](#schemainputcollection)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process results|
|»»» sendToRoutes|boolean|false|none|Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» output|string|false|none|Destination to send results to|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKafka](#schemainputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMsk](#schemainputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttp](#schemainputhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunk](#schemainputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» description|string|false|none|none|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» description|string|false|none|none|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» compress|string|false|none|Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkSearch](#schemainputsplunksearch)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» searchHead|string|true|none|Search head base URL. Can be an expression. Default is https://localhost:8089.|
|»»» search|string|true|none|Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'|
|»»» earliest|string|false|none|The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'|
|»»» latest|string|false|none|The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'|
|»»» cronSchedule|string|true|none|A cron schedule on which to run this job|
|»»» endpoint|string|true|none|REST API used to create a search|
|»»» outputMode|string|true|none|Format of the returned output|
|»»» endpointParams|[object]|false|none|Optional request parameters to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» endpointHeaders|[object]|false|none|Optional request headers to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» logLevel|string|false|none|Collector runtime log level (verbosity)|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 for no timeout.|
|»»» useRoundRobinDns|boolean|false|none|When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authType|string|false|none|Splunk Search authentication type|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkHec](#schemainputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAzureBlob](#schemainputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» visibilityTimeout|number|false|none|The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» maxMessages|number|false|none|The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.|
|»»» servicePeriodSecs|number|false|none|The duration (in seconds) which pollers should be validated and restarted if exited|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» authType|string|false|none|none|
|»»» description|string|false|none|none|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputElastic](#schemainputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» elasticAPI|string|true|none|Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.|
|»»» authType|string|false|none|none|
|»»» apiVersion|string|false|none|The API version to use for communicating with the server|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» url|string|false|none|URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»»» removeHeaders|[string]|false|none|List of headers to remove from the request to proxy|
|»»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a proxy request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» authTokens|[string]|false|none|Bearer tokens to include in the authorization header|
|»»» customAPIVersion|string|false|none|Custom version information to respond to requests|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputConfluentCloud](#schemainputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGrafana](#schemainputgrafana)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|false|none|Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.|
|»»» lokiAPI|string|false|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|Remote Write authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|Loki logs authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputLoki](#schemainputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» lokiAPI|string|true|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'.|
|»»» authType|string|false|none|Loki logs authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheusRw](#schemainputprometheusrw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|true|none|Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write.|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheus](#schemainputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|false|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail.|
|»»» logLevel|string|true|none|Collector runtime Log Level|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targetList|[string]|false|none|List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.|
|»»»» Targets|string|false|none|none|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEdgePrometheus](#schemainputedgeprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|true|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in seconds to scrape targets for metrics.|
|»»» timeout|number|false|none|Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targets|[object]|false|none|none|
|»»»» protocol|string|false|none|Protocol to use when collecting metrics|
|»»»» host|string|true|none|Name of host from which to pull metrics.|
|»»»» port|number|false|none|The port number in the metrics URL for discovered targets.|
|»»»» path|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» scrapeProtocolExpr|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePortExpr|string|false|none|The port number in the metrics URL for discovered targets.|
|»»» scrapePathExpr|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» podFilter|[object]|false|none|Add rules to decide which pods to discover for metrics.<br>  Pods are searched if no rules are given or of all the rules'<br>  expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to pods objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Mgmt](#schemainputoffice365mgmt)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|true|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» publisherIdentifier|string|false|none|Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Management Activity API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» ingestionLag|number|false|none|Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Service](#schemainputoffice365service)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Services API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365MsgTrace](#schemainputoffice365msgtrace)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» url|string|true|none|URL to use when retrieving report data.|
|»»» interval|number|true|none|How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.|
|»»» startDate|string|false|none|Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.|
|»»» endDate|string|false|none|Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.|
|»»» timeout|number|false|none|HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.|
|»»» disableTimeFilter|boolean|false|none|Disables time filtering of events when a date range is specified.|
|»»» authType|string|false|none|Select authentication method.|
|»»» rescheduleDroppedTasks|boolean|false|none|Reschedule tasks that failed with non-fatal errors|
|»»» maxTaskReschedule|number|false|none|Maximum number of times a task can be rescheduled|
|»»» logLevel|string|false|none|Log Level (verbosity) for collection runtime behavior.|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» description|string|false|none|none|
|»»» username|string|false|none|Username to run Message Trace API call.|
|»»» password|string|false|none|Password to run Message Trace API call.|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials.|
|»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter.|
|»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory.|
|»»» clientId|string|false|none|client_id to pass in the OAuth request parameter.|
|»»» resource|string|false|none|Resource to pass in the OAuth request parameter.|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» textSecret|string|false|none|Select or create a secret that references your client_secret to pass in the OAuth request parameter.|
|»»» certOptions|object|false|none|none|
|»»»» certificateName|string|false|none|The name of the predefined certificate.|
|»»»» privKeyPath|string|true|none|Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt the private key.|
|»»»» certPath|string|true|none|Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEventhub](#schemainputeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topics|[string]|true|none|The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.|
|»»» groupId|string|false|none|The consumer group this instance belongs to. Default is 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Start reading from earliest available data; relevant only during initial subscription|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» sessionTimeout|number|false|none|Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.<br>      Value must be lower than rebalanceTimeout.<br>      See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» heartbeatInterval|number|false|none|Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» minimizeDuplicates|boolean|false|none|Minimize duplicate events by starting only one consumer for each topic partition|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputExec](#schemainputexec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» command|string|true|none|Command to execute; supports Bourne shell (or CMD on Windows) syntax|
|»»» retries|number|false|none|Maximum number of retry attempts in the event that the command fails|
|»»» scheduleType|string|false|none|Select a schedule type; either an interval (in seconds) or a cron-style schedule.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» interval|number|false|none|Interval between command executions in seconds.|
|»»» cronSchedule|string|false|none|Cron schedule to execute the command on.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFirehose](#schemainputfirehose)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGooglePubsub](#schemainputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» topicName|string|true|none|ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.|
|»»» subscriptionName|string|true|none|ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription|
|»»» monitorSubscription|boolean|false|none|Use when the subscription is not created by this Source and topic is not known|
|»»» createTopic|boolean|false|none|Create topic if it does not exist|
|»»» createSubscription|boolean|false|none|Create subscription if it does not exist|
|»»» region|string|false|none|Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxBacklog|number|false|none|If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events|
|»»» concurrency|number|false|none|How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.|
|»»» requestTimeout|number|false|none|Pull request timeout, in milliseconds|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» orderedDelivery|boolean|false|none|Receive events in the order they were added to the queue. The process sending events must have ordering enabled.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCribl](#schemainputcribl)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» filter|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblTcp](#schemainputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblHttp](#schemainputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblLakeHttp](#schemainputcribllakehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|none|
|»»»» token|string|true|none|none|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»»» splunkHecMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»»» elasticsearchMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcpjson](#schemainputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemMetrics](#schemainputsystemmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for system metrics|
|»»»»»» processes|boolean|false|none|Generate metrics for the numbers of processes in various states|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for disk metrics|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» inodes|boolean|false|none|Generate filesystem inode metrics|
|»»»»»» devices|[string]|false|none|Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.|
|»»»»»» mountpoints|[string]|false|none|Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.|
|»»»»»» fstypes|[string]|false|none|Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.|
|»»»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» container|object|false|none|none|
|»»»» mode|string|false|none|Select the level of detail for container metrics|
|»»»» dockerSocket|[string]|false|none|Full paths for Docker's UNIX-domain socket|
|»»»» dockerTimeout|number|false|none|Timeout, in seconds, for the Docker API|
|»»»» filters|[object]|false|none|Containers matching any of these will be included. All are included if no filters are added.|
|»»»»» expr|string|true|none|none|
|»»»» allContainers|boolean|false|none|Include stopped and paused containers|
|»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»»» detail|boolean|false|none|Generate full container metrics|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemState](#schemainputsystemstate)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» collectors|object|false|none|none|
|»»»» hostsfile|object|false|none|Creates events based on entries collected from the hosts file|
|»»»»» enable|boolean|false|none|none|
|»»»» interfaces|object|false|none|Creates events for each of the host’s network interfaces|
|»»»»» enable|boolean|false|none|none|
|»»»» disk|object|false|none|Creates events for physical disks, partitions, and file systems|
|»»»»» enable|boolean|false|none|none|
|»»»» metadata|object|false|none|Creates events based on the host system’s current state|
|»»»»» enable|boolean|false|none|none|
|»»»» routes|object|false|none|Creates events based on entries collected from the host’s network routes|
|»»»»» enable|boolean|false|none|none|
|»»»» dns|object|false|none|Creates events for DNS resolvers and search entries|
|»»»»» enable|boolean|false|none|none|
|»»»» user|object|false|none|Creates events for local users and groups|
|»»»»» enable|boolean|false|none|none|
|»»»» firewall|object|false|none|Creates events for Firewall rules entries|
|»»»»» enable|boolean|false|none|none|
|»»»» services|object|false|none|Creates events from the list of services|
|»»»»» enable|boolean|false|none|none|
|»»»» ports|object|false|none|Creates events from list of listening ports|
|»»»»» enable|boolean|false|none|none|
|»»»» loginUsers|object|false|none|Creates events from list of logged-in users|
|»»»»» enable|boolean|false|none|none|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeMetrics](#schemainputkubemetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metrics collections. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics on disk for Cribl Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeLogs](#schemainputkubelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between checks for new containers. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Pod objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» timestamps|boolean|false|none|For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeEvents](#schemainputkubeevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» rules|[object]|false|none|Filtering on event fields|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWindowsMetrics](#schemainputwindowsmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for system metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all system information|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for disk metrics|
|»»»»»» perVolume|boolean|false|none|Generate separate metrics for each volume|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» volumes|[string]|false|none|Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCrowdstrike](#schemainputcrowdstrike)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatadogAgent](#schemainputdatadogagent)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» extractMetrics|boolean|false|none|Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.|
|»»»» rejectUnauthorized|boolean|false|none|Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatagen](#schemainputdatagen)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» samples|[object]|true|none|none|
|»»»» sample|string|true|none|none|
|»»»» eventsPerSec|number|true|none|Maximum number of events to generate per second per Worker Node. Defaults to 10.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttpRaw](#schemainputhttpraw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKinesis](#schemainputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» streamName|string|true|none|Kinesis Data Stream to read data from|
|»»» serviceInterval|number|false|none|Time interval in minutes between consecutive service calls|
|»»» shardExpr|string|false|none|A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.|
|»»» shardIteratorType|string|false|none|Location at which to start reading a shard for the first time|
|»»» payloadFormat|string|false|none|Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.|
|»»» getRecordsLimit|number|false|none|Maximum number of records per getRecords call|
|»»» getRecordsLimitTotal|number|false|none|Maximum number of records, across all shards, to pull down at once per Worker Process|
|»»» loadBalancingAlgorithm|string|false|none|The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» verifyKPLCheckSums|boolean|false|none|Verify Kinesis Producer Library (KPL) event checksums|
|»»» avoidDuplicates|boolean|false|none|When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblmetrics](#schemainputcriblmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» prefix|string|false|none|A prefix that is applied to the metrics provided by Cribl Stream|
|»»» fullFidelity|boolean|false|none|Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMetrics](#schemainputmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3](#schemainputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» tagAfterProcessing|boolean|false|none|Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3Inventory](#schemainputs3inventory)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» checksumSuffix|string|false|none|Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"|
|»»» maxManifestSizeKB|integer|false|none|Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.|
|»»» validateInventoryFiles|boolean|false|none|If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSnmp](#schemainputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|UDP port to receive SNMP traps on. Defaults to 162.|
|»»» snmpV3Auth|object|false|none|Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.|
|»»»» v3AuthEnabled|boolean|true|none|none|
|»»»» allowUnmatchedTrap|boolean|false|none|Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.|
|»»»» v3Users|[object]|false|none|User credentials for receiving v3 traps|
|»»»»» name|string|true|none|none|
|»»»»» authProtocol|string|false|none|none|
|»»»»» authKey|any|false|none|none|
|»»»»» privProtocol|any|false|none|none|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» varbindsWithTypes|boolean|false|none|If enabled, parses varbinds as an array of objects that include OID, value, and type|
|»»» bestEffortParsing|boolean|false|none|If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOpenTelemetry](#schemainputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|any|false|none|none|
|»»» captureHeaders|any|false|none|none|
|»»» activityLogSampleRate|any|false|none|none|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).|
|»»» enableHealthCheck|boolean|false|none|Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» protocol|string|false|none|Select whether to leverage gRPC or HTTP for OpenTelemetry|
|»»» extractSpans|boolean|false|none|Enable to extract each incoming span to a separate event|
|»»» extractMetrics|boolean|false|none|Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when interpreting received data|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» extractLogs|boolean|false|none|Enable to extract each incoming log record to a separate event|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputModelDrivenTelemetry](#schemainputmodeldriventelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» shutdownTimeoutMs|number|false|none|Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSqs](#schemainputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created)|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSyslog](#schemainputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» timestampTimezone|string|false|none|Timezone to assign to timestamps without timezone info|
|»»» singleMsgUdpPackets|boolean|false|none|Treat UDP packet data received as full syslog message|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» keepFieldsList|[string]|false|none|Wildcard list of fields to keep from source data; * = ALL (default)|
|»»» octetCounting|boolean|false|none|Enable if incoming messages use octet counting per RFC 6587.|
|»»» inferFraming|boolean|false|none|Enable if we should infer the syslog framing of the incoming messages.|
|»»» strictlyInferOctetCounting|boolean|false|none|Enable if we should infer octet counting only if the messages comply with RFC 5424.|
|»»» allowNonStandardAppName|boolean|false|none|Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|
|»»» enableEnhancedProxyHeaderParsing|boolean|false|none|When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFile](#schemainputfile)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» mode|string|false|none|Choose how to discover files to monitor|
|»»» interval|number|false|none|Time, in seconds, between scanning for files|
|»»» filenames|[string]|false|none|The full path of discovered files are matched against this wildcard list|
|»»» filterArchivedFiles|boolean|false|none|Apply filename allowlist to file entries in archive file types, like tar or zip.|
|»»» tailOnly|boolean|false|none|Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.|
|»»» idleTimeout|number|false|none|Time, in seconds, before an idle file is closed|
|»»» minAgeDur|string|false|none|The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.|
|»»» maxAgeDur|string|false|none|The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.|
|»»» checkFileModTime|boolean|false|none|Skip files with modification times earlier than the maximum age duration|
|»»» forceText|boolean|false|none|Forces files containing binary data to be streamed as text|
|»»» hashLen|number|false|none|Length of file header bytes to use in hash for unique file identification|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» description|string|false|none|none|
|»»» path|string|false|none|Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.|
|»»» depth|number|false|none|Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.|
|»»» suppressMissingPathErrors|boolean|false|none|none|
|»»» deleteFiles|boolean|false|none|Delete files after they have been collected|
|»»» includeUnidentifiableBinary|boolean|false|none|Stream binary files as Base64-encoded chunks.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcp](#schemainputtcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableHeader|boolean|false|none|Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAppscope](#schemainputappscope)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableUnixPath|boolean|false|none|Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.|
|»»» filter|object|false|none|none|
|»»»» allow|[object]|false|none|Specify processes that AppScope should be loaded into, and the config to use.|
|»»»»» procname|string|true|none|Specify the name of a process or family of processes.|
|»»»»» arg|string|false|none|Specify a string to substring-match against process command-line.|
|»»»»» config|string|true|none|Choose a config to apply to processes that match the process name and/or argument.|
|»»»» transportURL|string|false|none|To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events and metrics on disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|false|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» unixSocketPath|string|false|none|Path to the UNIX domain socket to listen on.|
|»»» unixSocketPerms|string|false|none|Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWef](#schemainputwef)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authMethod|string|false|none|How to authenticate incoming client connections|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|Enable TLS|
|»»»» rejectUnauthorized|boolean|false|none|Required for WEF certificate authentication|
|»»»» requestCert|boolean|false|none|Required for WEF certificate authentication|
|»»»» certificateName|string|false|none|Name of the predefined certificate|
|»»»» privKeyPath|string|true|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|true|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|true|none|Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»»» ocspCheck|boolean|false|none|Enable OCSP check of certificate|
|»»»» keytab|any|false|none|none|
|»»»» principal|any|false|none|none|
|»»»» ocspCheckFailClose|boolean|false|none|If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.|
|»»» captureHeaders|boolean|false|none|Add request headers to events in the __headers field|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» caFingerprint|string|false|none|SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain|
|»»» keytab|string|false|none|Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.|
|»»» principal|string|false|none|Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>|
|»»» allowMachineIdMismatch|boolean|false|none|Allow events to be ingested even if their MachineID does not match the client certificate CN|
|»»» subscriptions|[object]|true|none|Subscriptions to events on forwarding endpoints|
|»»»» subscriptionName|string|true|none|none|
|»»»» version|string|false|none|Version UUID for this subscription. If any subscription parameters are modified, this value will change.|
|»»»» contentFormat|string|true|none|Content format in which the endpoint should deliver events|
|»»»» heartbeatInterval|number|true|none|Maximum time (in seconds) between endpoint checkins before considering it unavailable|
|»»»» batchTimeout|number|true|none|Interval (in seconds) over which the endpoint should collect events before sending them to Stream|
|»»»» readExistingEvents|boolean|false|none|Newly subscribed endpoints will send previously existing events. Disable to receive new events only.|
|»»»» sendBookmarks|boolean|false|none|Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.|
|»»»» compress|boolean|false|none|Receive compressed events from the source|
|»»»» targets|[string]|true|none|The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com|
|»»»» locale|string|false|none|The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".|
|»»»» querySelector|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events ingested under this subscription|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» logFingerprintMismatch|boolean|false|none|Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWinEventLogs](#schemainputwineventlogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» logNames|[string]|true|none|Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.|
|»»» readMode|string|false|none|Read all stored and future event logs, or only future events|
|»»» eventFormat|string|false|none|Format of individual events|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)|
|»»» interval|number|false|none|Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» batchSize|number|false|none|The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxEventBytes|number|false|none|The maximum number of bytes in an event before it is flushed to the pipelines|
|»»» description|string|false|none|none|
|»»» disableJsonRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|
|»»» disableXmlRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputRawUdp](#schemainputrawudp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» singleMsgUdpPackets|boolean|false|none|If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.|
|»»» ingestRawBytes|boolean|false|none|If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputJournalFiles](#schemainputjournalfiles)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» path|string|true|none|Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.|
|»»» interval|number|false|none|Time, in seconds, between scanning for journals.|
|»»» journals|[string]|true|none|The full path of discovered journals are matched against this wildcard list.|
|»»» rules|[object]|false|none|Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Journal objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» currentBoot|boolean|false|none|Skip log messages that are not part of the current boot session.|
|»»» maxAgeDur|string|false|none|The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWiz](#schemainputwiz)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» endpoint|string|true|none|The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql|
|»»» authUrl|string|true|none|The authentication URL to generate an OAuth token|
|»»» authAudienceOverride|string|false|none|The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.|
|»»» clientId|string|true|none|The client ID of the Wiz application|
|»»» contentConfig|[object]|true|none|none|
|»»»» contentType|string|true|none|The name of the Wiz query|
|»»»» contentDescription|string|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 to disable.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret of the Wiz application|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWizWebhook](#schemainputwizwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputNetflow](#schemainputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» enablePassThrough|boolean|false|none|Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» templateCacheMinutes|number|false|none|Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.|
|»»» v5Enabled|boolean|false|none|Accept messages in Netflow V5 format.|
|»»» v9Enabled|boolean|false|none|Accept messages in Netflow V9 format.|
|»»» ipfixEnabled|boolean|false|none|Accept messages in IPFIX format.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSecurityLake](#schemainputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputZscalerHec](#schemainputzscalerhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» hecAcks|boolean|false|none|Whether to enable Zscaler HEC acknowledgements|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCloudflareHec](#schemainputcloudflarehec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» accessControlAllowOrigin|[string]|false|none|HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|collection|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|kafka|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|msk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|splunk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|compress|disabled|
|compress|auto|
|compress|always|
|type|splunk_search|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|outputMode|csv|
|outputMode|json|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|azure_blob|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|type|elastic|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|authTokens|
|apiVersion|6.8.4|
|apiVersion|8.3.2|
|apiVersion|custom|
|authType|none|
|authType|manual|
|authType|secret|
|type|confluent_cloud|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|type|grafana|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|loki|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus_rw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|authType|manual|
|authType|secret|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|edge_prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|discoveryType|k8s-node|
|discoveryType|k8s-pods|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|kubernetes|
|protocol|http|
|protocol|https|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|office365_mgmt|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_service|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_msg_trace|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|oauth|
|authType|oauthSecret|
|authType|oauthCert|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|logLevel|silly|
|type|none|
|type|backoff|
|type|static|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|type|eventhub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|type|exec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|scheduleType|interval|
|scheduleType|cronSchedule|
|type|firehose|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|google_pubsub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|type|cribl|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|cribl_tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_lake_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|tcpjson|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|system_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|system_state|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_events|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|windows_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|crowdstrike|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|datadog_agent|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|datagen|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|http_raw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|kinesis|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|shardIteratorType|TRIM_HORIZON|
|shardIteratorType|LATEST|
|payloadFormat|cribl|
|payloadFormat|ndjson|
|payloadFormat|cloudwatch|
|payloadFormat|line|
|loadBalancingAlgorithm|ConsistentHashing|
|loadBalancingAlgorithm|RoundRobin|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|criblmetrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|s3|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|s3_inventory|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|snmp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authProtocol|none|
|authProtocol|md5|
|authProtocol|sha|
|authProtocol|sha224|
|authProtocol|sha256|
|authProtocol|sha384|
|authProtocol|sha512|
|type|open_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|model_driven_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|sqs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|syslog|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|file|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|manual|
|mode|auto|
|type|tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|appscope|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|wef|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authMethod|clientCert|
|authMethod|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|contentFormat|Raw|
|contentFormat|RenderedText|
|querySelector|simple|
|querySelector|xml|
|type|win_event_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|readMode|oldest|
|readMode|newest|
|eventFormat|json|
|eventFormat|xml|
|type|raw_udp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|journal_files|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|wiz|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|wiz_webhook|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|netflow|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|security_lake|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|zscaler_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cloudflare_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

## Delete a Source

<a id="opIddeleteInputById"></a>

> Code samples

`DELETE /system/inputs/{id}`

Delete the specified Source.

<h3 id="delete-a-source-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Source to delete.|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "collection",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "preprocess": {
        "disabled": true,
        "command": "string",
        "args": [
          "string"
        ]
      },
      "throttleRatePerSec": "0",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "output": "string"
    }
  ]
}
```

<h3 id="delete-a-source-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="delete-a-source-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[oneOf]|false|none|none|

*oneOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCollection](#schemainputcollection)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process results|
|»»» sendToRoutes|boolean|false|none|Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» output|string|false|none|Destination to send results to|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKafka](#schemainputkafka)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMsk](#schemainputmsk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the MSK cluster is located|
|»»» endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttp](#schemainputhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunk](#schemainputsplunk)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|»»»» description|string|false|none|none|
|»»» maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|»»» description|string|false|none|none|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» compress|string|false|none|Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkSearch](#schemainputsplunksearch)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» searchHead|string|true|none|Search head base URL. Can be an expression. Default is https://localhost:8089.|
|»»» search|string|true|none|Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'|
|»»» earliest|string|false|none|The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'|
|»»» latest|string|false|none|The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'|
|»»» cronSchedule|string|true|none|A cron schedule on which to run this job|
|»»» endpoint|string|true|none|REST API used to create a search|
|»»» outputMode|string|true|none|Format of the returned output|
|»»» endpointParams|[object]|false|none|Optional request parameters to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» endpointHeaders|[object]|false|none|Optional request headers to send to the endpoint|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|»»» logLevel|string|false|none|Collector runtime log level (verbosity)|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 for no timeout.|
|»»» useRoundRobinDns|boolean|false|none|When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» authType|string|false|none|Splunk Search authentication type|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSplunkHec](#schemainputsplunkhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAzureBlob](#schemainputazureblob)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» visibilityTimeout|number|false|none|The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» maxMessages|number|false|none|The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.|
|»»» servicePeriodSecs|number|false|none|The duration (in seconds) which pollers should be validated and restarted if exited|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» authType|string|false|none|none|
|»»» description|string|false|none|none|
|»»» connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» storageAccountName|string|false|none|The name of your Azure storage account|
|»»» tenantId|string|false|none|The service principal's tenant ID|
|»»» clientId|string|false|none|The service principal's client ID|
|»»» azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|»»» endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»» certificate|object|false|none|none|
|»»»» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputElastic](#schemainputelastic)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» elasticAPI|string|true|none|Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.|
|»»» authType|string|false|none|none|
|»»» apiVersion|string|false|none|The API version to use for communicating with the server|
|»»» extraHttpHeaders|[object]|false|none|Headers to add to all events|
|»»»» name|string|false|none|none|
|»»»» value|string|true|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» url|string|false|none|URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|»»»» removeHeaders|[string]|false|none|List of headers to remove from the request to proxy|
|»»»» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a proxy request to complete before canceling it|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» authTokens|[string]|false|none|Bearer tokens to include in the authorization header|
|»»» customAPIVersion|string|false|none|Custom version information to respond to requests|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputConfluentCloud](#schemainputconfluentcloud)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|»»» groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|»»» kafkaSchemaRegistry|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|»»»» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|»»»» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|»»»» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|»»»» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»»»»» disabled|boolean|true|none|none|
|»»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» tls|object|false|none|none|
|»»»»» disabled|boolean|false|none|none|
|»»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»»»»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»»»»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»»» minVersion|string|false|none|none|
|»»»»» maxVersion|string|false|none|none|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» mechanism|string|false|none|none|
|»»»» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|»»»» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|»»»» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|»»»» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|»»»» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|»»»» clientId|string|false|none|Client ID to use for OAuth authentication|
|»»»» oauthSecretType|string|false|none|none|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»»» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|none|
|»»» sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|»»» heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGrafana](#schemainputgrafana)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|false|none|Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.|
|»»» lokiAPI|string|false|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.|
|»»» prometheusAuth|object|false|none|none|
|»»»» authType|string|false|none|Remote Write authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» lokiAuth|object|false|none|none|
|»»»» authType|string|false|none|Loki logs authentication type|
|»»»» username|string|false|none|none|
|»»»» password|string|false|none|none|
|»»»» token|string|false|none|Bearer token to include in the authorization header|
|»»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» loginUrl|string|false|none|URL for OAuth|
|»»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth parameter name|
|»»»»» value|string|true|none|OAuth parameter value|
|»»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»»» name|string|true|none|OAuth header name|
|»»»»» value|string|true|none|OAuth header value|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputLoki](#schemainputloki)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» lokiAPI|string|true|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'.|
|»»» authType|string|false|none|Loki logs authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheusRw](#schemainputprometheusrw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» prometheusAPI|string|true|none|Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write.|
|»»» authType|string|false|none|Remote Write authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputPrometheus](#schemainputprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|false|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail.|
|»»» logLevel|string|true|none|Collector runtime Log Level|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targetList|[string]|false|none|List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.|
|»»»» Targets|string|false|none|none|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEdgePrometheus](#schemainputedgeprometheus)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» dimensionList|[string]|false|none|Other dimensions to include in events|
|»»»» dimension|string|false|none|none|
|»»» discoveryType|string|true|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|»»» interval|number|true|none|How often in seconds to scrape targets for metrics.|
|»»» timeout|number|false|none|Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authType|string|false|none|Enter credentials directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» targets|[object]|false|none|none|
|»»»» protocol|string|false|none|Protocol to use when collecting metrics|
|»»»» host|string|true|none|Name of host from which to pull metrics.|
|»»»» port|number|false|none|The port number in the metrics URL for discovered targets.|
|»»»» path|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» recordType|string|false|none|DNS Record type to resolve|
|»»» scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|»»» nameList|[string]|false|none|List of DNS names to resolve|
|»»»» DNS Names|string|false|none|none|
|»»» scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|»»» searchFilter|[object]|false|none|EC2 Instance Search Filter|
|»»»» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|»»»» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|Region where the EC2 is located|
|»»» endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» scrapeProtocolExpr|string|false|none|Protocol to use when collecting metrics|
|»»» scrapePortExpr|string|false|none|The port number in the metrics URL for discovered targets.|
|»»» scrapePathExpr|string|false|none|Path to use when collecting metrics from discovered targets|
|»»» podFilter|[object]|false|none|Add rules to decide which pods to discover for metrics.<br>  Pods are searched if no rules are given or of all the rules'<br>  expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to pods objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» username|string|false|none|Username for Prometheus Basic authentication|
|»»» password|string|false|none|Password for Prometheus Basic authentication|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Mgmt](#schemainputoffice365mgmt)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|true|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» publisherIdentifier|string|false|none|Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Management Activity API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» ingestionLag|number|false|none|Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365Service](#schemainputoffice365service)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» tenantId|string|true|none|Office 365 Azure Tenant ID|
|»»» appId|string|true|none|Office 365 Azure Application ID|
|»»» timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» contentConfig|[object]|false|none|Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.|
|»»»» contentType|string|false|none|Office 365 Services API Content Type|
|»»»» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|»»»» interval|number|false|none|none|
|»»»» logLevel|string|false|none|Collector runtime Log Level|
|»»»» enabled|boolean|false|none|none|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|Office 365 Azure client secret|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOffice365MsgTrace](#schemainputoffice365msgtrace)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» url|string|true|none|URL to use when retrieving report data.|
|»»» interval|number|true|none|How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.|
|»»» startDate|string|false|none|Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.|
|»»» endDate|string|false|none|Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.|
|»»» timeout|number|false|none|HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.|
|»»» disableTimeFilter|boolean|false|none|Disables time filtering of events when a date range is specified.|
|»»» authType|string|false|none|Select authentication method.|
|»»» rescheduleDroppedTasks|boolean|false|none|Reschedule tasks that failed with non-fatal errors|
|»»» maxTaskReschedule|number|false|none|Maximum number of times a task can be rescheduled|
|»»» logLevel|string|false|none|Log Level (verbosity) for collection runtime behavior.|
|»»» jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» description|string|false|none|none|
|»»» username|string|false|none|Username to run Message Trace API call.|
|»»» password|string|false|none|Password to run Message Trace API call.|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials.|
|»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter.|
|»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory.|
|»»» clientId|string|false|none|client_id to pass in the OAuth request parameter.|
|»»» resource|string|false|none|Resource to pass in the OAuth request parameter.|
|»»» planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|»»» textSecret|string|false|none|Select or create a secret that references your client_secret to pass in the OAuth request parameter.|
|»»» certOptions|object|false|none|none|
|»»»» certificateName|string|false|none|The name of the predefined certificate.|
|»»»» privKeyPath|string|true|none|Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt the private key.|
|»»»» certPath|string|true|none|Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputEventhub](#schemainputeventhub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|»»» topics|[string]|true|none|The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.|
|»»» groupId|string|false|none|The consumer group this instance belongs to. Default is 'Cribl'.|
|»»» fromBeginning|boolean|false|none|Start reading from earliest available data; relevant only during initial subscription|
|»»» connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|»»» requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|»»» maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|»»» maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|»»» initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|»»» backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|»»» authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|»»» reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|»»» sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|»»»» disabled|boolean|true|none|none|
|»»»» authType|string|false|none|Enter password directly, or select a stored secret|
|»»»» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|»»»» textSecret|string|false|none|Select or create a stored text secret|
|»»»» mechanism|string|false|none|none|
|»»»» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|»»»» clientSecretAuthType|string|false|none|none|
|»»»» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|»»»» clientTextSecret|string|false|none|Select or create a stored text secret|
|»»»» certificateName|string|false|none|Select or create a stored certificate|
|»»»» certPath|string|false|none|none|
|»»»» privKeyPath|string|false|none|none|
|»»»» passphrase|string|false|none|none|
|»»»» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|»»»» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|»»»» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|»»»» scope|string|false|none|Scope to pass in the OAuth request parameter|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|»»» sessionTimeout|number|false|none|Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.<br>      Value must be lower than rebalanceTimeout.<br>      See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» rebalanceTimeout|number|false|none|Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» heartbeatInterval|number|false|none|Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|»»» autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|»»» maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|»»» maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|»»» maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|»»» minimizeDuplicates|boolean|false|none|Minimize duplicate events by starting only one consumer for each topic partition|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputExec](#schemainputexec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» command|string|true|none|Command to execute; supports Bourne shell (or CMD on Windows) syntax|
|»»» retries|number|false|none|Maximum number of retry attempts in the event that the command fails|
|»»» scheduleType|string|false|none|Select a schedule type; either an interval (in seconds) or a cron-style schedule.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» interval|number|false|none|Interval between command executions in seconds.|
|»»» cronSchedule|string|false|none|Cron schedule to execute the command on.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFirehose](#schemainputfirehose)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputGooglePubsub](#schemainputgooglepubsub)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» topicName|string|true|none|ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.|
|»»» subscriptionName|string|true|none|ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription|
|»»» monitorSubscription|boolean|false|none|Use when the subscription is not created by this Source and topic is not known|
|»»» createTopic|boolean|false|none|Create topic if it does not exist|
|»»» createSubscription|boolean|false|none|Create subscription if it does not exist|
|»»» region|string|false|none|Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|»»» googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|»»» serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|»»» secret|string|false|none|Select or create a stored text secret|
|»»» maxBacklog|number|false|none|If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events|
|»»» concurrency|number|false|none|How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.|
|»»» requestTimeout|number|false|none|Pull request timeout, in milliseconds|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» orderedDelivery|boolean|false|none|Receive events in the order they were added to the queue. The process sending events must have ordering enabled.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCribl](#schemainputcribl)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» filter|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblTcp](#schemainputcribltcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblHttp](#schemainputcriblhttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.|
|»»»» tokenSecret|string|true|none|Select or create a stored text secret|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|Optional token description|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblLakeHttp](#schemainputcribllakehttp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|»»» elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|»»» splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|»»» splunkHecAcks|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» authTokensExt|[object]|false|none|none|
|»»»» token|string|true|none|none|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»»» splunkHecMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»»» elasticsearchMetadata|object|false|none|none|
|»»»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcpjson](#schemainputtcpjson)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemMetrics](#schemainputsystemmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for system metrics|
|»»»»»» processes|boolean|false|none|Generate metrics for the numbers of processes in various states|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of detail for disk metrics|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» inodes|boolean|false|none|Generate filesystem inode metrics|
|»»»»»» devices|[string]|false|none|Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.|
|»»»»»» mountpoints|[string]|false|none|Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.|
|»»»»»» fstypes|[string]|false|none|Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.|
|»»»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» container|object|false|none|none|
|»»»» mode|string|false|none|Select the level of detail for container metrics|
|»»»» dockerSocket|[string]|false|none|Full paths for Docker's UNIX-domain socket|
|»»»» dockerTimeout|number|false|none|Timeout, in seconds, for the Docker API|
|»»»» filters|[object]|false|none|Containers matching any of these will be included. All are included if no filters are added.|
|»»»»» expr|string|true|none|none|
|»»»» allContainers|boolean|false|none|Include stopped and paused containers|
|»»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|»»»» detail|boolean|false|none|Generate full container metrics|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSystemState](#schemainputsystemstate)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» collectors|object|false|none|none|
|»»»» hostsfile|object|false|none|Creates events based on entries collected from the hosts file|
|»»»»» enable|boolean|false|none|none|
|»»»» interfaces|object|false|none|Creates events for each of the host’s network interfaces|
|»»»»» enable|boolean|false|none|none|
|»»»» disk|object|false|none|Creates events for physical disks, partitions, and file systems|
|»»»»» enable|boolean|false|none|none|
|»»»» metadata|object|false|none|Creates events based on the host system’s current state|
|»»»»» enable|boolean|false|none|none|
|»»»» routes|object|false|none|Creates events based on entries collected from the host’s network routes|
|»»»»» enable|boolean|false|none|none|
|»»»» dns|object|false|none|Creates events for DNS resolvers and search entries|
|»»»»» enable|boolean|false|none|none|
|»»»» user|object|false|none|Creates events for local users and groups|
|»»»»» enable|boolean|false|none|none|
|»»»» firewall|object|false|none|Creates events for Firewall rules entries|
|»»»»» enable|boolean|false|none|none|
|»»»» services|object|false|none|Creates events from the list of services|
|»»»»» enable|boolean|false|none|none|
|»»»» ports|object|false|none|Creates events from list of listening ports|
|»»»»» enable|boolean|false|none|none|
|»»»» loginUsers|object|false|none|Creates events from list of logged-in users|
|»»»»» enable|boolean|false|none|none|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeMetrics](#schemainputkubemetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metrics collections. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics on disk for Cribl Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeLogs](#schemainputkubelogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between checks for new containers. Default is 15 secs.|
|»»» rules|[object]|false|none|Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Pod objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» timestamps|boolean|false|none|For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|»»»» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|»»»» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|»»»» compress|string|false|none|Data compression format. Default is gzip.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKubeEvents](#schemainputkubeevents)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» rules|[object]|false|none|Filtering on event fields|
|»»»» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWindowsMetrics](#schemainputwindowsmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|»»» host|object|false|none|none|
|»»»» mode|string|false|none|Select level of detail for host metrics|
|»»»» custom|object|false|none|none|
|»»»»» system|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for system metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all system information|
|»»»»» cpu|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for CPU metrics|
|»»»»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»»»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»»»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»»»»» memory|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for memory metrics|
|»»»»»» detail|boolean|false|none|Generate metrics for all memory states|
|»»»»» network|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for network metrics|
|»»»»»» detail|boolean|false|none|Generate full network metrics|
|»»»»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»»»»» devices|[string]|false|none|Network interfaces to include/exclude. All interfaces are included if this list is empty.|
|»»»»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»»»»» disk|object|false|none|none|
|»»»»»» mode|string|false|none|Select the level of details for disk metrics|
|»»»»»» perVolume|boolean|false|none|Generate separate metrics for each volume|
|»»»»»» detail|boolean|false|none|Generate full disk metrics|
|»»»»»» volumes|[string]|false|none|Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.|
|»»» process|object|false|none|none|
|»»»» sets|[object]|false|none|Configure sets to collect process metrics|
|»»»»» name|string|true|none|none|
|»»»»» filter|string|true|none|none|
|»»»»» includeChildren|boolean|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCrowdstrike](#schemainputcrowdstrike)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatadogAgent](#schemainputdatadogagent)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» extractMetrics|boolean|false|none|Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» proxyMode|object|false|none|none|
|»»»» enabled|boolean|true|none|Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.|
|»»»» rejectUnauthorized|boolean|false|none|Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputDatagen](#schemainputdatagen)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» samples|[object]|true|none|none|
|»»»» sample|string|true|none|none|
|»»»» eventsPerSec|number|true|none|Maximum number of events to generate per second per Worker Node. Defaults to 10.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputHttpRaw](#schemainputhttpraw)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputKinesis](#schemainputkinesis)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» streamName|string|true|none|Kinesis Data Stream to read data from|
|»»» serviceInterval|number|false|none|Time interval in minutes between consecutive service calls|
|»»» shardExpr|string|false|none|A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.|
|»»» shardIteratorType|string|false|none|Location at which to start reading a shard for the first time|
|»»» payloadFormat|string|false|none|Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.|
|»»» getRecordsLimit|number|false|none|Maximum number of records per getRecords call|
|»»» getRecordsLimitTotal|number|false|none|Maximum number of records, across all shards, to pull down at once per Worker Process|
|»»» loadBalancingAlgorithm|string|false|none|The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|true|none|Region where the Kinesis stream is located|
|»»» endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» verifyKPLCheckSums|boolean|false|none|Verify Kinesis Producer Library (KPL) event checksums|
|»»» avoidDuplicates|boolean|false|none|When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCriblmetrics](#schemainputcriblmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» prefix|string|false|none|A prefix that is applied to the metrics provided by Cribl Stream|
|»»» fullFidelity|boolean|false|none|Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputMetrics](#schemainputmetrics)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3](#schemainputs3)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» tagAfterProcessing|boolean|false|none|Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputS3Inventory](#schemainputs3inventory)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» checksumSuffix|string|false|none|Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"|
|»»» maxManifestSizeKB|integer|false|none|Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.|
|»»» validateInventoryFiles|boolean|false|none|If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSnmp](#schemainputsnmp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|UDP port to receive SNMP traps on. Defaults to 162.|
|»»» snmpV3Auth|object|false|none|Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.|
|»»»» v3AuthEnabled|boolean|true|none|none|
|»»»» allowUnmatchedTrap|boolean|false|none|Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.|
|»»»» v3Users|[object]|false|none|User credentials for receiving v3 traps|
|»»»»» name|string|true|none|none|
|»»»»» authProtocol|string|false|none|none|
|»»»»» authKey|any|false|none|none|
|»»»»» privProtocol|any|false|none|none|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» varbindsWithTypes|boolean|false|none|If enabled, parses varbinds as an array of objects that include OID, value, and type|
|»»» bestEffortParsing|boolean|false|none|If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputOpenTelemetry](#schemainputopentelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|any|false|none|none|
|»»» captureHeaders|any|false|none|none|
|»»» activityLogSampleRate|any|false|none|none|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).|
|»»» enableHealthCheck|boolean|false|none|Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» protocol|string|false|none|Select whether to leverage gRPC or HTTP for OpenTelemetry|
|»»» extractSpans|boolean|false|none|Enable to extract each incoming span to a separate event|
|»»» extractMetrics|boolean|false|none|Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point|
|»»» otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when interpreting received data|
|»»» authType|string|false|none|OpenTelemetry authentication type|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» description|string|false|none|none|
|»»» username|string|false|none|none|
|»»» password|string|false|none|none|
|»»» token|string|false|none|Bearer token to include in the authorization header|
|»»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|»»» textSecret|string|false|none|Select or create a stored text secret|
|»»» loginUrl|string|false|none|URL for OAuth|
|»»» secretParamName|string|false|none|Secret parameter name to pass in request body|
|»»» secret|string|false|none|Secret parameter value to pass in request body|
|»»» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|»»» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|»»» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|»»» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth parameter name|
|»»»» value|string|true|none|OAuth parameter value|
|»»» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»»»» name|string|true|none|OAuth header name|
|»»»» value|string|true|none|OAuth header value|
|»»» extractLogs|boolean|false|none|Enable to extract each incoming log record to a separate event|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputModelDrivenTelemetry](#schemainputmodeldriventelemetry)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» shutdownTimeoutMs|number|false|none|Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSqs](#schemainputsqs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» queueType|string|true|none|The queue type used (or created)|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» createQueue|boolean|false|none|Create queue if it does not exist|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSyslog](#schemainputsyslog)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|»»» tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» timestampTimezone|string|false|none|Timezone to assign to timestamps without timezone info|
|»»» singleMsgUdpPackets|boolean|false|none|Treat UDP packet data received as full syslog message|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|»»» keepFieldsList|[string]|false|none|Wildcard list of fields to keep from source data; * = ALL (default)|
|»»» octetCounting|boolean|false|none|Enable if incoming messages use octet counting per RFC 6587.|
|»»» inferFraming|boolean|false|none|Enable if we should infer the syslog framing of the incoming messages.|
|»»» strictlyInferOctetCounting|boolean|false|none|Enable if we should infer octet counting only if the messages comply with RFC 5424.|
|»»» allowNonStandardAppName|boolean|false|none|Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|»»» description|string|false|none|none|
|»»» enableEnhancedProxyHeaderParsing|boolean|false|none|When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.|

*anyOf*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*or*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»»» *anonymous*|object|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputFile](#schemainputfile)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» mode|string|false|none|Choose how to discover files to monitor|
|»»» interval|number|false|none|Time, in seconds, between scanning for files|
|»»» filenames|[string]|false|none|The full path of discovered files are matched against this wildcard list|
|»»» filterArchivedFiles|boolean|false|none|Apply filename allowlist to file entries in archive file types, like tar or zip.|
|»»» tailOnly|boolean|false|none|Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.|
|»»» idleTimeout|number|false|none|Time, in seconds, before an idle file is closed|
|»»» minAgeDur|string|false|none|The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.|
|»»» maxAgeDur|string|false|none|The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.|
|»»» checkFileModTime|boolean|false|none|Skip files with modification times earlier than the maximum age duration|
|»»» forceText|boolean|false|none|Forces files containing binary data to be streamed as text|
|»»» hashLen|number|false|none|Length of file header bytes to use in hash for unique file identification|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» description|string|false|none|none|
|»»» path|string|false|none|Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.|
|»»» depth|number|false|none|Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.|
|»»» suppressMissingPathErrors|boolean|false|none|none|
|»»» deleteFiles|boolean|false|none|Delete files after they have been collected|
|»»» includeUnidentifiableBinary|boolean|false|none|Stream binary files as Base64-encoded chunks.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputTcp](#schemainputtcp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableHeader|boolean|false|none|Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» description|string|false|none|none|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputAppscope](#schemainputappscope)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|»»» maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|»»» socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|»»» socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|»»» socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|»»» enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» enableUnixPath|boolean|false|none|Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.|
|»»» filter|object|false|none|none|
|»»»» allow|[object]|false|none|Specify processes that AppScope should be loaded into, and the config to use.|
|»»»»» procname|string|true|none|Specify the name of a process or family of processes.|
|»»»»» arg|string|false|none|Specify a string to substring-match against process command-line.|
|»»»»» config|string|true|none|Choose a config to apply to processes that match the process name and/or argument.|
|»»»» transportURL|string|false|none|To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.|
|»»» persistence|object|false|none|none|
|»»»» enable|boolean|false|none|Spool events and metrics on disk for Cribl Edge and Search|
|»»»» timeWindow|string|false|none|Time span for each file bucket|
|»»»» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|»»»» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|»»»» compress|string|false|none|none|
|»»»» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» description|string|false|none|none|
|»»» host|string|false|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|false|none|Port to listen on|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» unixSocketPath|string|false|none|Path to the UNIX domain socket to listen on.|
|»»» unixSocketPerms|string|false|none|Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.|
|»»» authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWef](#schemainputwef)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authMethod|string|false|none|How to authenticate incoming client connections|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|Enable TLS|
|»»»» rejectUnauthorized|boolean|false|none|Required for WEF certificate authentication|
|»»»» requestCert|boolean|false|none|Required for WEF certificate authentication|
|»»»» certificateName|string|false|none|Name of the predefined certificate|
|»»»» privKeyPath|string|true|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|true|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|true|none|Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»»» ocspCheck|boolean|false|none|Enable OCSP check of certificate|
|»»»» keytab|any|false|none|none|
|»»»» principal|any|false|none|none|
|»»»» ocspCheckFailClose|boolean|false|none|If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.|
|»»» captureHeaders|boolean|false|none|Add request headers to events in the __headers field|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» caFingerprint|string|false|none|SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain|
|»»» keytab|string|false|none|Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.|
|»»» principal|string|false|none|Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>|
|»»» allowMachineIdMismatch|boolean|false|none|Allow events to be ingested even if their MachineID does not match the client certificate CN|
|»»» subscriptions|[object]|true|none|Subscriptions to events on forwarding endpoints|
|»»»» subscriptionName|string|true|none|none|
|»»»» version|string|false|none|Version UUID for this subscription. If any subscription parameters are modified, this value will change.|
|»»»» contentFormat|string|true|none|Content format in which the endpoint should deliver events|
|»»»» heartbeatInterval|number|true|none|Maximum time (in seconds) between endpoint checkins before considering it unavailable|
|»»»» batchTimeout|number|true|none|Interval (in seconds) over which the endpoint should collect events before sending them to Stream|
|»»»» readExistingEvents|boolean|false|none|Newly subscribed endpoints will send previously existing events. Disable to receive new events only.|
|»»»» sendBookmarks|boolean|false|none|Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.|
|»»»» compress|boolean|false|none|Receive compressed events from the source|
|»»»» targets|[string]|true|none|The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com|
|»»»» locale|string|false|none|The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".|
|»»»» querySelector|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events ingested under this subscription|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|
|»»» logFingerprintMismatch|boolean|false|none|Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWinEventLogs](#schemainputwineventlogs)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» logNames|[string]|true|none|Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.|
|»»» readMode|string|false|none|Read all stored and future event logs, or only future events|
|»»» eventFormat|string|false|none|Format of individual events|
|»»» disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)|
|»»» interval|number|false|none|Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» batchSize|number|false|none|The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» maxEventBytes|number|false|none|The maximum number of bytes in an event before it is flushed to the pipelines|
|»»» description|string|false|none|none|
|»»» disableJsonRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|
|»»» disableXmlRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputRawUdp](#schemainputrawudp)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|»»» ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|»»» singleMsgUdpPackets|boolean|false|none|If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.|
|»»» ingestRawBytes|boolean|false|none|If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputJournalFiles](#schemainputjournalfiles)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» path|string|true|none|Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.|
|»»» interval|number|false|none|Time, in seconds, between scanning for journals.|
|»»» journals|[string]|true|none|The full path of discovered journals are matched against this wildcard list.|
|»»» rules|[object]|false|none|Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.|
|»»»» filter|string|true|none|JavaScript expression applied to Journal objects. Return 'true' to include it.|
|»»»» description|string|false|none|Optional description of this rule's purpose|
|»»» currentBoot|boolean|false|none|Skip log messages that are not part of the current boot session.|
|»»» maxAgeDur|string|false|none|The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWiz](#schemainputwiz)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» endpoint|string|true|none|The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql|
|»»» authUrl|string|true|none|The authentication URL to generate an OAuth token|
|»»» authAudienceOverride|string|false|none|The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.|
|»»» clientId|string|true|none|The client ID of the Wiz application|
|»»» contentConfig|[object]|true|none|none|
|»»»» contentType|string|true|none|The name of the Wiz query|
|»»»» contentDescription|string|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»» requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 to disable.|
|»»» keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|»»» maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|»»» ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|»»» ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» retryRules|object|false|none|none|
|»»»» type|string|true|none|The algorithm to use when performing HTTP retries|
|»»»» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|»»»» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|»»»» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|»»»» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|»»»» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|»»»» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|»»»» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|»»» authType|string|false|none|Enter client secret directly, or select a stored secret|
|»»» description|string|false|none|none|
|»»» clientSecret|string|false|none|The client secret of the Wiz application|
|»»» textSecret|string|false|none|Select or create a stored text secret|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputWizWebhook](#schemainputwizwebhook)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedPaths|[string]|false|none|List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all.|
|»»» allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|»»» authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|»»»» description|string|false|none|none|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputNetflow](#schemainputnetflow)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|»»» port|number|true|none|Port to listen on|
|»»» enablePassThrough|boolean|false|none|Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|»»» templateCacheMinutes|number|false|none|Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.|
|»»» v5Enabled|boolean|false|none|Accept messages in Netflow V5 format.|
|»»» v9Enabled|boolean|false|none|Accept messages in Netflow V9 format.|
|»»» ipfixEnabled|boolean|false|none|Accept messages in IPFIX format.|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputSecurityLake](#schemainputsecuritylake)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|»»» fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|»»» awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|»»» awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|»»» awsSecretKey|string|false|none|none|
|»»» region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|»»» endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|»»» signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|»»» reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|»»» visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|»»» numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|»»» socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|»»» skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|»»» includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|»»» enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|»»» assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|»»» assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|»»» durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|»»» enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|»»» preprocess|object|false|none|none|
|»»»» disabled|boolean|true|none|none|
|»»»» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|»»»» args|[string]|false|none|Arguments to be added to the custom command|
|»»» metadata|[object]|false|none|Fields to add to events from this input|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|»»» parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|»»» checkpointing|object|false|none|none|
|»»»» enabled|boolean|true|none|Resume processing files after an interruption|
|»»»» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|»»» pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|»»» encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|»»» description|string|false|none|none|
|»»» awsApiKey|string|false|none|none|
|»»» awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|»»» tagAfterProcessing|any|false|none|none|
|»»» processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|»»» processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputZscalerHec](#schemainputzscalerhec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|true|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» hecAcks|boolean|false|none|Whether to enable Zscaler HEC acknowledgements|
|»»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

*xor*

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|»» *anonymous*|[InputCloudflareHec](#schemainputcloudflarehec)|false|none|none|
|»»» id|string|false|none|Unique ID for this input|
|»»» type|string|true|none|none|
|»»» disabled|boolean|false|none|none|
|»»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»»» pipeline|string|false|none|none|
|»»»» output|string|true|none|none|
|»»» pq|object|false|none|none|
|»»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»»» pqControls|object|false|none|none|
|»»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»»» port|number|true|none|Port to listen on|
|»»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»»» authType|string|false|none|Select Secret to use a text secret to authenticate|
|»»»» tokenSecret|any|false|none|none|
|»»»» token|any|false|none|none|
|»»»» enabled|boolean|false|none|none|
|»»»» description|string|false|none|none|
|»»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»»» name|string|true|none|none|
|»»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» tls|object|false|none|none|
|»»»» disabled|boolean|false|none|none|
|»»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»»» certificateName|string|false|none|The name of the predefined certificate|
|»»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»»» minVersion|string|false|none|none|
|»»»» maxVersion|string|false|none|none|
|»»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»»» enableHealthCheck|any|false|none|none|
|»»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»»» hecAPI|string|true|none|Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.|
|»»» metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»»» accessControlAllowOrigin|[string]|false|none|HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»»» accessControlAllowHeaders|[string]|false|none|HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|collection|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|kafka|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|msk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|splunk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|compress|disabled|
|compress|auto|
|compress|always|
|type|splunk_search|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|outputMode|csv|
|outputMode|json|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|azure_blob|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|
|type|elastic|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|authTokens|
|apiVersion|6.8.4|
|apiVersion|8.3.2|
|apiVersion|custom|
|authType|none|
|authType|manual|
|authType|secret|
|type|confluent_cloud|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|type|grafana|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|loki|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus_rw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|authType|manual|
|authType|secret|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|edge_prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|discoveryType|k8s-node|
|discoveryType|k8s-pods|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|kubernetes|
|protocol|http|
|protocol|https|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|office365_mgmt|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_service|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|office365_msg_trace|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|oauth|
|authType|oauthSecret|
|authType|oauthCert|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|logLevel|silly|
|type|none|
|type|backoff|
|type|static|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|type|eventhub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|
|type|exec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|scheduleType|interval|
|scheduleType|cronSchedule|
|type|firehose|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|google_pubsub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|
|type|cribl|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|cribl_tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cribl_lake_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|tcpjson|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|system_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|system_state|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|type|kube_events|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|windows_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|
|type|crowdstrike|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|datadog_agent|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|datagen|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|http_raw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|kinesis|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|shardIteratorType|TRIM_HORIZON|
|shardIteratorType|LATEST|
|payloadFormat|cribl|
|payloadFormat|ndjson|
|payloadFormat|cloudwatch|
|payloadFormat|line|
|loadBalancingAlgorithm|ConsistentHashing|
|loadBalancingAlgorithm|RoundRobin|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|criblmetrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|s3|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|s3_inventory|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|snmp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authProtocol|none|
|authProtocol|md5|
|authProtocol|sha|
|authProtocol|sha224|
|authProtocol|sha256|
|authProtocol|sha384|
|authProtocol|sha512|
|type|open_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|type|model_driven_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|sqs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|type|syslog|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|file|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|manual|
|mode|auto|
|type|tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|type|appscope|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|wef|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authMethod|clientCert|
|authMethod|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|contentFormat|Raw|
|contentFormat|RenderedText|
|querySelector|simple|
|querySelector|xml|
|type|win_event_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|readMode|oldest|
|readMode|newest|
|eventFormat|json|
|eventFormat|xml|
|type|raw_udp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|journal_files|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|wiz|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|
|type|wiz_webhook|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|netflow|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|security_lake|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|
|type|zscaler_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|type|cloudflare_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

## Get a list of Source Status objects

<a id="opIdlistInputStatus"></a>

> Code samples

`GET /system/status/inputs`

Get a list of Source Status objects

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "status": {
        "error": {
          "details": {},
          "message": "string"
        },
        "health": "Green",
        "healthCounts": {
          "Green": 0,
          "Yellow": 0,
          "Red": 0,
          "Unknown": 0
        },
        "metrics": {},
        "pq": {
          "error": {
            "details": {},
            "message": "string"
          },
          "health": "Green",
          "healthCounts": {
            "Green": 0,
            "Yellow": 0,
            "Red": 0,
            "Unknown": 0
          },
          "timestamp": 0
        },
        "timestamp": 0
      }
    }
  ]
}
```

<h3 id="get-a-list-of-source-status-objects-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source Status objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-a-list-of-source-status-objects-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[InputStatus](#schemainputstatus)]|false|none|none|
|»» id|string|true|none|none|
|»» status|object|true|none|none|
|»»» error|object|false|none|none|
|»»»» details|object|false|none|none|
|»»»» message|string|true|none|none|
|»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» Green|number|true|none|none|
|»»»» Yellow|number|true|none|none|
|»»»» Red|number|true|none|none|
|»»»» Unknown|number|true|none|none|
|»»» metrics|object|true|none|none|
|»»» pq|object|false|none|none|
|»»»» error|object|false|none|none|
|»»»»» details|object|false|none|none|
|»»»»» message|string|true|none|none|
|»»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» timestamp|number|true|none|none|
|»»» timestamp|number|true|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|

<aside class="success">
This operation does not require authentication
</aside>

## Get Source Status by ID

<a id="opIdgetInputStatusById"></a>

> Code samples

`GET /system/status/inputs/{id}`

Get Source Status by ID

<h3 id="get-source-status-by-id-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|Unique ID to GET|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "status": {
        "error": {
          "details": {},
          "message": "string"
        },
        "health": "Green",
        "healthCounts": {
          "Green": 0,
          "Yellow": 0,
          "Red": 0,
          "Unknown": 0
        },
        "metrics": {},
        "pq": {
          "error": {
            "details": {},
            "message": "string"
          },
          "health": "Green",
          "healthCounts": {
            "Green": 0,
            "Yellow": 0,
            "Red": 0,
            "Unknown": 0
          },
          "timestamp": 0
        },
        "timestamp": 0
      }
    }
  ]
}
```

<h3 id="get-source-status-by-id-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of Source Status objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="get-source-status-by-id-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[InputStatus](#schemainputstatus)]|false|none|none|
|»» id|string|true|none|none|
|»» status|object|true|none|none|
|»»» error|object|false|none|none|
|»»»» details|object|false|none|none|
|»»»» message|string|true|none|none|
|»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» Green|number|true|none|none|
|»»»» Yellow|number|true|none|none|
|»»»» Red|number|true|none|none|
|»»»» Unknown|number|true|none|none|
|»»» metrics|object|true|none|none|
|»»» pq|object|false|none|none|
|»»»» error|object|false|none|none|
|»»»»» details|object|false|none|none|
|»»»»» message|string|true|none|none|
|»»»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»»»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»»»» timestamp|number|true|none|none|
|»»» timestamp|number|true|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|
|health|Green|
|health|Yellow|
|health|Red|
|health|Unknown|

<aside class="success">
This operation does not require authentication
</aside>

## Add an HEC token and optional metadata to a Splunk HEC Source

<a id="opIdcreateInputHecTokenById"></a>

> Code samples

`POST /system/inputs/{id}/hectoken`

Add an HEC token and optional metadata to the specified Splunk HEC Source.

> Body parameter

```json
{
  "description": "string",
  "enabled": true,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "token": "string"
}
```

<h3 id="add-an-hec-token-and-optional-metadata-to-a-splunk-hec-source-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Splunk HEC Source.|
|body|body|[AddHecTokenRequest](#schemaaddhectokenrequest)|true|AddHecTokenRequest object|
|» description|body|string|false|none|
|» enabled|body|boolean|false|none|
|» metadata|body|[object]|false|none|
|»» name|body|string|true|none|
|»» value|body|string|true|none|
|» token|body|string|true|none|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "splunk_hec",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "host": "0.0.0.0",
      "port": 65535,
      "authTokens": [
        {
          "authType": "manual",
          "tokenSecret": null,
          "token": null,
          "enabled": true,
          "description": "string",
          "allowedIndexesAtToken": [
            "string"
          ],
          "metadata": [
            {
              "name": "string",
              "value": "string"
            }
          ]
        }
      ],
      "tls": {
        "disabled": true,
        "requestCert": false,
        "rejectUnauthorized": true,
        "commonNameRegex": "/.*/",
        "certificateName": "string",
        "privKeyPath": "string",
        "passphrase": "string",
        "certPath": "string",
        "caPath": "string",
        "minVersion": "TLSv1",
        "maxVersion": "TLSv1"
      },
      "maxActiveReq": 256,
      "maxRequestsPerSocket": 0,
      "enableProxyHeader": false,
      "captureHeaders": false,
      "activityLogSampleRate": 100,
      "requestTimeout": 0,
      "socketTimeout": 0,
      "keepAliveTimeout": 5,
      "enableHealthCheck": null,
      "ipAllowlistRegex": "/.*/",
      "ipDenylistRegex": "/^$/",
      "splunkHecAPI": "/services/collector",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "allowedIndexes": [
        "string"
      ],
      "splunkHecAcks": false,
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "useFwdTimezone": true,
      "dropControlFields": true,
      "extractMetrics": false,
      "accessControlAllowOrigin": [
        "string"
      ],
      "accessControlAllowHeaders": [
        "string"
      ],
      "emitTokenMetrics": false,
      "description": "string"
    }
  ]
}
```

<h3 id="add-an-hec-token-and-optional-metadata-to-a-splunk-hec-source-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of InputSplunkHec objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="add-an-hec-token-and-optional-metadata-to-a-splunk-hec-source-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[InputSplunkHec](#schemainputsplunkhec)]|false|none|none|
|»» id|string|false|none|Unique ID for this input|
|»» type|string|true|none|none|
|»» disabled|boolean|false|none|none|
|»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»» pipeline|string|false|none|none|
|»»» output|string|true|none|none|
|»» pq|object|false|none|none|
|»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»» pqControls|object|false|none|none|
|»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»» port|number|true|none|Port to listen on|
|»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» tokenSecret|any|false|none|none|
|»»» token|any|true|none|none|
|»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|Optional token description|
|»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»» tls|object|false|none|none|
|»»» disabled|boolean|false|none|none|
|»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»» certificateName|string|false|none|The name of the predefined certificate|
|»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»» minVersion|string|false|none|none|
|»»» maxVersion|string|false|none|none|
|»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»» enableHealthCheck|any|false|none|none|
|»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»» name|string|true|none|none|
|»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

## Update metadata for an HEC token for a Splunk HEC Source

<a id="opIdupdateInputHecTokenByIdAndToken"></a>

> Code samples

`PATCH /system/inputs/{id}/hectoken/{token}`

Update the metadata for the specified HEC token for the specified Splunk HEC Source.

> Body parameter

```json
{
  "description": "string",
  "enabled": true,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}
```

<h3 id="update-metadata-for-an-hec-token-for-a-splunk-hec-source-parameters">Parameters</h3>

|Name|In|Type|Required|Description|
|---|---|---|---|---|
|id|path|string|true|The <code>id</code> of the Splunk HEC Source.|
|token|path|string|true|The <code>id</code> of the HEC token to update.|
|body|body|[UpdateHecTokenRequest](#schemaupdatehectokenrequest)|true|UpdateHecTokenRequest object|
|» description|body|string|false|none|
|» enabled|body|boolean|false|none|
|» metadata|body|[object]|false|none|
|»» name|body|string|true|none|
|»» value|body|string|true|none|

> Example responses

> 200 Response

```json
{
  "count": 0,
  "items": [
    {
      "id": "string",
      "type": "splunk_hec",
      "disabled": false,
      "pipeline": "string",
      "sendToRoutes": true,
      "environment": "string",
      "pqEnabled": false,
      "streamtags": [],
      "connections": [
        {
          "pipeline": "string",
          "output": "string"
        }
      ],
      "pq": {
        "mode": "smart",
        "maxBufferSize": 1000,
        "commitFrequency": 42,
        "maxFileSize": "1 MB",
        "maxSize": "5GB",
        "path": "$CRIBL_HOME/state/queues",
        "compress": "none",
        "pqControls": {}
      },
      "host": "0.0.0.0",
      "port": 65535,
      "authTokens": [
        {
          "authType": "manual",
          "tokenSecret": null,
          "token": null,
          "enabled": true,
          "description": "string",
          "allowedIndexesAtToken": [
            "string"
          ],
          "metadata": [
            {
              "name": "string",
              "value": "string"
            }
          ]
        }
      ],
      "tls": {
        "disabled": true,
        "requestCert": false,
        "rejectUnauthorized": true,
        "commonNameRegex": "/.*/",
        "certificateName": "string",
        "privKeyPath": "string",
        "passphrase": "string",
        "certPath": "string",
        "caPath": "string",
        "minVersion": "TLSv1",
        "maxVersion": "TLSv1"
      },
      "maxActiveReq": 256,
      "maxRequestsPerSocket": 0,
      "enableProxyHeader": false,
      "captureHeaders": false,
      "activityLogSampleRate": 100,
      "requestTimeout": 0,
      "socketTimeout": 0,
      "keepAliveTimeout": 5,
      "enableHealthCheck": null,
      "ipAllowlistRegex": "/.*/",
      "ipDenylistRegex": "/^$/",
      "splunkHecAPI": "/services/collector",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "allowedIndexes": [
        "string"
      ],
      "splunkHecAcks": false,
      "breakerRulesets": [
        "string"
      ],
      "staleChannelFlushMs": 10000,
      "useFwdTimezone": true,
      "dropControlFields": true,
      "extractMetrics": false,
      "accessControlAllowOrigin": [
        "string"
      ],
      "accessControlAllowHeaders": [
        "string"
      ],
      "emitTokenMetrics": false,
      "description": "string"
    }
  ]
}
```

<h3 id="update-metadata-for-an-hec-token-for-a-splunk-hec-source-responses">Responses</h3>

|Status|Meaning|Description|Schema|
|---|---|---|---|
|200|[OK](https://tools.ietf.org/html/rfc7231#section-6.3.1)|a list of InputSplunkHec objects|Inline|
|401|[Unauthorized](https://tools.ietf.org/html/rfc7235#section-3.1)|Unauthorized|None|
|500|[Internal Server Error](https://tools.ietf.org/html/rfc7231#section-6.6.1)|Unexpected error|[Error](#schemaerror)|

<h3 id="update-metadata-for-an-hec-token-for-a-splunk-hec-source-responseschema">Response Schema</h3>

Status Code **200**

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|» count|integer|false|none|number of items present in the items array|
|» items|[[InputSplunkHec](#schemainputsplunkhec)]|false|none|none|
|»» id|string|false|none|Unique ID for this input|
|»» type|string|true|none|none|
|»» disabled|boolean|false|none|none|
|»» pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|»» sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|»» environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|»» pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|»» streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|»» connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|»»» pipeline|string|false|none|none|
|»»» output|string|true|none|none|
|»» pq|object|false|none|none|
|»»» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|»»» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|»»» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|»»» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|»»» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|»»» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|»»» compress|string|false|none|Codec to use to compress the persisted data|
|»»» pqControls|object|false|none|none|
|»» host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|»» port|number|true|none|Port to listen on|
|»» authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|»»» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|»»» tokenSecret|any|false|none|none|
|»»» token|any|true|none|none|
|»»» enabled|boolean|false|none|none|
|»»» description|string|false|none|Optional token description|
|»»» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|»»» metadata|[object]|false|none|Fields to add to events referencing this token|
|»»»» name|string|true|none|none|
|»»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»» tls|object|false|none|none|
|»»» disabled|boolean|false|none|none|
|»»» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|»»» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|»»» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|»»» certificateName|string|false|none|The name of the predefined certificate|
|»»» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|»»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»»» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|»»» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|»»» minVersion|string|false|none|none|
|»»» maxVersion|string|false|none|none|
|»» maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|»» maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|»» enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|»» captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|»» activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|»» requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|»» socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|»» keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|»» enableHealthCheck|any|false|none|none|
|»» ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|»» ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|»» splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|»» metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|»»» name|string|true|none|none|
|»»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|»» allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|»» splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|»» breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|»» staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|»» useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|»» dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|»» extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|»» accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|»» accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|»» emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|»» description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<aside class="success">
This operation does not require authentication
</aside>

# Schemas

<h2 id="tocS_Input">Input</h2>
<!-- backwards compatibility -->
<a id="schemainput"></a>
<a id="schema_Input"></a>
<a id="tocSinput"></a>
<a id="tocsinput"></a>

```json
{
  "id": "string",
  "type": "collection",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "throttleRatePerSec": "0",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "output": "string"
}

```

### Properties

oneOf

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCollection](#schemainputcollection)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputKafka](#schemainputkafka)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputMsk](#schemainputmsk)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputHttp](#schemainputhttp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSplunk](#schemainputsplunk)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSplunkSearch](#schemainputsplunksearch)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSplunkHec](#schemainputsplunkhec)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputAzureBlob](#schemainputazureblob)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputElastic](#schemainputelastic)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputConfluentCloud](#schemainputconfluentcloud)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputGrafana](#schemainputgrafana)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputLoki](#schemainputloki)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputPrometheusRw](#schemainputprometheusrw)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputPrometheus](#schemainputprometheus)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputEdgePrometheus](#schemainputedgeprometheus)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputOffice365Mgmt](#schemainputoffice365mgmt)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputOffice365Service](#schemainputoffice365service)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputOffice365MsgTrace](#schemainputoffice365msgtrace)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputEventhub](#schemainputeventhub)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputExec](#schemainputexec)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputFirehose](#schemainputfirehose)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputGooglePubsub](#schemainputgooglepubsub)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCribl](#schemainputcribl)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCriblTcp](#schemainputcribltcp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCriblHttp](#schemainputcriblhttp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCriblLakeHttp](#schemainputcribllakehttp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputTcpjson](#schemainputtcpjson)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSystemMetrics](#schemainputsystemmetrics)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSystemState](#schemainputsystemstate)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputKubeMetrics](#schemainputkubemetrics)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputKubeLogs](#schemainputkubelogs)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputKubeEvents](#schemainputkubeevents)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputWindowsMetrics](#schemainputwindowsmetrics)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCrowdstrike](#schemainputcrowdstrike)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputDatadogAgent](#schemainputdatadogagent)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputDatagen](#schemainputdatagen)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputHttpRaw](#schemainputhttpraw)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputKinesis](#schemainputkinesis)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCriblmetrics](#schemainputcriblmetrics)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputMetrics](#schemainputmetrics)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputS3](#schemainputs3)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputS3Inventory](#schemainputs3inventory)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSnmp](#schemainputsnmp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputOpenTelemetry](#schemainputopentelemetry)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputModelDrivenTelemetry](#schemainputmodeldriventelemetry)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSqs](#schemainputsqs)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSyslog](#schemainputsyslog)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputFile](#schemainputfile)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputTcp](#schemainputtcp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputAppscope](#schemainputappscope)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputWef](#schemainputwef)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputWinEventLogs](#schemainputwineventlogs)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputRawUdp](#schemainputrawudp)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputJournalFiles](#schemainputjournalfiles)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputWiz](#schemainputwiz)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputWizWebhook](#schemainputwizwebhook)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputNetflow](#schemainputnetflow)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputSecurityLake](#schemainputsecuritylake)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputZscalerHec](#schemainputzscalerhec)|false|none|none|

xor

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|[InputCloudflareHec](#schemainputcloudflarehec)|false|none|none|

<h2 id="tocS_Error">Error</h2>
<!-- backwards compatibility -->
<a id="schemaerror"></a>
<a id="schema_Error"></a>
<a id="tocSerror"></a>
<a id="tocserror"></a>

```json
{
  "message": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|message|string|false|none|Error message|

<h2 id="tocS_InputStatus">InputStatus</h2>
<!-- backwards compatibility -->
<a id="schemainputstatus"></a>
<a id="schema_InputStatus"></a>
<a id="tocSinputstatus"></a>
<a id="tocsinputstatus"></a>

```json
{
  "id": "string",
  "status": {
    "error": {
      "details": {},
      "message": "string"
    },
    "health": "Green",
    "healthCounts": {
      "Green": 0,
      "Yellow": 0,
      "Red": 0,
      "Unknown": 0
    },
    "metrics": {},
    "pq": {
      "error": {
        "details": {},
        "message": "string"
      },
      "health": "Green",
      "healthCounts": {
        "Green": 0,
        "Yellow": 0,
        "Red": 0,
        "Unknown": 0
      },
      "timestamp": 0
    },
    "timestamp": 0
  }
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|true|none|none|
|status|object|true|none|none|
|» error|object|false|none|none|
|»» details|object|false|none|none|
|»» message|string|true|none|none|
|» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|» metrics|object|true|none|none|
|» pq|object|false|none|none|
|»» error|object|false|none|none|
|»»» details|object|false|none|none|
|»»» message|string|true|none|none|
|»» health|[HealthStringType](#schemahealthstringtype)|true|none|none|
|»» healthCounts|[HealthCountType](#schemahealthcounttype)|true|none|none|
|»» timestamp|number|true|none|none|
|» timestamp|number|true|none|none|

<h2 id="tocS_InputSplunkHec">InputSplunkHec</h2>
<!-- backwards compatibility -->
<a id="schemainputsplunkhec"></a>
<a id="schema_InputSplunkHec"></a>
<a id="tocSinputsplunkhec"></a>
<a id="tocsinputsplunkhec"></a>

```json
{
  "id": "string",
  "type": "splunk_hec",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    {
      "authType": "manual",
      "tokenSecret": null,
      "token": null,
      "enabled": true,
      "description": "string",
      "allowedIndexesAtToken": [
        "string"
      ],
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": null,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "splunkHecAPI": "/services/collector",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "allowedIndexes": [
    "string"
  ],
  "splunkHecAcks": false,
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "useFwdTimezone": true,
  "dropControlFields": true,
  "extractMetrics": false,
  "accessControlAllowOrigin": [
    "string"
  ],
  "accessControlAllowHeaders": [
    "string"
  ],
  "emitTokenMetrics": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|» tokenSecret|any|false|none|none|
|» token|any|true|none|none|
|» enabled|boolean|false|none|none|
|» description|string|false|none|Optional token description|
|» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|any|false|none|none|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|splunkHecAPI|string|true|none|Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.|
|metadata|[object]|false|none|Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|splunkHecAcks|boolean|false|none|Enable Splunk HEC acknowledgements|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_AddHecTokenRequest">AddHecTokenRequest</h2>
<!-- backwards compatibility -->
<a id="schemaaddhectokenrequest"></a>
<a id="schema_AddHecTokenRequest"></a>
<a id="tocSaddhectokenrequest"></a>
<a id="tocsaddhectokenrequest"></a>

```json
{
  "description": "string",
  "enabled": true,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "token": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|description|string|false|none|none|
|enabled|boolean|false|none|none|
|metadata|[object]|false|none|none|
|» name|string|true|none|none|
|» value|string|true|none|none|
|token|string|true|none|none|

<h2 id="tocS_UpdateHecTokenRequest">UpdateHecTokenRequest</h2>
<!-- backwards compatibility -->
<a id="schemaupdatehectokenrequest"></a>
<a id="schema_UpdateHecTokenRequest"></a>
<a id="tocSupdatehectokenrequest"></a>
<a id="tocsupdatehectokenrequest"></a>

```json
{
  "description": "string",
  "enabled": true,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|description|string|false|none|none|
|enabled|boolean|false|none|none|
|metadata|[object]|false|none|none|
|» name|string|true|none|none|
|» value|string|true|none|none|

<h2 id="tocS_InputCollection">InputCollection</h2>
<!-- backwards compatibility -->
<a id="schemainputcollection"></a>
<a id="schema_InputCollection"></a>
<a id="tocSinputcollection"></a>
<a id="tocsinputcollection"></a>

```json
{
  "id": "string",
  "type": "collection",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "throttleRatePerSec": "0",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "output": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process results|
|sendToRoutes|boolean|false|none|Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|preprocess|object|false|none|none|
|» disabled|boolean|true|none|none|
|» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|» args|[string]|false|none|Arguments to be added to the custom command|
|throttleRatePerSec|string|false|none|Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|output|string|false|none|Destination to send results to|

#### Enumerated Values

|Property|Value|
|---|---|
|type|collection|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputKafka">InputKafka</h2>
<!-- backwards compatibility -->
<a id="schemainputkafka"></a>
<a id="schema_InputKafka"></a>
<a id="tocSinputkafka"></a>
<a id="tocsinputkafka"></a>

```json
{
  "id": "string",
  "type": "kafka",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "brokers": [
    "string"
  ],
  "topics": [],
  "groupId": "Cribl",
  "fromBeginning": true,
  "kafkaSchemaRegistry": {
    "disabled": true,
    "schemaRegistryURL": "http://localhost:8081",
    "connectionTimeout": 30000,
    "requestTimeout": 30000,
    "maxRetries": 1,
    "auth": {
      "disabled": true,
      "credentialsSecret": "string"
    },
    "tls": {
      "disabled": true,
      "rejectUnauthorized": true,
      "servername": "string",
      "certificateName": "string",
      "caPath": "string",
      "privKeyPath": "string",
      "certPath": "string",
      "passphrase": "string",
      "minVersion": "TLSv1",
      "maxVersion": "TLSv1"
    }
  },
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": true,
    "username": "string",
    "password": "string",
    "authType": "manual",
    "credentialsSecret": "string",
    "mechanism": "plain",
    "keytabLocation": "string",
    "principal": "string",
    "brokerServiceClass": "string",
    "oauthEnabled": false,
    "tokenUrl": "string",
    "clientId": "string",
    "oauthSecretType": "secret",
    "clientTextSecret": "string",
    "oauthParams": [
      {
        "name": "string",
        "value": "string"
      }
    ],
    "saslExtensions": [
      {
        "name": "string",
        "value": "string"
      }
    ]
  },
  "tls": {
    "disabled": true,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "sessionTimeout": 30000,
  "rebalanceTimeout": 60000,
  "heartbeatInterval": 3000,
  "autoCommitInterval": 1000,
  "autoCommitThreshold": 1,
  "maxBytesPerPartition": 1048576,
  "maxBytes": 10485760,
  "maxSocketErrors": 0,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|kafkaSchemaRegistry|object|false|none|none|
|» disabled|boolean|true|none|none|
|» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»» disabled|boolean|true|none|none|
|»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» tls|object|false|none|none|
|»» disabled|boolean|false|none|none|
|»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»» certificateName|string|false|none|The name of the predefined certificate|
|»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»» minVersion|string|false|none|none|
|»» maxVersion|string|false|none|none|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» mechanism|string|false|none|none|
|» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|» clientId|string|false|none|Client ID to use for OAuth authentication|
|» oauthSecretType|string|false|none|none|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kafka|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputMsk">InputMsk</h2>
<!-- backwards compatibility -->
<a id="schemainputmsk"></a>
<a id="schema_InputMsk"></a>
<a id="tocSinputmsk"></a>
<a id="tocsinputmsk"></a>

```json
{
  "id": "string",
  "type": "msk",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "brokers": [
    "string"
  ],
  "topics": [],
  "groupId": "Cribl",
  "fromBeginning": true,
  "sessionTimeout": 30000,
  "rebalanceTimeout": 60000,
  "heartbeatInterval": 3000,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "kafkaSchemaRegistry": {
    "disabled": true,
    "schemaRegistryURL": "http://localhost:8081",
    "connectionTimeout": 30000,
    "requestTimeout": 30000,
    "maxRetries": 1,
    "auth": {
      "disabled": true,
      "credentialsSecret": "string"
    },
    "tls": {
      "disabled": true,
      "rejectUnauthorized": true,
      "servername": "string",
      "certificateName": "string",
      "caPath": "string",
      "privKeyPath": "string",
      "certPath": "string",
      "passphrase": "string",
      "minVersion": "TLSv1",
      "maxVersion": "TLSv1"
    }
  },
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "autoCommitInterval": 1000,
  "autoCommitThreshold": 1,
  "maxBytesPerPartition": 1048576,
  "maxBytes": 10485760,
  "maxSocketErrors": 0,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|brokers|[string]|true|none|Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).|
|topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|kafkaSchemaRegistry|object|false|none|none|
|» disabled|boolean|true|none|none|
|» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»» disabled|boolean|true|none|none|
|»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» tls|object|false|none|none|
|»» disabled|boolean|false|none|none|
|»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»» certificateName|string|false|none|The name of the predefined certificate|
|»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»» minVersion|string|false|none|none|
|»» maxVersion|string|false|none|none|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|awsAuthenticationMethod|string|true|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|true|none|Region where the MSK cluster is located|
|endpoint|string|false|none|MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing MSK cluster requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access MSK|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

#### Enumerated Values

|Property|Value|
|---|---|
|type|msk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputHttp">InputHttp</h2>
<!-- backwards compatibility -->
<a id="schemainputhttp"></a>
<a id="schema_InputHttp"></a>
<a id="tocSinputhttp"></a>
<a id="tocsinputhttp"></a>

```json
{
  "id": "string",
  "type": "http",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    "string"
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "criblAPI": "/cribl",
  "elasticAPI": "/elastic",
  "splunkHecAPI": "/services/collector",
  "splunkHecAcks": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "authTokensExt": [
    {
      "token": "string",
      "description": "string",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|splunkHecAcks|boolean|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|» description|string|false|none|none|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputSplunk">InputSplunk</h2>
<!-- backwards compatibility -->
<a id="schemainputsplunk"></a>
<a id="schema_InputSplunk"></a>
<a id="tocSinputsplunk"></a>
<a id="tocsinputsplunk"></a>

```json
{
  "id": "string",
  "type": "splunk",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "ipWhitelistRegex": "/.*/",
  "maxActiveCxn": 1000,
  "socketIdleTimeout": 0,
  "socketEndingMaxWait": 30,
  "socketMaxLifespan": 0,
  "enableProxyHeader": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "authTokens": [
    {
      "token": "string",
      "description": "string"
    }
  ],
  "maxS2Sversion": "v3",
  "description": "string",
  "useFwdTimezone": true,
  "dropControlFields": true,
  "extractMetrics": false,
  "compress": "disabled"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|authTokens|[object]|false|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|» token|string|true|none|Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.|
|» description|string|false|none|none|
|maxS2Sversion|string|false|none|The highest S2S protocol version to advertise during handshake|
|description|string|false|none|none|
|useFwdTimezone|boolean|false|none|Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event|
|dropControlFields|boolean|false|none|Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.|
|extractMetrics|boolean|false|none|Extract and process Splunk-generated metrics as Cribl metrics|
|compress|string|false|none|Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|maxS2Sversion|v3|
|maxS2Sversion|v4|
|compress|disabled|
|compress|auto|
|compress|always|

<h2 id="tocS_InputSplunkSearch">InputSplunkSearch</h2>
<!-- backwards compatibility -->
<a id="schemainputsplunksearch"></a>
<a id="schema_InputSplunkSearch"></a>
<a id="tocSinputsplunksearch"></a>
<a id="tocsinputsplunksearch"></a>

```json
{
  "id": "string",
  "type": "splunk_search",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "searchHead": "https://localhost:8089",
  "search": "string",
  "earliest": "-16m@m",
  "latest": "-1m@m",
  "cronSchedule": "*/15 * * * *",
  "endpoint": "/services/search/v2/jobs/export",
  "outputMode": "csv",
  "endpointParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "endpointHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "logLevel": "error",
  "requestTimeout": 0,
  "useRoundRobinDns": false,
  "rejectUnauthorized": false,
  "encoding": "string",
  "keepAliveTime": 30,
  "jobTimeout": "0",
  "maxMissedKeepAlives": 3,
  "ttl": "4h",
  "ignoreGroupJobsLimit": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "retryRules": {
    "type": "none",
    "interval": 1000,
    "limit": 5,
    "multiplier": 2,
    "codes": [
      429,
      503
    ],
    "enableHeader": true,
    "retryConnectTimeout": false,
    "retryConnectReset": false
  },
  "breakerRulesets": [
    "Splunk Search Ruleset"
  ],
  "staleChannelFlushMs": 10000,
  "authType": "none",
  "description": "string",
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|searchHead|string|true|none|Search head base URL. Can be an expression. Default is https://localhost:8089.|
|search|string|true|none|Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'|
|earliest|string|false|none|The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'|
|latest|string|false|none|The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'|
|cronSchedule|string|true|none|A cron schedule on which to run this job|
|endpoint|string|true|none|REST API used to create a search|
|outputMode|string|true|none|Format of the returned output|
|endpointParams|[object]|false|none|Optional request parameters to send to the endpoint|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|endpointHeaders|[object]|false|none|Optional request headers to send to the endpoint|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.|
|logLevel|string|false|none|Collector runtime log level (verbosity)|
|requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 for no timeout.|
|useRoundRobinDns|boolean|false|none|When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|retryRules|object|false|none|none|
|» type|string|true|none|The algorithm to use when performing HTTP retries|
|» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|authType|string|false|none|Splunk Search authentication type|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|

#### Enumerated Values

|Property|Value|
|---|---|
|type|splunk_search|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|outputMode|csv|
|outputMode|json|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|

<h2 id="tocS_InputAzureBlob">InputAzureBlob</h2>
<!-- backwards compatibility -->
<a id="schemainputazureblob"></a>
<a id="schema_InputAzureBlob"></a>
<a id="tocSinputazureblob"></a>
<a id="tocsinputazureblob"></a>

```json
{
  "id": "string",
  "type": "azure_blob",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "queueName": "string",
  "fileFilter": "/.*/",
  "visibilityTimeout": 600,
  "numReceivers": 1,
  "maxMessages": 1,
  "servicePeriodSecs": 5,
  "skipOnError": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "parquetChunkSizeMB": 5,
  "parquetChunkDownloadTimeout": 600,
  "authType": "manual",
  "description": "string",
  "connectionString": "string",
  "textSecret": "string",
  "storageAccountName": "string",
  "tenantId": "string",
  "clientId": "string",
  "azureCloud": "string",
  "endpointSuffix": "string",
  "clientTextSecret": "string",
  "certificate": {
    "certificateName": "string"
  }
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|queueName|string|true|none|The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`|
|fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|visibilityTimeout|number|false|none|The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.|
|numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|maxMessages|number|false|none|The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.|
|servicePeriodSecs|number|false|none|The duration (in seconds) which pollers should be validated and restarted if exited|
|skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|authType|string|false|none|none|
|description|string|false|none|none|
|connectionString|string|false|none|Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.|
|textSecret|string|false|none|Select or create a stored text secret|
|storageAccountName|string|false|none|The name of your Azure storage account|
|tenantId|string|false|none|The service principal's tenant ID|
|clientId|string|false|none|The service principal's client ID|
|azureCloud|string|false|none|The Azure cloud to use. Defaults to Azure Public Cloud.|
|endpointSuffix|string|false|none|Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.|
|clientTextSecret|string|false|none|Select or create a stored text secret|
|certificate|object|false|none|none|
|» certificateName|string|true|none|The certificate you registered as credentials for your app in the Azure portal|

#### Enumerated Values

|Property|Value|
|---|---|
|type|azure_blob|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|clientSecret|
|authType|clientCert|

<h2 id="tocS_InputElastic">InputElastic</h2>
<!-- backwards compatibility -->
<a id="schemainputelastic"></a>
<a id="schema_InputElastic"></a>
<a id="tocSinputelastic"></a>
<a id="tocsinputelastic"></a>

```json
{
  "id": "string",
  "type": "elastic",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "elasticAPI": "/",
  "authType": "none",
  "apiVersion": "6.8.4",
  "extraHttpHeaders": [
    {
      "name": "X-elastic-product",
      "value": "Elasticsearch"
    }
  ],
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "proxyMode": {
    "enabled": false,
    "authType": "none",
    "username": "string",
    "password": "string",
    "credentialsSecret": "string",
    "url": "string",
    "rejectUnauthorized": false,
    "removeHeaders": [
      "string"
    ],
    "timeoutSec": 60
  },
  "description": "string",
  "username": "string",
  "password": "string",
  "credentialsSecret": "string",
  "authTokens": [
    "string"
  ],
  "customAPIVersion": "{\n    \"name\": \"AzU84iL\",\n    \"cluster_name\": \"cribl\",\n    \"cluster_uuid\": \"Js6_Z2VKS3KbfRSxPmPbaw\",\n    \"version\": {\n        \"number\": \"8.3.2\",\n        \"build_type\": \"tar\",\n        \"build_hash\": \"bca0c8d\",\n        \"build_date\": \"2019-10-16T06:19:49.319352Z\",\n        \"build_snapshot\": false,\n        \"lucene_version\": \"9.7.2\",\n        \"minimum_wire_compatibility_version\": \"7.17.0\",\n        \"minimum_index_compatibility_version\": \"7.0.0\"\n    },\n    \"tagline\": \"You Know, for Search\"\n}"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|elasticAPI|string|true|none|Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.|
|authType|string|false|none|none|
|apiVersion|string|false|none|The API version to use for communicating with the server|
|extraHttpHeaders|[object]|false|none|Headers to add to all events|
|» name|string|false|none|none|
|» value|string|true|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|proxyMode|object|false|none|none|
|» enabled|boolean|true|none|Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» url|string|false|none|URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200|
|» rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)|
|» removeHeaders|[string]|false|none|List of headers to remove from the request to proxy|
|» timeoutSec|number|false|none|Amount of time, in seconds, to wait for a proxy request to complete before canceling it|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|authTokens|[string]|false|none|Bearer tokens to include in the authorization header|
|customAPIVersion|string|false|none|Custom version information to respond to requests|

#### Enumerated Values

|Property|Value|
|---|---|
|type|elastic|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|authTokens|
|apiVersion|6.8.4|
|apiVersion|8.3.2|
|apiVersion|custom|
|authType|none|
|authType|manual|
|authType|secret|

<h2 id="tocS_InputConfluentCloud">InputConfluentCloud</h2>
<!-- backwards compatibility -->
<a id="schemainputconfluentcloud"></a>
<a id="schema_InputConfluentCloud"></a>
<a id="tocSinputconfluentcloud"></a>
<a id="tocsinputconfluentcloud"></a>

```json
{
  "id": "string",
  "type": "confluent_cloud",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "brokers": [
    "string"
  ],
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true,
    "servername": "string",
    "certificateName": "string",
    "caPath": "string",
    "privKeyPath": "string",
    "certPath": "string",
    "passphrase": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "topics": [],
  "groupId": "Cribl",
  "fromBeginning": true,
  "kafkaSchemaRegistry": {
    "disabled": true,
    "schemaRegistryURL": "http://localhost:8081",
    "connectionTimeout": 30000,
    "requestTimeout": 30000,
    "maxRetries": 1,
    "auth": {
      "disabled": true,
      "credentialsSecret": "string"
    },
    "tls": {
      "disabled": true,
      "rejectUnauthorized": true,
      "servername": "string",
      "certificateName": "string",
      "caPath": "string",
      "privKeyPath": "string",
      "certPath": "string",
      "passphrase": "string",
      "minVersion": "TLSv1",
      "maxVersion": "TLSv1"
    }
  },
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": true,
    "username": "string",
    "password": "string",
    "authType": "manual",
    "credentialsSecret": "string",
    "mechanism": "plain",
    "keytabLocation": "string",
    "principal": "string",
    "brokerServiceClass": "string",
    "oauthEnabled": false,
    "tokenUrl": "string",
    "clientId": "string",
    "oauthSecretType": "secret",
    "clientTextSecret": "string",
    "oauthParams": [
      {
        "name": "string",
        "value": "string"
      }
    ],
    "saslExtensions": [
      {
        "name": "string",
        "value": "string"
      }
    ]
  },
  "sessionTimeout": 30000,
  "rebalanceTimeout": 60000,
  "heartbeatInterval": 3000,
  "autoCommitInterval": 1000,
  "autoCommitThreshold": 1,
  "maxBytesPerPartition": 1048576,
  "maxBytes": 10485760,
  "maxSocketErrors": 0,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|brokers|[string]|true|none|List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|» certificateName|string|false|none|The name of the predefined certificate|
|» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|topics|[string]|true|none|Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.|
|groupId|string|false|none|The consumer group to which this instance belongs. Defaults to 'Cribl'.|
|fromBeginning|boolean|false|none|Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message|
|kafkaSchemaRegistry|object|false|none|none|
|» disabled|boolean|true|none|none|
|» schemaRegistryURL|string|false|none|URL for accessing the Confluent Schema Registry. Example: http://localhost:8081. To connect over TLS, use https instead of http.|
|» connectionTimeout|number|false|none|Maximum time to wait for a Schema Registry connection to complete successfully|
|» requestTimeout|number|false|none|Maximum time to wait for the Schema Registry to respond to a request|
|» maxRetries|number|false|none|Maximum number of times to try fetching schemas from the Schema Registry|
|» auth|object|false|none|Credentials to use when authenticating with the schema registry using basic HTTP authentication|
|»» disabled|boolean|true|none|none|
|»» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» tls|object|false|none|none|
|»» disabled|boolean|false|none|none|
|»» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another <br>                    trusted CA (such as the system's). Defaults to Enabled. Overrides the toggle from Advanced Settings, when also present.|
|»» servername|string|false|none|Server name for the SNI (Server Name Indication) TLS extension. It must be a host name, and not an IP address.|
|»» certificateName|string|false|none|The name of the predefined certificate|
|»» caPath|string|false|none|Path on client in which to find CA certificates to verify the server's cert. PEM format. Can reference $ENV_VARS.|
|»» privKeyPath|string|false|none|Path on client in which to find the private key to use. PEM format. Can reference $ENV_VARS.|
|»» certPath|string|false|none|Path on client in which to find certificates to use. PEM format. Can reference $ENV_VARS.|
|»» passphrase|string|false|none|Passphrase to use to decrypt private key|
|»» minVersion|string|false|none|none|
|»» maxVersion|string|false|none|none|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» authType|string|false|none|Enter credentials directly, or select a stored secret|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» mechanism|string|false|none|none|
|» keytabLocation|string|false|none|Location of keytab file for authentication principal|
|» principal|string|false|none|Authentication principal, such as `kafka_user@example.com`|
|» brokerServiceClass|string|false|none|Kerberos service class for Kafka brokers, such as `kafka`|
|» oauthEnabled|boolean|false|none|Enable OAuth authentication|
|» tokenUrl|string|false|none|URL of the token endpoint to use for OAuth authentication|
|» clientId|string|false|none|Client ID to use for OAuth authentication|
|» oauthSecretType|string|false|none|none|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» oauthParams|[object]|false|none|Additional fields to send to the token endpoint, such as scope or audience|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|» saslExtensions|[object]|false|none|Additional SASL extension fields, such as Confluent's logicalCluster or identityPoolId|
|»» name|string|true|none|none|
|»» value|string|true|none|none|
|sessionTimeout|number|false|none|Timeout used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, <br>      the broker will remove the client from the group and initiate a rebalance.<br>      Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.|
|rebalanceTimeout|number|false|none|Maximum allowed time for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.|
|heartbeatInterval|number|false|none|Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.|
|autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|confluent_cloud|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|scram-sha-256|
|mechanism|scram-sha-512|
|mechanism|kerberos|

<h2 id="tocS_InputGrafana">InputGrafana</h2>
<!-- backwards compatibility -->
<a id="schemainputgrafana"></a>
<a id="schema_InputGrafana"></a>
<a id="tocSinputgrafana"></a>
<a id="tocsinputgrafana"></a>

```json
{
  "id": "string",
  "type": "grafana",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "prometheusAPI": "/api/prom/push",
  "lokiAPI": "/loki/api/v1/push",
  "prometheusAuth": {
    "authType": "none",
    "username": "string",
    "password": "string",
    "token": "string",
    "credentialsSecret": "string",
    "textSecret": "string",
    "loginUrl": "string",
    "secretParamName": "string",
    "secret": "string",
    "tokenAttributeName": "string",
    "authHeaderExpr": "`Bearer ${token}`",
    "tokenTimeoutSecs": 3600,
    "oauthParams": [
      {
        "name": "string",
        "value": "string"
      }
    ],
    "oauthHeaders": [
      {
        "name": "string",
        "value": "string"
      }
    ]
  },
  "lokiAuth": {
    "authType": "none",
    "username": "string",
    "password": "string",
    "token": "string",
    "credentialsSecret": "string",
    "textSecret": "string",
    "loginUrl": "string",
    "secretParamName": "string",
    "secret": "string",
    "tokenAttributeName": "string",
    "authHeaderExpr": "`Bearer ${token}`",
    "tokenTimeoutSecs": 3600,
    "oauthParams": [
      {
        "name": "string",
        "value": "string"
      }
    ],
    "oauthHeaders": [
      {
        "name": "string",
        "value": "string"
      }
    ]
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|prometheusAPI|string|false|none|Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<your‑upstream‑URL>:<your‑port>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.|
|lokiAPI|string|false|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.|
|prometheusAuth|object|false|none|none|
|» authType|string|false|none|Remote Write authentication type|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» token|string|false|none|Bearer token to include in the authorization header|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» textSecret|string|false|none|Select or create a stored text secret|
|» loginUrl|string|false|none|URL for OAuth|
|» secretParamName|string|false|none|Secret parameter name to pass in request body|
|» secret|string|false|none|Secret parameter value to pass in request body|
|» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»» name|string|true|none|OAuth parameter name|
|»» value|string|true|none|OAuth parameter value|
|» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»» name|string|true|none|OAuth header name|
|»» value|string|true|none|OAuth header value|
|lokiAuth|object|false|none|none|
|» authType|string|false|none|Loki logs authentication type|
|» username|string|false|none|none|
|» password|string|false|none|none|
|» token|string|false|none|Bearer token to include in the authorization header|
|» credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|» textSecret|string|false|none|Select or create a stored text secret|
|» loginUrl|string|false|none|URL for OAuth|
|» secretParamName|string|false|none|Secret parameter name to pass in request body|
|» secret|string|false|none|Secret parameter value to pass in request body|
|» tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|» authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|» tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|» oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»» name|string|true|none|OAuth parameter name|
|»» value|string|true|none|OAuth parameter value|
|» oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|»» name|string|true|none|OAuth header name|
|»» value|string|true|none|OAuth header value|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

anyOf

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|object|false|none|none|

or

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|grafana|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|

<h2 id="tocS_InputLoki">InputLoki</h2>
<!-- backwards compatibility -->
<a id="schemainputloki"></a>
<a id="schema_InputLoki"></a>
<a id="tocSinputloki"></a>
<a id="tocsinputloki"></a>

```json
{
  "id": "string",
  "type": "loki",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "lokiAPI": "/loki/api/v1/push",
  "authType": "none",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string",
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|lokiAPI|string|true|none|Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<your‑upstream‑URL>:<your‑port>/loki/api/v1/push'.|
|authType|string|false|none|Loki logs authentication type|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|

#### Enumerated Values

|Property|Value|
|---|---|
|type|loki|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|

<h2 id="tocS_InputPrometheusRw">InputPrometheusRw</h2>
<!-- backwards compatibility -->
<a id="schemainputprometheusrw"></a>
<a id="schema_InputPrometheusRw"></a>
<a id="tocSinputprometheusrw"></a>
<a id="tocsinputprometheusrw"></a>

```json
{
  "id": "string",
  "type": "prometheus_rw",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "prometheusAPI": "/write",
  "authType": "none",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string",
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ]
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|prometheusAPI|string|true|none|Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<your‑upstream‑URL>:<your‑port>/write.|
|authType|string|false|none|Remote Write authentication type|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|

#### Enumerated Values

|Property|Value|
|---|---|
|type|prometheus_rw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|

<h2 id="tocS_InputPrometheus">InputPrometheus</h2>
<!-- backwards compatibility -->
<a id="schemainputprometheus"></a>
<a id="schema_InputPrometheus"></a>
<a id="tocSinputprometheus"></a>
<a id="tocsinputprometheus"></a>

```json
{
  "id": "string",
  "type": "prometheus",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "dimensionList": [
    "host",
    "source"
  ],
  "discoveryType": "static",
  "interval": 15,
  "logLevel": "error",
  "rejectUnauthorized": true,
  "keepAliveTime": 30,
  "jobTimeout": "0",
  "maxMissedKeepAlives": 3,
  "ttl": "4h",
  "ignoreGroupJobsLimit": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "authType": "manual",
  "description": "string",
  "targetList": [],
  "recordType": "SRV",
  "scrapePort": 9090,
  "nameList": [],
  "scrapeProtocol": "http",
  "scrapePath": "/metrics",
  "awsAuthenticationMethod": "auto",
  "awsApiKey": "string",
  "awsSecret": "string",
  "usePublicIp": true,
  "searchFilter": [
    {
      "Name": "string",
      "Values": []
    }
  ],
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "username": "string",
  "password": "string",
  "credentialsSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|dimensionList|[string]|false|none|Other dimensions to include in events|
|» dimension|string|false|none|none|
|discoveryType|string|false|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|interval|number|true|none|How often in minutes to scrape targets for metrics, 60 must be evenly divisible by the value or save will fail.|
|logLevel|string|true|none|Collector runtime Log Level|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|authType|string|false|none|Enter credentials directly, or select a stored secret|
|description|string|false|none|none|
|targetList|[string]|false|none|List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.|
|» Targets|string|false|none|none|
|recordType|string|false|none|DNS Record type to resolve|
|scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|nameList|[string]|false|none|List of DNS names to resolve|
|» DNS Names|string|false|none|none|
|scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|searchFilter|[object]|false|none|EC2 Instance Search Filter|
|» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|awsSecretKey|string|false|none|none|
|region|string|false|none|Region where the EC2 is located|
|endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|username|string|false|none|Username for Prometheus Basic authentication|
|password|string|false|none|Password for Prometheus Basic authentication|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|

#### Enumerated Values

|Property|Value|
|---|---|
|type|prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|authType|manual|
|authType|secret|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|

<h2 id="tocS_InputEdgePrometheus">InputEdgePrometheus</h2>
<!-- backwards compatibility -->
<a id="schemainputedgeprometheus"></a>
<a id="schema_InputEdgePrometheus"></a>
<a id="tocSinputedgeprometheus"></a>
<a id="tocsinputedgeprometheus"></a>

```json
{
  "id": "string",
  "type": "edge_prometheus",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "dimensionList": [
    "host",
    "source"
  ],
  "discoveryType": "static",
  "interval": 15,
  "timeout": 5000,
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none"
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "authType": "manual",
  "description": "string",
  "targets": [],
  "recordType": "SRV",
  "scrapePort": 9090,
  "nameList": [],
  "scrapeProtocol": "http",
  "scrapePath": "/metrics",
  "awsAuthenticationMethod": "auto",
  "awsApiKey": "string",
  "awsSecret": "string",
  "usePublicIp": true,
  "searchFilter": [
    {
      "Name": "string",
      "Values": []
    }
  ],
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "scrapeProtocolExpr": "metadata.annotations['prometheus.io/scheme'] || 'http'",
  "scrapePortExpr": "metadata.annotations['prometheus.io/port'] || 9090",
  "scrapePathExpr": "metadata.annotations['prometheus.io/path'] || '/metrics'",
  "podFilter": [
    {
      "filter": "metadata.annotations['prometheus.io/scrape']",
      "description": "Scrape pod if annotation is true"
    }
  ],
  "username": "string",
  "password": "string",
  "credentialsSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|dimensionList|[string]|false|none|Other dimensions to include in events|
|» dimension|string|false|none|none|
|discoveryType|string|true|none|Target discovery mechanism. Use static to manually enter a list of targets.|
|interval|number|true|none|How often in seconds to scrape targets for metrics.|
|timeout|number|false|none|Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|» compress|string|false|none|Data compression format. Default is gzip.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|authType|string|false|none|Enter credentials directly, or select a stored secret|
|description|string|false|none|none|
|targets|[object]|false|none|none|
|» protocol|string|false|none|Protocol to use when collecting metrics|
|» host|string|true|none|Name of host from which to pull metrics.|
|» port|number|false|none|The port number in the metrics URL for discovered targets.|
|» path|string|false|none|Path to use when collecting metrics from discovered targets|
|recordType|string|false|none|DNS Record type to resolve|
|scrapePort|number|false|none|The port number in the metrics URL for discovered targets.|
|nameList|[string]|false|none|List of DNS names to resolve|
|» DNS Names|string|false|none|none|
|scrapeProtocol|string|false|none|Protocol to use when collecting metrics|
|scrapePath|string|false|none|Path to use when collecting metrics from discovered targets|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|usePublicIp|boolean|false|none|Use public IP address for discovered targets. Set to false if the private IP address should be used.|
|searchFilter|[object]|false|none|EC2 Instance Search Filter|
|» Name|string|true|none|Search filter attribute name, see: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInstances.html for more information. Attributes can be manually entered if not present in the drop down list|
|» Values|[string]|true|none|Search Filter Values, if empty only "running" EC2 instances will be returned|
|awsSecretKey|string|false|none|none|
|region|string|false|none|Region where the EC2 is located|
|endpoint|string|false|none|EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing EC2 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access EC2|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|scrapeProtocolExpr|string|false|none|Protocol to use when collecting metrics|
|scrapePortExpr|string|false|none|The port number in the metrics URL for discovered targets.|
|scrapePathExpr|string|false|none|Path to use when collecting metrics from discovered targets|
|podFilter|[object]|false|none|Add rules to decide which pods to discover for metrics.<br>  Pods are searched if no rules are given or of all the rules'<br>  expressions evaluate to true.|
|» filter|string|true|none|JavaScript expression applied to pods objects. Return 'true' to include it.|
|» description|string|false|none|Optional description of this rule's purpose|
|username|string|false|none|Username for Prometheus Basic authentication|
|password|string|false|none|Password for Prometheus Basic authentication|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|

#### Enumerated Values

|Property|Value|
|---|---|
|type|edge_prometheus|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|discoveryType|static|
|discoveryType|dns|
|discoveryType|ec2|
|discoveryType|k8s-node|
|discoveryType|k8s-pods|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|kubernetes|
|protocol|http|
|protocol|https|
|recordType|SRV|
|recordType|A|
|recordType|AAAA|
|scrapeProtocol|http|
|scrapeProtocol|https|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|

<h2 id="tocS_InputOffice365Mgmt">InputOffice365Mgmt</h2>
<!-- backwards compatibility -->
<a id="schemainputoffice365mgmt"></a>
<a id="schema_InputOffice365Mgmt"></a>
<a id="tocSinputoffice365mgmt"></a>
<a id="tocsinputoffice365mgmt"></a>

```json
{
  "id": "string",
  "type": "office365_mgmt",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "planType": "enterprise_gcc",
  "tenantId": "string",
  "appId": "string",
  "timeout": 300,
  "keepAliveTime": 30,
  "jobTimeout": "0",
  "maxMissedKeepAlives": 3,
  "ttl": "4h",
  "ignoreGroupJobsLimit": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "publisherIdentifier": "string",
  "contentConfig": [
    {
      "contentType": "Active Directory",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    },
    {
      "contentType": "Exchange",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    },
    {
      "contentType": "SharePoint",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    },
    {
      "contentType": "General",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    },
    {
      "contentType": "DLP",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    }
  ],
  "ingestionLag": 0,
  "retryRules": {
    "type": "none",
    "interval": 1000,
    "limit": 5,
    "multiplier": 2,
    "codes": [
      429,
      500,
      503
    ],
    "enableHeader": true,
    "retryConnectTimeout": false,
    "retryConnectReset": false
  },
  "authType": "manual",
  "description": "string",
  "clientSecret": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|planType|string|true|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|tenantId|string|true|none|Office 365 Azure Tenant ID|
|appId|string|true|none|Office 365 Azure Application ID|
|timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|publisherIdentifier|string|false|none|Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)|
|contentConfig|[object]|false|none|Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.|
|» contentType|string|false|none|Office 365 Management Activity API Content Type|
|» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|» interval|number|false|none|none|
|» logLevel|string|false|none|Collector runtime Log Level|
|» enabled|boolean|false|none|none|
|ingestionLag|number|false|none|Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.|
|retryRules|object|false|none|none|
|» type|string|true|none|The algorithm to use when performing HTTP retries|
|» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|authType|string|false|none|Enter client secret directly, or select a stored secret|
|description|string|false|none|none|
|clientSecret|string|false|none|Office 365 Azure client secret|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|office365_mgmt|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|

<h2 id="tocS_InputOffice365Service">InputOffice365Service</h2>
<!-- backwards compatibility -->
<a id="schemainputoffice365service"></a>
<a id="schema_InputOffice365Service"></a>
<a id="tocSinputoffice365service"></a>
<a id="tocsinputoffice365service"></a>

```json
{
  "id": "string",
  "type": "office365_service",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "planType": "enterprise_gcc",
  "tenantId": "string",
  "appId": "string",
  "timeout": 300,
  "keepAliveTime": 30,
  "jobTimeout": "0",
  "maxMissedKeepAlives": 3,
  "ttl": "4h",
  "ignoreGroupJobsLimit": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "contentConfig": [
    {
      "contentType": "Current Status",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    },
    {
      "contentType": "Messages",
      "description": "Poll interval minutes (1-60)",
      "interval": 15,
      "logLevel": "info",
      "enabled": false
    }
  ],
  "retryRules": {
    "type": "none",
    "interval": 1000,
    "limit": 5,
    "multiplier": 2,
    "codes": [
      429,
      500,
      503
    ],
    "enableHeader": true,
    "retryConnectTimeout": false,
    "retryConnectReset": false
  },
  "authType": "manual",
  "description": "string",
  "clientSecret": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|tenantId|string|true|none|Office 365 Azure Tenant ID|
|appId|string|true|none|Office 365 Azure Application ID|
|timeout|number|false|none|HTTP request inactivity timeout, use 0 to disable|
|keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|contentConfig|[object]|false|none|Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: */${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.|
|» contentType|string|false|none|Office 365 Services API Content Type|
|» description|string|false|none|If interval type is minutes the value entered must evenly divisible by 60 or save will fail|
|» interval|number|false|none|none|
|» logLevel|string|false|none|Collector runtime Log Level|
|» enabled|boolean|false|none|none|
|retryRules|object|false|none|none|
|» type|string|true|none|The algorithm to use when performing HTTP retries|
|» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|authType|string|false|none|Enter client secret directly, or select a stored secret|
|description|string|false|none|none|
|clientSecret|string|false|none|Office 365 Azure client secret|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|office365_service|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|

<h2 id="tocS_InputOffice365MsgTrace">InputOffice365MsgTrace</h2>
<!-- backwards compatibility -->
<a id="schemainputoffice365msgtrace"></a>
<a id="schema_InputOffice365MsgTrace"></a>
<a id="tocSinputoffice365msgtrace"></a>
<a id="tocsinputoffice365msgtrace"></a>

```json
{
  "id": "string",
  "type": "office365_msg_trace",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "url": "https://reports.office365.com/ecp/reportingwebservice/reporting.svc/MessageTrace",
  "interval": 60,
  "startDate": "string",
  "endDate": "string",
  "timeout": 300,
  "disableTimeFilter": true,
  "authType": "manual",
  "rescheduleDroppedTasks": true,
  "maxTaskReschedule": 1,
  "logLevel": "error",
  "jobTimeout": "0",
  "keepAliveTime": 30,
  "maxMissedKeepAlives": 3,
  "ttl": "4h",
  "ignoreGroupJobsLimit": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "retryRules": {
    "type": "none",
    "interval": 1000,
    "limit": 5,
    "multiplier": 2,
    "codes": [
      429,
      500,
      503
    ],
    "enableHeader": true,
    "retryConnectTimeout": false,
    "retryConnectReset": false
  },
  "description": "string",
  "username": "string",
  "password": "string",
  "credentialsSecret": "string",
  "clientSecret": "string",
  "tenantId": "string",
  "clientId": "string",
  "resource": "https://outlook.office365.com",
  "planType": "enterprise_gcc",
  "textSecret": "string",
  "certOptions": {
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string"
  }
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|url|string|true|none|URL to use when retrieving report data.|
|interval|number|true|none|How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.|
|startDate|string|false|none|Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.|
|endDate|string|false|none|Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.|
|timeout|number|false|none|HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.|
|disableTimeFilter|boolean|false|none|Disables time filtering of events when a date range is specified.|
|authType|string|false|none|Select authentication method.|
|rescheduleDroppedTasks|boolean|false|none|Reschedule tasks that failed with non-fatal errors|
|maxTaskReschedule|number|false|none|Maximum number of times a task can be rescheduled|
|logLevel|string|false|none|Log Level (verbosity) for collection runtime behavior.|
|jobTimeout|string|false|none|Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.|
|keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|retryRules|object|false|none|none|
|» type|string|true|none|The algorithm to use when performing HTTP retries|
|» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|» codes|[number]|false|none|List of http codes that trigger a retry. Leave empty to use the default list of 429, 500, and 503.|
|» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|description|string|false|none|none|
|username|string|false|none|Username to run Message Trace API call.|
|password|string|false|none|Password to run Message Trace API call.|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials.|
|clientSecret|string|false|none|client_secret to pass in the OAuth request parameter.|
|tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory.|
|clientId|string|false|none|client_id to pass in the OAuth request parameter.|
|resource|string|false|none|Resource to pass in the OAuth request parameter.|
|planType|string|false|none|Office 365 subscription plan for your organization, typically Office 365 Enterprise|
|textSecret|string|false|none|Select or create a secret that references your client_secret to pass in the OAuth request parameter.|
|certOptions|object|false|none|none|
|» certificateName|string|false|none|The name of the predefined certificate.|
|» privKeyPath|string|true|none|Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt the private key.|
|» certPath|string|true|none|Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|office365_msg_trace|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|authType|oauth|
|authType|oauthSecret|
|authType|oauthCert|
|logLevel|error|
|logLevel|warn|
|logLevel|info|
|logLevel|debug|
|logLevel|silly|
|type|none|
|type|backoff|
|type|static|
|planType|enterprise_gcc|
|planType|gcc|
|planType|gcc_high|
|planType|dod|

<h2 id="tocS_InputEventhub">InputEventhub</h2>
<!-- backwards compatibility -->
<a id="schemainputeventhub"></a>
<a id="schema_InputEventhub"></a>
<a id="tocSinputeventhub"></a>
<a id="tocsinputeventhub"></a>

```json
{
  "id": "string",
  "type": "eventhub",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "brokers": [
    "string"
  ],
  "topics": [],
  "groupId": "Cribl",
  "fromBeginning": true,
  "connectionTimeout": 10000,
  "requestTimeout": 60000,
  "maxRetries": 5,
  "maxBackOff": 30000,
  "initialBackoff": 300,
  "backoffRate": 2,
  "authenticationTimeout": 10000,
  "reauthenticationThreshold": 10000,
  "sasl": {
    "disabled": false,
    "authType": "manual",
    "password": "string",
    "textSecret": "string",
    "mechanism": "plain",
    "username": "$ConnectionString",
    "clientSecretAuthType": "manual",
    "clientSecret": "string",
    "clientTextSecret": "string",
    "certificateName": "string",
    "certPath": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "oauthEndpoint": "https://login.microsoftonline.com",
    "clientId": "string",
    "tenantId": "string",
    "scope": "string"
  },
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true
  },
  "sessionTimeout": 30000,
  "rebalanceTimeout": 60000,
  "heartbeatInterval": 3000,
  "autoCommitInterval": 1000,
  "autoCommitThreshold": 1,
  "maxBytesPerPartition": 1048576,
  "maxBytes": 10485760,
  "maxSocketErrors": 0,
  "minimizeDuplicates": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|brokers|[string]|true|none|List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.|
|topics|[string]|true|none|The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.|
|groupId|string|false|none|The consumer group this instance belongs to. Default is 'Cribl'.|
|fromBeginning|boolean|false|none|Start reading from earliest available data; relevant only during initial subscription|
|connectionTimeout|number|false|none|Maximum time to wait for a connection to complete successfully|
|requestTimeout|number|false|none|Maximum time to wait for Kafka to respond to a request|
|maxRetries|number|false|none|If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data|
|maxBackOff|number|false|none|The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).|
|initialBackoff|number|false|none|Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).|
|backoffRate|number|false|none|Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.|
|authenticationTimeout|number|false|none|Maximum time to wait for Kafka to respond to an authentication request|
|reauthenticationThreshold|number|false|none|Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.|
|sasl|object|false|none|Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.|
|» disabled|boolean|true|none|none|
|» authType|string|false|none|Enter password directly, or select a stored secret|
|» password|string|false|none|Connection-string primary key, or connection-string secondary key, from the Event Hubs workspace|
|» textSecret|string|false|none|Select or create a stored text secret|
|» mechanism|string|false|none|none|
|» username|string|false|none|The username for authentication. For Event Hubs, this should always be $ConnectionString.|
|» clientSecretAuthType|string|false|none|none|
|» clientSecret|string|false|none|client_secret to pass in the OAuth request parameter|
|» clientTextSecret|string|false|none|Select or create a stored text secret|
|» certificateName|string|false|none|Select or create a stored certificate|
|» certPath|string|false|none|none|
|» privKeyPath|string|false|none|none|
|» passphrase|string|false|none|none|
|» oauthEndpoint|string|false|none|Endpoint used to acquire authentication tokens from Azure|
|» clientId|string|false|none|client_id to pass in the OAuth request parameter|
|» tenantId|string|false|none|Directory ID (tenant identifier) in Azure Active Directory|
|» scope|string|false|none|Scope to pass in the OAuth request parameter|
|tls|object|false|none|none|
|» disabled|boolean|true|none|none|
|» rejectUnauthorized|boolean|false|none|Reject certificates that are not authorized by a CA in the CA certificate path, or by another trusted CA (such as the system's)|
|sessionTimeout|number|false|none|Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.<br>      If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.<br>      Value must be lower than rebalanceTimeout.<br>      See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|rebalanceTimeout|number|false|none|Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.<br>      If the timeout is exceeded, the coordinator broker will remove the worker from the group.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|heartbeatInterval|number|false|none|Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.<br>      Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.<br>      See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).|
|autoCommitInterval|number|false|none|How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|autoCommitThreshold|number|false|none|How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.|
|maxBytesPerPartition|number|false|none|Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).|
|maxBytes|number|false|none|Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).|
|maxSocketErrors|number|false|none|Maximum number of network errors before the consumer re-creates a socket|
|minimizeDuplicates|boolean|false|none|Minimize duplicate events by starting only one consumer for each topic partition|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|eventhub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|mechanism|plain|
|mechanism|oauthbearer|
|clientSecretAuthType|manual|
|clientSecretAuthType|secret|
|clientSecretAuthType|certificate|
|oauthEndpoint|https://login.microsoftonline.com|
|oauthEndpoint|https://login.microsoftonline.us|
|oauthEndpoint|https://login.partner.microsoftonline.cn|

<h2 id="tocS_InputExec">InputExec</h2>
<!-- backwards compatibility -->
<a id="schemainputexec"></a>
<a id="schema_InputExec"></a>
<a id="tocSinputexec"></a>
<a id="tocsinputexec"></a>

```json
{
  "id": "string",
  "type": "exec",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "command": "string",
  "retries": 10,
  "scheduleType": "interval",
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string",
  "interval": 60,
  "cronSchedule": "* * * * *"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|command|string|true|none|Command to execute; supports Bourne shell (or CMD on Windows) syntax|
|retries|number|false|none|Maximum number of retry attempts in the event that the command fails|
|scheduleType|string|false|none|Select a schedule type; either an interval (in seconds) or a cron-style schedule.|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|
|interval|number|false|none|Interval between command executions in seconds.|
|cronSchedule|string|false|none|Cron schedule to execute the command on.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|exec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|scheduleType|interval|
|scheduleType|cronSchedule|

<h2 id="tocS_InputFirehose">InputFirehose</h2>
<!-- backwards compatibility -->
<a id="schemainputfirehose"></a>
<a id="schema_InputFirehose"></a>
<a id="tocSinputfirehose"></a>
<a id="tocsinputfirehose"></a>

```json
{
  "id": "string",
  "type": "firehose",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    "string"
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|firehose|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputGooglePubsub">InputGooglePubsub</h2>
<!-- backwards compatibility -->
<a id="schemainputgooglepubsub"></a>
<a id="schema_InputGooglePubsub"></a>
<a id="tocSinputgooglepubsub"></a>
<a id="tocsinputgooglepubsub"></a>

```json
{
  "id": "string",
  "type": "google_pubsub",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "topicName": "cribl",
  "subscriptionName": "string",
  "monitorSubscription": false,
  "createTopic": false,
  "createSubscription": true,
  "region": "string",
  "googleAuthMethod": "auto",
  "serviceAccountCredentials": "string",
  "secret": "string",
  "maxBacklog": 1000,
  "concurrency": 5,
  "requestTimeout": 60000,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string",
  "orderedDelivery": false
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|topicName|string|true|none|ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.|
|subscriptionName|string|true|none|ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription|
|monitorSubscription|boolean|false|none|Use when the subscription is not created by this Source and topic is not known|
|createTopic|boolean|false|none|Create topic if it does not exist|
|createSubscription|boolean|false|none|Create subscription if it does not exist|
|region|string|false|none|Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.|
|googleAuthMethod|string|false|none|Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.|
|serviceAccountCredentials|string|false|none|Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.|
|secret|string|false|none|Select or create a stored text secret|
|maxBacklog|number|false|none|If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events|
|concurrency|number|false|none|How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.|
|requestTimeout|number|false|none|Pull request timeout, in milliseconds|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|
|orderedDelivery|boolean|false|none|Receive events in the order they were added to the queue. The process sending events must have ordering enabled.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|google_pubsub|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|googleAuthMethod|auto|
|googleAuthMethod|manual|
|googleAuthMethod|secret|

<h2 id="tocS_InputCribl">InputCribl</h2>
<!-- backwards compatibility -->
<a id="schemainputcribl"></a>
<a id="schema_InputCribl"></a>
<a id="tocSinputcribl"></a>
<a id="tocsinputcribl"></a>

```json
{
  "id": "string",
  "type": "cribl",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "filter": "string",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|filter|string|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputCriblTcp">InputCriblTcp</h2>
<!-- backwards compatibility -->
<a id="schemainputcribltcp"></a>
<a id="schema_InputCriblTcp"></a>
<a id="tocSinputcribltcp"></a>
<a id="tocsinputcribltcp"></a>

```json
{
  "id": "string",
  "type": "cribl_tcp",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveCxn": 1000,
  "socketIdleTimeout": 0,
  "socketEndingMaxWait": 30,
  "socketMaxLifespan": 0,
  "enableProxyHeader": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "enableLoadBalancing": false,
  "authTokens": [
    {
      "tokenSecret": "string",
      "enabled": true,
      "description": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.|
|» tokenSecret|string|true|none|Select or create a stored text secret|
|» enabled|boolean|false|none|none|
|» description|string|false|none|Optional token description|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl_tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputCriblHttp">InputCriblHttp</h2>
<!-- backwards compatibility -->
<a id="schemainputcriblhttp"></a>
<a id="schema_InputCriblHttp"></a>
<a id="tocSinputcriblhttp"></a>
<a id="tocsinputcriblhttp"></a>

```json
{
  "id": "string",
  "type": "cribl_http",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    {
      "tokenSecret": "string",
      "enabled": true,
      "description": "string"
    }
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[object]|false|none|Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.|
|» tokenSecret|string|true|none|Select or create a stored text secret|
|» enabled|boolean|false|none|none|
|» description|string|false|none|Optional token description|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputCriblLakeHttp">InputCriblLakeHttp</h2>
<!-- backwards compatibility -->
<a id="schemainputcribllakehttp"></a>
<a id="schema_InputCriblLakeHttp"></a>
<a id="tocSinputcribllakehttp"></a>
<a id="tocsinputcribllakehttp"></a>

```json
{
  "id": "string",
  "type": "cribl_lake_http",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    "string"
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "criblAPI": "/cribl",
  "elasticAPI": "/elastic",
  "splunkHecAPI": "/services/collector",
  "splunkHecAcks": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "authTokensExt": [
    {
      "token": "string",
      "description": "string",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ],
      "splunkHecMetadata": {
        "enabled": true
      },
      "elasticsearchMetadata": {
        "enabled": true
      }
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|criblAPI|string|false|none|Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.|
|elasticAPI|string|false|none|Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.|
|splunkHecAPI|string|false|none|Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.|
|splunkHecAcks|boolean|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|authTokensExt|[object]|false|none|none|
|» token|string|true|none|none|
|» description|string|false|none|none|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|» splunkHecMetadata|object|false|none|none|
|»» enabled|boolean|false|none|none|
|» elasticsearchMetadata|object|false|none|none|
|»» enabled|boolean|false|none|none|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cribl_lake_http|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputTcpjson">InputTcpjson</h2>
<!-- backwards compatibility -->
<a id="schemainputtcpjson"></a>
<a id="schema_InputTcpjson"></a>
<a id="tocSinputtcpjson"></a>
<a id="tocsinputtcpjson"></a>

```json
{
  "id": "string",
  "type": "tcpjson",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "ipWhitelistRegex": "/.*/",
  "maxActiveCxn": 1000,
  "socketIdleTimeout": 0,
  "socketEndingMaxWait": 30,
  "socketMaxLifespan": 0,
  "enableProxyHeader": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "enableLoadBalancing": false,
  "authType": "manual",
  "description": "string",
  "authToken": "",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|description|string|false|none|none|
|authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|tcpjson|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|

<h2 id="tocS_InputSystemMetrics">InputSystemMetrics</h2>
<!-- backwards compatibility -->
<a id="schemainputsystemmetrics"></a>
<a id="schema_InputSystemMetrics"></a>
<a id="tocSinputsystemmetrics"></a>
<a id="tocsinputsystemmetrics"></a>

```json
{
  "id": "string",
  "type": "system_metrics",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "interval": 10,
  "host": {
    "mode": "basic",
    "custom": {
      "system": {
        "mode": "basic",
        "processes": false
      },
      "cpu": {
        "mode": "basic",
        "perCpu": false,
        "detail": false,
        "time": false
      },
      "memory": {
        "mode": "basic",
        "detail": false
      },
      "network": {
        "mode": "basic",
        "detail": false,
        "protocols": false,
        "devices": [
          "!lo",
          "*"
        ],
        "perInterface": false
      },
      "disk": {
        "mode": "basic",
        "detail": false,
        "inodes": false,
        "devices": [
          "!loop*",
          "*"
        ],
        "mountpoints": [],
        "fstypes": [],
        "perDevice": false
      }
    }
  },
  "process": {
    "sets": [
      {
        "name": "string",
        "filter": "string",
        "includeChildren": false
      }
    ]
  },
  "container": {
    "mode": "basic",
    "dockerSocket": [
      "/var/run/docker.sock",
      "/run/docker.sock"
    ],
    "dockerTimeout": 5,
    "filters": [
      {
        "expr": "string"
      }
    ],
    "allContainers": false,
    "perDevice": false,
    "detail": false
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none",
    "destPath": "$CRIBL_HOME/state/system_metrics"
  },
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|host|object|false|none|none|
|» mode|string|false|none|Select level of detail for host metrics|
|» custom|object|false|none|none|
|»» system|object|false|none|none|
|»»» mode|string|false|none|Select the level of detail for system metrics|
|»»» processes|boolean|false|none|Generate metrics for the numbers of processes in various states|
|»» cpu|object|false|none|none|
|»»» mode|string|false|none|Select the level of detail for CPU metrics|
|»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»» memory|object|false|none|none|
|»»» mode|string|false|none|Select the level of detail for memory metrics|
|»»» detail|boolean|false|none|Generate metrics for all memory states|
|»» network|object|false|none|none|
|»»» mode|string|false|none|Select the level of detail for network metrics|
|»»» detail|boolean|false|none|Generate full network metrics|
|»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»» devices|[string]|false|none|Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.|
|»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»» disk|object|false|none|none|
|»»» mode|string|false|none|Select the level of detail for disk metrics|
|»»» detail|boolean|false|none|Generate full disk metrics|
|»»» inodes|boolean|false|none|Generate filesystem inode metrics|
|»»» devices|[string]|false|none|Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.|
|»»» mountpoints|[string]|false|none|Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.|
|»»» fstypes|[string]|false|none|Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.|
|»»» perDevice|boolean|false|none|Generate separate metrics for each device|
|process|object|false|none|none|
|» sets|[object]|false|none|Configure sets to collect process metrics|
|»» name|string|true|none|none|
|»» filter|string|true|none|none|
|»» includeChildren|boolean|false|none|none|
|container|object|false|none|none|
|» mode|string|false|none|Select the level of detail for container metrics|
|» dockerSocket|[string]|false|none|Full paths for Docker's UNIX-domain socket|
|» dockerTimeout|number|false|none|Timeout, in seconds, for the Docker API|
|» filters|[object]|false|none|Containers matching any of these will be included. All are included if no filters are added.|
|»» expr|string|true|none|none|
|» allContainers|boolean|false|none|Include stopped and paused containers|
|» perDevice|boolean|false|none|Generate separate metrics for each device|
|» detail|boolean|false|none|Generate full container metrics|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|» timeWindow|string|false|none|Time span for each file bucket|
|» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|» compress|string|false|none|none|
|» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|system_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputSystemState">InputSystemState</h2>
<!-- backwards compatibility -->
<a id="schemainputsystemstate"></a>
<a id="schema_InputSystemState"></a>
<a id="tocSinputsystemstate"></a>
<a id="tocsinputsystemstate"></a>

```json
{
  "id": "string",
  "type": "system_state",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "interval": 300,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "collectors": {
    "hostsfile": {
      "enable": true
    },
    "interfaces": {
      "enable": true
    },
    "disk": {
      "enable": true
    },
    "metadata": {
      "enable": true
    },
    "routes": {
      "enable": true
    },
    "dns": {
      "enable": true
    },
    "user": {
      "enable": true
    },
    "firewall": {
      "enable": true
    },
    "services": {
      "enable": true
    },
    "ports": {
      "enable": true
    },
    "loginUsers": {
      "enable": true
    }
  },
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none",
    "destPath": "$CRIBL_HOME/state/system_state"
  },
  "disableNativeModule": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|interval|number|false|none|Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|collectors|object|false|none|none|
|» hostsfile|object|false|none|Creates events based on entries collected from the hosts file|
|»» enable|boolean|false|none|none|
|» interfaces|object|false|none|Creates events for each of the host’s network interfaces|
|»» enable|boolean|false|none|none|
|» disk|object|false|none|Creates events for physical disks, partitions, and file systems|
|»» enable|boolean|false|none|none|
|» metadata|object|false|none|Creates events based on the host system’s current state|
|»» enable|boolean|false|none|none|
|» routes|object|false|none|Creates events based on entries collected from the host’s network routes|
|»» enable|boolean|false|none|none|
|» dns|object|false|none|Creates events for DNS resolvers and search entries|
|»» enable|boolean|false|none|none|
|» user|object|false|none|Creates events for local users and groups|
|»» enable|boolean|false|none|none|
|» firewall|object|false|none|Creates events for Firewall rules entries|
|»» enable|boolean|false|none|none|
|» services|object|false|none|Creates events from the list of services|
|»» enable|boolean|false|none|none|
|» ports|object|false|none|Creates events from list of listening ports|
|»» enable|boolean|false|none|none|
|» loginUsers|object|false|none|Creates events from list of logged-in users|
|»» enable|boolean|false|none|none|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|» timeWindow|string|false|none|Time span for each file bucket|
|» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|» compress|string|false|none|none|
|» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state|
|disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|system_state|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputKubeMetrics">InputKubeMetrics</h2>
<!-- backwards compatibility -->
<a id="schemainputkubemetrics"></a>
<a id="schema_InputKubeMetrics"></a>
<a id="tocSinputkubemetrics"></a>
<a id="tocsinputkubemetrics"></a>

```json
{
  "id": "string",
  "type": "kube_metrics",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "interval": 15,
  "rules": [
    {
      "filter": "!metadata.namespace.startsWith('kube-')",
      "description": "Ignore the kube-* namespace"
    }
  ],
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none",
    "destPath": "$CRIBL_HOME/state/kube_metrics"
  },
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|interval|number|false|none|Time, in seconds, between consecutive metrics collections. Default is 15 secs.|
|rules|[object]|false|none|Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.|
|» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|» description|string|false|none|Optional description of this rule's purpose|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool metrics on disk for Cribl Search|
|» timeWindow|string|false|none|Time span for each file bucket|
|» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|» compress|string|false|none|none|
|» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kube_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputKubeLogs">InputKubeLogs</h2>
<!-- backwards compatibility -->
<a id="schemainputkubelogs"></a>
<a id="schema_InputKubeLogs"></a>
<a id="tocSinputkubelogs"></a>
<a id="tocsinputkubelogs"></a>

```json
{
  "id": "string",
  "type": "kube_logs",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "interval": 15,
  "rules": [
    {
      "filter": "!metadata.namespace.startsWith('kube-')",
      "description": "Ignore the kube-* namespace"
    }
  ],
  "timestamps": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none"
  },
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "enableLoadBalancing": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|interval|number|false|none|Time, in seconds, between checks for new containers. Default is 15 secs.|
|rules|[object]|false|none|Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.|
|» filter|string|true|none|JavaScript expression applied to Pod objects. Return 'true' to include it.|
|» description|string|false|none|Optional description of this rule's purpose|
|timestamps|boolean|false|none|For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool events on disk for Cribl Edge and Search. Default is disabled.|
|» timeWindow|string|false|none|Time period for grouping spooled events. Default is 10m.|
|» maxDataSize|string|false|none|Maximum disk space that can be consumed before older buckets are deleted. Examples: 420MB, 4GB. Default is 1GB.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data before older buckets are deleted. Examples: 2h, 4d. Default is 24h.|
|» compress|string|false|none|Data compression format. Default is gzip.|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kube_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputKubeEvents">InputKubeEvents</h2>
<!-- backwards compatibility -->
<a id="schemainputkubeevents"></a>
<a id="schema_InputKubeEvents"></a>
<a id="tocSinputkubeevents"></a>
<a id="tocsinputkubeevents"></a>

```json
{
  "id": "string",
  "type": "kube_events",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "rules": [],
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|rules|[object]|false|none|Filtering on event fields|
|» filter|string|true|none|JavaScript expression applied to Kubernetes objects. Return 'true' to include it.|
|» description|string|false|none|Optional description of this rule's purpose|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kube_events|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputWindowsMetrics">InputWindowsMetrics</h2>
<!-- backwards compatibility -->
<a id="schemainputwindowsmetrics"></a>
<a id="schema_InputWindowsMetrics"></a>
<a id="tocSinputwindowsmetrics"></a>
<a id="tocsinputwindowsmetrics"></a>

```json
{
  "id": "string",
  "type": "windows_metrics",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "interval": 10,
  "host": {
    "mode": "basic",
    "custom": {
      "system": {
        "mode": "basic",
        "detail": false
      },
      "cpu": {
        "mode": "basic",
        "perCpu": false,
        "detail": false,
        "time": false
      },
      "memory": {
        "mode": "basic",
        "detail": false
      },
      "network": {
        "mode": "basic",
        "detail": false,
        "protocols": false,
        "devices": [
          "!6to4*",
          "!*Debug*",
          "!*Virtual*",
          "!*Tunneling*",
          "!*IP-HTTPS*",
          "*"
        ],
        "perInterface": false
      },
      "disk": {
        "mode": "basic",
        "perVolume": false,
        "detail": false,
        "volumes": [
          "!HarddiskVolume*",
          "*"
        ]
      }
    }
  },
  "process": {
    "sets": [
      {
        "name": "string",
        "filter": "string",
        "includeChildren": false
      }
    ]
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none",
    "destPath": "$CRIBL_HOME/state/windows_metrics"
  },
  "disableNativeModule": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|interval|number|false|none|Time, in seconds, between consecutive metric collections. Default is 10 seconds.|
|host|object|false|none|none|
|» mode|string|false|none|Select level of detail for host metrics|
|» custom|object|false|none|none|
|»» system|object|false|none|none|
|»»» mode|string|false|none|Select the level of details for system metrics|
|»»» detail|boolean|false|none|Generate metrics for all system information|
|»» cpu|object|false|none|none|
|»»» mode|string|false|none|Select the level of details for CPU metrics|
|»»» perCpu|boolean|false|none|Generate metrics for each CPU|
|»»» detail|boolean|false|none|Generate metrics for all CPU states|
|»»» time|boolean|false|none|Generate raw, monotonic CPU time counters|
|»» memory|object|false|none|none|
|»»» mode|string|false|none|Select the level of details for memory metrics|
|»»» detail|boolean|false|none|Generate metrics for all memory states|
|»» network|object|false|none|none|
|»»» mode|string|false|none|Select the level of details for network metrics|
|»»» detail|boolean|false|none|Generate full network metrics|
|»»» protocols|boolean|false|none|Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite|
|»»» devices|[string]|false|none|Network interfaces to include/exclude. All interfaces are included if this list is empty.|
|»»» perInterface|boolean|false|none|Generate separate metrics for each interface|
|»» disk|object|false|none|none|
|»»» mode|string|false|none|Select the level of details for disk metrics|
|»»» perVolume|boolean|false|none|Generate separate metrics for each volume|
|»»» detail|boolean|false|none|Generate full disk metrics|
|»»» volumes|[string]|false|none|Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.|
|process|object|false|none|none|
|» sets|[object]|false|none|Configure sets to collect process metrics|
|»» name|string|true|none|none|
|»» filter|string|true|none|none|
|»» includeChildren|boolean|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool metrics to disk for Cribl Edge and Search|
|» timeWindow|string|false|none|Time span for each file bucket|
|» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|» compress|string|false|none|none|
|» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics|
|disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|windows_metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|mode|basic|
|mode|all|
|mode|custom|
|mode|disabled|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputCrowdstrike">InputCrowdstrike</h2>
<!-- backwards compatibility -->
<a id="schemainputcrowdstrike"></a>
<a id="schema_InputCrowdstrike"></a>
<a id="tocSinputcrowdstrike"></a>
<a id="tocsinputcrowdstrike"></a>

```json
{
  "id": "string",
  "type": "crowdstrike",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "queueName": "string",
  "fileFilter": "/.*/",
  "awsAccountId": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "maxMessages": 1,
  "visibilityTimeout": 21600,
  "numReceivers": 1,
  "socketTimeout": 300,
  "skipOnError": false,
  "includeSqsMetadata": false,
  "enableAssumeRole": true,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "enableSQSAssumeRole": false,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "checkpointing": {
    "enabled": false,
    "retries": 5
  },
  "pollTimeout": 10,
  "encoding": "string",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "tagAfterProcessing": false,
  "processedTagKey": "string",
  "processedTagValue": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|preprocess|object|false|none|none|
|» disabled|boolean|true|none|none|
|» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|» args|[string]|false|none|Arguments to be added to the custom command|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|checkpointing|object|false|none|none|
|» enabled|boolean|true|none|Resume processing files after an interruption|
|» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|tagAfterProcessing|any|false|none|none|
|processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|crowdstrike|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|

<h2 id="tocS_InputDatadogAgent">InputDatadogAgent</h2>
<!-- backwards compatibility -->
<a id="schemainputdatadogagent"></a>
<a id="schema_InputDatadogAgent"></a>
<a id="tocSinputdatadogagent"></a>
<a id="tocsinputdatadogagent"></a>

```json
{
  "id": "string",
  "type": "datadog_agent",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "extractMetrics": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "proxyMode": {
    "enabled": false,
    "rejectUnauthorized": true
  },
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|extractMetrics|boolean|false|none|Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|proxyMode|object|false|none|none|
|» enabled|boolean|true|none|Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.|
|» rejectUnauthorized|boolean|false|none|Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|datadog_agent|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputDatagen">InputDatagen</h2>
<!-- backwards compatibility -->
<a id="schemainputdatagen"></a>
<a id="schema_InputDatagen"></a>
<a id="tocSinputdatagen"></a>
<a id="tocsinputdatagen"></a>

```json
{
  "id": "string",
  "type": "datagen",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "samples": [
    {
      "sample": "string",
      "eventsPerSec": 10
    }
  ],
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|samples|[object]|true|none|none|
|» sample|string|true|none|none|
|» eventsPerSec|number|true|none|Maximum number of events to generate per second per Worker Node. Defaults to 10.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|datagen|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputHttpRaw">InputHttpRaw</h2>
<!-- backwards compatibility -->
<a id="schemainputhttpraw"></a>
<a id="schema_InputHttpRaw"></a>
<a id="tocSinputhttpraw"></a>
<a id="tocsinputhttpraw"></a>

```json
{
  "id": "string",
  "type": "http_raw",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    "string"
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "allowedPaths": [
    "*"
  ],
  "allowedMethods": [
    "*"
  ],
  "authTokensExt": [
    {
      "token": "string",
      "description": "string",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|allowedPaths|[string]|false|none|List of URI paths accepted by this input, wildcards are supported, e.g /api/v*/hook. Defaults to allow all.|
|allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|» description|string|false|none|none|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|http_raw|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputKinesis">InputKinesis</h2>
<!-- backwards compatibility -->
<a id="schemainputkinesis"></a>
<a id="schema_InputKinesis"></a>
<a id="tocSinputkinesis"></a>
<a id="tocsinputkinesis"></a>

```json
{
  "id": "string",
  "type": "kinesis",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "streamName": "string",
  "serviceInterval": 1,
  "shardExpr": "true",
  "shardIteratorType": "TRIM_HORIZON",
  "payloadFormat": "cribl",
  "getRecordsLimit": 5000,
  "getRecordsLimitTotal": 20000,
  "loadBalancingAlgorithm": "ConsistentHashing",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "verifyKPLCheckSums": false,
  "avoidDuplicates": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|streamName|string|true|none|Kinesis Data Stream to read data from|
|serviceInterval|number|false|none|Time interval in minutes between consecutive service calls|
|shardExpr|string|false|none|A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.|
|shardIteratorType|string|false|none|Location at which to start reading a shard for the first time|
|payloadFormat|string|false|none|Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.|
|getRecordsLimit|number|false|none|Maximum number of records per getRecords call|
|getRecordsLimitTotal|number|false|none|Maximum number of records, across all shards, to pull down at once per Worker Process|
|loadBalancingAlgorithm|string|false|none|The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|true|none|Region where the Kinesis stream is located|
|endpoint|string|false|none|Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing Kinesis stream requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Kinesis stream|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|verifyKPLCheckSums|boolean|false|none|Verify Kinesis Producer Library (KPL) event checksums|
|avoidDuplicates|boolean|false|none|When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|

#### Enumerated Values

|Property|Value|
|---|---|
|type|kinesis|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|shardIteratorType|TRIM_HORIZON|
|shardIteratorType|LATEST|
|payloadFormat|cribl|
|payloadFormat|ndjson|
|payloadFormat|cloudwatch|
|payloadFormat|line|
|loadBalancingAlgorithm|ConsistentHashing|
|loadBalancingAlgorithm|RoundRobin|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|

<h2 id="tocS_InputCriblmetrics">InputCriblmetrics</h2>
<!-- backwards compatibility -->
<a id="schemainputcriblmetrics"></a>
<a id="schema_InputCriblmetrics"></a>
<a id="tocSinputcriblmetrics"></a>
<a id="tocsinputcriblmetrics"></a>

```json
{
  "id": "string",
  "type": "criblmetrics",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "prefix": "cribl.logstream.",
  "fullFidelity": true,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|prefix|string|false|none|A prefix that is applied to the metrics provided by Cribl Stream|
|fullFidelity|boolean|false|none|Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|criblmetrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputMetrics">InputMetrics</h2>
<!-- backwards compatibility -->
<a id="schemainputmetrics"></a>
<a id="schema_InputMetrics"></a>
<a id="tocSinputmetrics"></a>
<a id="tocsinputmetrics"></a>

```json
{
  "id": "string",
  "type": "metrics",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "udpPort": 65535,
  "tcpPort": 65535,
  "maxBufferSize": 1000,
  "ipWhitelistRegex": "/.*/",
  "enableProxyHeader": false,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "udpSocketRxBufSize": 256,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|metrics|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputS3">InputS3</h2>
<!-- backwards compatibility -->
<a id="schemainputs3"></a>
<a id="schema_InputS3"></a>
<a id="tocSinputs3"></a>
<a id="tocsinputs3"></a>

```json
{
  "id": "string",
  "type": "s3",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "queueName": "string",
  "fileFilter": "/.*/",
  "awsAccountId": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "maxMessages": 1,
  "visibilityTimeout": 600,
  "numReceivers": 1,
  "socketTimeout": 300,
  "skipOnError": false,
  "includeSqsMetadata": false,
  "enableAssumeRole": true,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "enableSQSAssumeRole": false,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "parquetChunkSizeMB": 5,
  "parquetChunkDownloadTimeout": 600,
  "checkpointing": {
    "enabled": false,
    "retries": 5
  },
  "pollTimeout": 10,
  "encoding": "string",
  "tagAfterProcessing": false,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "processedTagKey": "string",
  "processedTagValue": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|preprocess|object|false|none|none|
|» disabled|boolean|true|none|none|
|» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|» args|[string]|false|none|Arguments to be added to the custom command|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|checkpointing|object|false|none|none|
|» enabled|boolean|true|none|Resume processing files after an interruption|
|» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|tagAfterProcessing|boolean|false|none|Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|s3|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|

<h2 id="tocS_InputS3Inventory">InputS3Inventory</h2>
<!-- backwards compatibility -->
<a id="schemainputs3inventory"></a>
<a id="schema_InputS3Inventory"></a>
<a id="tocSinputs3inventory"></a>
<a id="tocsinputs3inventory"></a>

```json
{
  "id": "string",
  "type": "s3_inventory",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "queueName": "string",
  "fileFilter": "/.*/",
  "awsAccountId": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "maxMessages": 1,
  "visibilityTimeout": 600,
  "numReceivers": 1,
  "socketTimeout": 300,
  "skipOnError": false,
  "includeSqsMetadata": false,
  "enableAssumeRole": true,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "enableSQSAssumeRole": false,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "parquetChunkSizeMB": 5,
  "parquetChunkDownloadTimeout": 600,
  "checkpointing": {
    "enabled": false,
    "retries": 5
  },
  "pollTimeout": 10,
  "checksumSuffix": "checksum",
  "maxManifestSizeKB": 4096,
  "validateInventoryFiles": false,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "tagAfterProcessing": false,
  "processedTagKey": "string",
  "processedTagValue": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|preprocess|object|false|none|none|
|» disabled|boolean|true|none|none|
|» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|» args|[string]|false|none|Arguments to be added to the custom command|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|checkpointing|object|false|none|none|
|» enabled|boolean|true|none|Resume processing files after an interruption|
|» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|checksumSuffix|string|false|none|Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"|
|maxManifestSizeKB|integer|false|none|Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.|
|validateInventoryFiles|boolean|false|none|If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|tagAfterProcessing|any|false|none|none|
|processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|s3_inventory|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|

<h2 id="tocS_InputSnmp">InputSnmp</h2>
<!-- backwards compatibility -->
<a id="schemainputsnmp"></a>
<a id="schema_InputSnmp"></a>
<a id="tocSinputsnmp"></a>
<a id="tocsinputsnmp"></a>

```json
{
  "id": "string",
  "type": "snmp",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 162,
  "snmpV3Auth": {
    "v3AuthEnabled": false,
    "allowUnmatchedTrap": false,
    "v3Users": [
      {
        "name": "string",
        "authProtocol": "none",
        "authKey": null,
        "privProtocol": "none"
      }
    ]
  },
  "maxBufferSize": 1000,
  "ipWhitelistRegex": "/.*/",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "udpSocketRxBufSize": 256,
  "varbindsWithTypes": false,
  "bestEffortParsing": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|port|number|true|none|UDP port to receive SNMP traps on. Defaults to 162.|
|snmpV3Auth|object|false|none|Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.|
|» v3AuthEnabled|boolean|true|none|none|
|» allowUnmatchedTrap|boolean|false|none|Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.|
|» v3Users|[object]|false|none|User credentials for receiving v3 traps|
|»» name|string|true|none|none|
|»» authProtocol|string|false|none|none|
|»» authKey|any|false|none|none|
|»» privProtocol|any|false|none|none|
|maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|varbindsWithTypes|boolean|false|none|If enabled, parses varbinds as an array of objects that include OID, value, and type|
|bestEffortParsing|boolean|false|none|If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|snmp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authProtocol|none|
|authProtocol|md5|
|authProtocol|sha|
|authProtocol|sha224|
|authProtocol|sha256|
|authProtocol|sha384|
|authProtocol|sha512|

<h2 id="tocS_InputOpenTelemetry">InputOpenTelemetry</h2>
<!-- backwards compatibility -->
<a id="schemainputopentelemetry"></a>
<a id="schema_InputOpenTelemetry"></a>
<a id="tocSinputopentelemetry"></a>
<a id="tocsinputopentelemetry"></a>

```json
{
  "id": "string",
  "type": "open_telemetry",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 4317,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": null,
  "captureHeaders": null,
  "activityLogSampleRate": null,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 15,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "protocol": "grpc",
  "extractSpans": false,
  "extractMetrics": false,
  "otlpVersion": "0.10.0",
  "authType": "none",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "maxActiveCxn": 1000,
  "description": "string",
  "username": "string",
  "password": "string",
  "token": "string",
  "credentialsSecret": "string",
  "textSecret": "string",
  "loginUrl": "string",
  "secretParamName": "string",
  "secret": "string",
  "tokenAttributeName": "string",
  "authHeaderExpr": "`Bearer ${token}`",
  "tokenTimeoutSecs": 3600,
  "oauthParams": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "oauthHeaders": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "extractLogs": false
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|any|false|none|none|
|captureHeaders|any|false|none|none|
|activityLogSampleRate|any|false|none|none|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).|
|enableHealthCheck|boolean|false|none|Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|protocol|string|false|none|Select whether to leverage gRPC or HTTP for OpenTelemetry|
|extractSpans|boolean|false|none|Enable to extract each incoming span to a separate event|
|extractMetrics|boolean|false|none|Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point|
|otlpVersion|string|false|none|The version of OTLP Protobuf definitions to use when interpreting received data|
|authType|string|false|none|OpenTelemetry authentication type|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|description|string|false|none|none|
|username|string|false|none|none|
|password|string|false|none|none|
|token|string|false|none|Bearer token to include in the authorization header|
|credentialsSecret|string|false|none|Select or create a secret that references your credentials|
|textSecret|string|false|none|Select or create a stored text secret|
|loginUrl|string|false|none|URL for OAuth|
|secretParamName|string|false|none|Secret parameter name to pass in request body|
|secret|string|false|none|Secret parameter value to pass in request body|
|tokenAttributeName|string|false|none|Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').|
|authHeaderExpr|string|false|none|JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.|
|tokenTimeoutSecs|number|false|none|How often the OAuth token should be refreshed.|
|oauthParams|[object]|false|none|Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth parameter name|
|» value|string|true|none|OAuth parameter value|
|oauthHeaders|[object]|false|none|Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.|
|» name|string|true|none|OAuth header name|
|» value|string|true|none|OAuth header value|
|extractLogs|boolean|false|none|Enable to extract each incoming log record to a separate event|

#### Enumerated Values

|Property|Value|
|---|---|
|type|open_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|protocol|grpc|
|protocol|http|
|otlpVersion|0.10.0|
|otlpVersion|1.3.1|
|authType|none|
|authType|basic|
|authType|credentialsSecret|
|authType|token|
|authType|textSecret|
|authType|oauth|

<h2 id="tocS_InputModelDrivenTelemetry">InputModelDrivenTelemetry</h2>
<!-- backwards compatibility -->
<a id="schemainputmodeldriventelemetry"></a>
<a id="schema_InputModelDrivenTelemetry"></a>
<a id="tocSinputmodeldriventelemetry"></a>
<a id="tocsinputmodeldriventelemetry"></a>

```json
{
  "id": "string",
  "type": "model_driven_telemetry",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 57000,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "maxActiveCxn": 1000,
  "shutdownTimeoutMs": 5000,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|shutdownTimeoutMs|number|false|none|Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|model_driven_telemetry|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputSqs">InputSqs</h2>
<!-- backwards compatibility -->
<a id="schemainputsqs"></a>
<a id="schema_InputSqs"></a>
<a id="tocSinputsqs"></a>
<a id="tocsinputsqs"></a>

```json
{
  "id": "string",
  "type": "sqs",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "queueName": "string",
  "queueType": "standard",
  "awsAccountId": "string",
  "createQueue": false,
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "enableAssumeRole": false,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "maxMessages": 10,
  "visibilityTimeout": 600,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "pollTimeout": 10,
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "numReceivers": 3
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|queueName|string|true|none|The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|queueType|string|true|none|The queue type used (or created)|
|awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|createQueue|boolean|false|none|Create queue if it does not exist|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|endpoint|string|false|none|SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing SQS requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access SQS|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|sqs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|queueType|standard|
|queueType|fifo|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|

<h2 id="tocS_InputSyslog">InputSyslog</h2>
<!-- backwards compatibility -->
<a id="schemainputsyslog"></a>
<a id="schema_InputSyslog"></a>
<a id="tocSinputsyslog"></a>
<a id="tocsinputsyslog"></a>

```json
{
  "id": "string",
  "type": "syslog",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "udpPort": 65535,
  "tcpPort": 65535,
  "maxBufferSize": 1000,
  "ipWhitelistRegex": "/.*/",
  "timestampTimezone": "local",
  "singleMsgUdpPackets": false,
  "enableProxyHeader": false,
  "keepFieldsList": [],
  "octetCounting": false,
  "inferFraming": true,
  "strictlyInferOctetCounting": true,
  "allowNonStandardAppName": false,
  "maxActiveCxn": 1000,
  "socketIdleTimeout": 0,
  "socketEndingMaxWait": 30,
  "socketMaxLifespan": 0,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "udpSocketRxBufSize": 256,
  "enableLoadBalancing": false,
  "description": "string",
  "enableEnhancedProxyHeaderParsing": true
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|udpPort|number|false|none|Enter UDP port number to listen on. Not required if listening on TCP.|
|tcpPort|number|false|none|Enter TCP port number to listen on. Not required if listening on UDP.|
|maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking. Only applies to UDP.|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|timestampTimezone|string|false|none|Timezone to assign to timestamps without timezone info|
|singleMsgUdpPackets|boolean|false|none|Treat UDP packet data received as full syslog message|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2|
|keepFieldsList|[string]|false|none|Wildcard list of fields to keep from source data; * = ALL (default)|
|octetCounting|boolean|false|none|Enable if incoming messages use octet counting per RFC 6587.|
|inferFraming|boolean|false|none|Enable if we should infer the syslog framing of the incoming messages.|
|strictlyInferOctetCounting|boolean|false|none|Enable if we should infer octet counting only if the messages comply with RFC 5424.|
|allowNonStandardAppName|boolean|false|none|Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.|
|socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|enableLoadBalancing|boolean|false|none|Load balance traffic across all Worker Processes|
|description|string|false|none|none|
|enableEnhancedProxyHeaderParsing|boolean|false|none|When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.|

anyOf

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|object|false|none|none|

or

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|object|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|syslog|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputFile">InputFile</h2>
<!-- backwards compatibility -->
<a id="schemainputfile"></a>
<a id="schema_InputFile"></a>
<a id="tocSinputfile"></a>
<a id="tocsinputfile"></a>

```json
{
  "id": "string",
  "type": "file",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "mode": "manual",
  "interval": 10,
  "filenames": [
    "*/log/*",
    "*log"
  ],
  "filterArchivedFiles": false,
  "tailOnly": true,
  "idleTimeout": 300,
  "minAgeDur": "string",
  "maxAgeDur": "string",
  "checkFileModTime": false,
  "forceText": false,
  "hashLen": 256,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "description": "string",
  "path": "string",
  "depth": 0,
  "suppressMissingPathErrors": false,
  "deleteFiles": false,
  "includeUnidentifiableBinary": false
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|mode|string|false|none|Choose how to discover files to monitor|
|interval|number|false|none|Time, in seconds, between scanning for files|
|filenames|[string]|false|none|The full path of discovered files are matched against this wildcard list|
|filterArchivedFiles|boolean|false|none|Apply filename allowlist to file entries in archive file types, like tar or zip.|
|tailOnly|boolean|false|none|Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.|
|idleTimeout|number|false|none|Time, in seconds, before an idle file is closed|
|minAgeDur|string|false|none|The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.|
|maxAgeDur|string|false|none|The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.|
|checkFileModTime|boolean|false|none|Skip files with modification times earlier than the maximum age duration|
|forceText|boolean|false|none|Forces files containing binary data to be streamed as text|
|hashLen|number|false|none|Length of file header bytes to use in hash for unique file identification|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|description|string|false|none|none|
|path|string|false|none|Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.|
|depth|number|false|none|Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.|
|suppressMissingPathErrors|boolean|false|none|none|
|deleteFiles|boolean|false|none|Delete files after they have been collected|
|includeUnidentifiableBinary|boolean|false|none|Stream binary files as Base64-encoded chunks.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|file|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|mode|manual|
|mode|auto|

<h2 id="tocS_InputTcp">InputTcp</h2>
<!-- backwards compatibility -->
<a id="schemainputtcp"></a>
<a id="schema_InputTcp"></a>
<a id="tocSinputtcp"></a>
<a id="tocsinputtcp"></a>

```json
{
  "id": "string",
  "type": "tcp",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "ipWhitelistRegex": "/.*/",
  "maxActiveCxn": 1000,
  "socketIdleTimeout": 0,
  "socketEndingMaxWait": 30,
  "socketMaxLifespan": 0,
  "enableProxyHeader": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "enableHeader": false,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "description": "string",
  "authToken": "",
  "authType": "manual",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|enableHeader|boolean|false|none|Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }|
|preprocess|object|false|none|none|
|» disabled|boolean|true|none|none|
|» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|» args|[string]|false|none|Arguments to be added to the custom command|
|description|string|false|none|none|
|authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|tcp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|authType|manual|
|authType|secret|

<h2 id="tocS_InputAppscope">InputAppscope</h2>
<!-- backwards compatibility -->
<a id="schemainputappscope"></a>
<a id="schema_InputAppscope"></a>
<a id="tocSinputappscope"></a>
<a id="tocsinputappscope"></a>

```json
{
  "id": "string",
  "type": "appscope",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "ipWhitelistRegex": "/.*/",
  "maxActiveCxn": 1000,
  "socketIdleTimeout": 0,
  "socketEndingMaxWait": 30,
  "socketMaxLifespan": 0,
  "enableProxyHeader": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "enableUnixPath": false,
  "filter": {
    "allow": [
      {
        "procname": "string",
        "arg": "string",
        "config": "string"
      }
    ],
    "transportURL": "string"
  },
  "persistence": {
    "enable": false,
    "timeWindow": "10m",
    "maxDataSize": "1GB",
    "maxDataTime": "24h",
    "compress": "none",
    "destPath": "$CRIBL_HOME/state/appscope"
  },
  "authType": "manual",
  "description": "string",
  "host": "string",
  "port": 65535,
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "unixSocketPath": "$CRIBL_HOME/state/appscope.sock",
  "unixSocketPerms": "string",
  "authToken": "",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to establish a connection|
|maxActiveCxn|number|false|none|Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.|
|socketIdleTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.|
|socketEndingMaxWait|number|false|none|How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.|
|socketMaxLifespan|number|false|none|The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.|
|enableProxyHeader|boolean|false|none|Enable if the connection is proxied by a device that supports proxy protocol v1 or v2|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|enableUnixPath|boolean|false|none|Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.|
|filter|object|false|none|none|
|» allow|[object]|false|none|Specify processes that AppScope should be loaded into, and the config to use.|
|»» procname|string|true|none|Specify the name of a process or family of processes.|
|»» arg|string|false|none|Specify a string to substring-match against process command-line.|
|»» config|string|true|none|Choose a config to apply to processes that match the process name and/or argument.|
|» transportURL|string|false|none|To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.|
|persistence|object|false|none|none|
|» enable|boolean|false|none|Spool events and metrics on disk for Cribl Edge and Search|
|» timeWindow|string|false|none|Time span for each file bucket|
|» maxDataSize|string|false|none|Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.|
|» maxDataTime|string|false|none|Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.|
|» compress|string|false|none|none|
|» destPath|string|false|none|Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope|
|authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|description|string|false|none|none|
|host|string|false|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|false|none|Port to listen on|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|unixSocketPath|string|false|none|Path to the UNIX domain socket to listen on.|
|unixSocketPerms|string|false|none|Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.|
|authToken|string|false|none|Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|appscope|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputWef">InputWef</h2>
<!-- backwards compatibility -->
<a id="schemainputwef"></a>
<a id="schema_InputWef"></a>
<a id="tocSinputwef"></a>
<a id="tocsinputwef"></a>

```json
{
  "id": "string",
  "type": "wef",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 5986,
  "authMethod": "clientCert",
  "tls": {
    "disabled": false,
    "rejectUnauthorized": true,
    "requestCert": true,
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "commonNameRegex": "/.*/",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1",
    "ocspCheck": false,
    "keytab": null,
    "principal": null,
    "ocspCheckFailClose": false
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "keepAliveTimeout": 90,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "socketTimeout": 0,
  "caFingerprint": "string",
  "keytab": "string",
  "principal": "string",
  "allowMachineIdMismatch": false,
  "subscriptions": [
    {
      "subscriptionName": "string",
      "version": "string",
      "contentFormat": "Raw",
      "heartbeatInterval": 60,
      "batchTimeout": 60,
      "readExistingEvents": false,
      "sendBookmarks": true,
      "compress": true,
      "targets": [
        "*"
      ],
      "locale": "en-US",
      "querySelector": "simple",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string",
  "logFingerprintMismatch": false
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authMethod|string|false|none|How to authenticate incoming client connections|
|tls|object|false|none|none|
|» disabled|boolean|false|none|Enable TLS|
|» rejectUnauthorized|boolean|false|none|Required for WEF certificate authentication|
|» requestCert|boolean|false|none|Required for WEF certificate authentication|
|» certificateName|string|false|none|Name of the predefined certificate|
|» privKeyPath|string|true|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|true|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|true|none|Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|» ocspCheck|boolean|false|none|Enable OCSP check of certificate|
|» keytab|any|false|none|none|
|» principal|any|false|none|none|
|» ocspCheckFailClose|boolean|false|none|If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Preserve the client’s original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.|
|captureHeaders|boolean|false|none|Add request headers to events in the __headers field|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|caFingerprint|string|false|none|SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain|
|keytab|string|false|none|Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.|
|principal|string|false|none|Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>|
|allowMachineIdMismatch|boolean|false|none|Allow events to be ingested even if their MachineID does not match the client certificate CN|
|subscriptions|[object]|true|none|Subscriptions to events on forwarding endpoints|
|» subscriptionName|string|true|none|none|
|» version|string|false|none|Version UUID for this subscription. If any subscription parameters are modified, this value will change.|
|» contentFormat|string|true|none|Content format in which the endpoint should deliver events|
|» heartbeatInterval|number|true|none|Maximum time (in seconds) between endpoint checkins before considering it unavailable|
|» batchTimeout|number|true|none|Interval (in seconds) over which the endpoint should collect events before sending them to Stream|
|» readExistingEvents|boolean|false|none|Newly subscribed endpoints will send previously existing events. Disable to receive new events only.|
|» sendBookmarks|boolean|false|none|Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.|
|» compress|boolean|false|none|Receive compressed events from the source|
|» targets|[string]|true|none|The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com|
|» locale|string|false|none|The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".|
|» querySelector|string|false|none|none|
|» metadata|[object]|false|none|Fields to add to events ingested under this subscription|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|
|logFingerprintMismatch|boolean|false|none|Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|wef|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authMethod|clientCert|
|authMethod|kerberos|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|
|contentFormat|Raw|
|contentFormat|RenderedText|
|querySelector|simple|
|querySelector|xml|

<h2 id="tocS_InputWinEventLogs">InputWinEventLogs</h2>
<!-- backwards compatibility -->
<a id="schemainputwineventlogs"></a>
<a id="schema_InputWinEventLogs"></a>
<a id="tocSinputwineventlogs"></a>
<a id="tocsinputwineventlogs"></a>

```json
{
  "id": "string",
  "type": "win_event_logs",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "logNames": [
    ""
  ],
  "readMode": "oldest",
  "eventFormat": "json",
  "disableNativeModule": false,
  "interval": 10,
  "batchSize": 500,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "maxEventBytes": 51200,
  "description": "string",
  "disableJsonRendering": false,
  "disableXmlRendering": true
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|logNames|[string]|true|none|Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.|
|readMode|string|false|none|Read all stored and future event logs, or only future events|
|eventFormat|string|false|none|Format of individual events|
|disableNativeModule|boolean|false|none|Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)|
|interval|number|false|none|Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|batchSize|number|false|none|The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|maxEventBytes|number|false|none|The maximum number of bytes in an event before it is flushed to the pipelines|
|description|string|false|none|none|
|disableJsonRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|
|disableXmlRendering|boolean|false|none|Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)|

#### Enumerated Values

|Property|Value|
|---|---|
|type|win_event_logs|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|readMode|oldest|
|readMode|newest|
|eventFormat|json|
|eventFormat|xml|

<h2 id="tocS_InputRawUdp">InputRawUdp</h2>
<!-- backwards compatibility -->
<a id="schemainputrawudp"></a>
<a id="schema_InputRawUdp"></a>
<a id="tocSinputrawudp"></a>
<a id="tocsinputrawudp"></a>

```json
{
  "id": "string",
  "type": "raw_udp",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "maxBufferSize": 1000,
  "ipWhitelistRegex": "/.*/",
  "singleMsgUdpPackets": false,
  "ingestRawBytes": false,
  "udpSocketRxBufSize": 256,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|port|number|true|none|Port to listen on|
|maxBufferSize|number|false|none|Maximum number of events to buffer when downstream is blocking.|
|ipWhitelistRegex|string|false|none|Regex matching IP addresses that are allowed to send data|
|singleMsgUdpPackets|boolean|false|none|If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.|
|ingestRawBytes|boolean|false|none|If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.|
|udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|raw_udp|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputJournalFiles">InputJournalFiles</h2>
<!-- backwards compatibility -->
<a id="schemainputjournalfiles"></a>
<a id="schema_InputJournalFiles"></a>
<a id="tocSinputjournalfiles"></a>
<a id="tocsinputjournalfiles"></a>

```json
{
  "id": "string",
  "type": "journal_files",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "path": "string",
  "interval": 10,
  "journals": [
    "system"
  ],
  "rules": [
    {
      "filter": "severity <= 4",
      "description": "Allow log messages having 'emergency', 'alert', 'critical', 'error', or 'warning' priority"
    }
  ],
  "currentBoot": false,
  "maxAgeDur": "string",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|path|string|true|none|Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.|
|interval|number|false|none|Time, in seconds, between scanning for journals.|
|journals|[string]|true|none|The full path of discovered journals are matched against this wildcard list.|
|rules|[object]|false|none|Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.|
|» filter|string|true|none|JavaScript expression applied to Journal objects. Return 'true' to include it.|
|» description|string|false|none|Optional description of this rule's purpose|
|currentBoot|boolean|false|none|Skip log messages that are not part of the current boot session.|
|maxAgeDur|string|false|none|The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|journal_files|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputWiz">InputWiz</h2>
<!-- backwards compatibility -->
<a id="schemainputwiz"></a>
<a id="schema_InputWiz"></a>
<a id="tocSinputwiz"></a>
<a id="tocsinputwiz"></a>

```json
{
  "id": "string",
  "type": "wiz",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "endpoint": "https://api.<region>.app.wiz.io/graphql",
  "authUrl": "string",
  "authAudienceOverride": "string",
  "clientId": "string",
  "contentConfig": [
    {
      "contentType": "Audit Logs",
      "contentDescription": "Get all Audit Logs",
      "contentQuery": "({\n  query: `query AuditLogTable($first: Int $after: String $filterBy: AuditLogEntryFilters){\n    auditLogEntries(first: $first after: $after filterBy: $filterBy) {\n      nodes {\n        id\n        action\n        requestId\n        status\n        timestamp\n        actionParameters\n        userAgent\n        sourceIP\n        serviceAccount {\n          id\n          name\n        }\n        user {\n          id\n          name\n        }\n      }\n      pageInfo {\n        hasNextPage\n        endCursor\n      }\n    }\n  }`,\n  variables: {\n    first: 100,\n    after: endCursor,\n    filterBy: {\n      timestamp: {\n        after: `${C.Time.strftime(state.latestTime || earliest, '%Y-%m-%dT%H:%M:%SZ')}`,\n        before: `${C.Time.strftime(latest, '%Y-%m-%dT%H:%M:%SZ')}`\n      },\n      status: [\"SUCCESS\", \"FAILED\",\"INVALID\",\"ACCESS_DENIED\"]\n    }\n  }\n})",
      "cronSchedule": "*/15 * * * *",
      "earliest": "-15m@m",
      "latest": "now",
      "jobTimeout": "0",
      "logLevel": "info",
      "maxPages": 100,
      "enabled": false,
      "stateTracking": false,
      "stateUpdateExpression": "__timestampExtracted !== false && {latestTime: (state.latestTime || 0) > _time ? state.latestTime : _time}",
      "stateMergeExpression": "prevState.latestTime > newState.latestTime ? prevState : newState"
    },
    {
      "contentType": "Configuration Findings",
      "contentDescription": "Get Cloud Configuration Report",
      "contentQuery": "({\n  query: `query CloudConfigurationFindingsPage($filterBy: ConfigurationFindingFilters, $first: Int, $after: String, $orderBy: ConfigurationFindingOrder) {\n    configurationFindings(filterBy: $filterBy first: $first after: $after orderBy: $orderBy) {\n      nodes {\n        id\n        targetExternalId\n        targetObjectProviderUniqueId\n        firstSeenAt\n        severity\n        result\n        status\n        remediation\n        resource {\n          id\n          providerId\n          name\n          nativeType\n          type\n          region\n          subscription {\n            id\n            name\n            externalId\n            cloudProvider\n          }\n          projects {\n            id\n            name\n            riskProfile {\n              businessImpact\n            }\n          }\n          tags {\n            key\n            value\n          }\n        }\n        rule {\n          id\n          graphId\n          name\n          description\n          remediationInstructions\n          functionAsControl\n        }\n        securitySubCategories {\n          id\n          title\n          category {\n            id\n            name\n            framework {\n              id\n              name\n            }\n          }\n        }\n        ignoreRules {\n          id\n          name\n          enabled\n          expiredAt\n        }\n      }\n      pageInfo {\n        hasNextPage\n        endCursor\n      }\n    }\n  }`,\n  variables: {\n    first: 100,\n    after: endCursor,\n    filterBy: {\n      id: [],\n      source: [],\n      rule: {\n        id: [],\n        name: [],\n        description: []\n      },\n      result: [\"PASS\",\"FAIL\",\"ERROR\", \"NOT_ASSESSED\"],\n      severity: [\"NONE\",\"LOW\",\"MEDIUM\",\"HIGH\",\"CRITICAL\"],\n      status: [\"OPEN\",\"IN_PROGRESS\",\"RESOLVED\",\"REJECTED\"\n      ],\n      frameworkCategory: [],\n      firstSeenAt: {\n        after: `${C.Time.strftime(state.latestTime || earliest, '%Y-%m-%dT%H:%M:%SZ')}`,\n        before: `${C.Time.strftime(latest, '%Y-%m-%dT%H:%M:%SZ')}`\n      }\n    }\n  }\n})",
      "cronSchedule": "0 */12 * * *",
      "earliest": "-12h@h",
      "latest": "now",
      "jobTimeout": "0",
      "logLevel": "info",
      "maxPages": 100,
      "enabled": false,
      "stateTracking": false,
      "stateUpdateExpression": "__timestampExtracted !== false && {latestTime: (state.latestTime || 0) > _time ? state.latestTime : _time}",
      "stateMergeExpression": "prevState.latestTime > newState.latestTime ? prevState : newState"
    },
    {
      "contentType": "Issues",
      "contentDescription": "Get Open Issues",
      "contentQuery": "({\n  query: `query IssuesTable($filterBy: IssueFilters, $first: Int, $after: String, $orderBy: IssueOrder) {\n    issues: issuesV2(filterBy: $filterBy first: $first after: $after orderBy: $orderBy) {\n      nodes {\n        id\n        control {\n          id\n          name\n          description\n          resolutionRecommendation\n          securitySubCategories {\n            title\n            category {\n              name\n              framework {\n                name\n              }\n            }\n          }\n        }\n        createdAt\n        updatedAt\n        sourceRule {\n          id\n          name\n        }\n        dueAt\n        resolvedAt\n        statusChangedAt\n        project {\n          id\n          name\n          slug\n          businessUnit\n          riskProfile {\n            businessImpact\n          }\n        }\n        status\n        severity\n        type\n        entitySnapshot {\n          id\n          type\n          nativeType\n          name\n          status\n          cloudPlatform\n          cloudProviderURL\n          providerId\n          region\n          resourceGroupExternalId\n          subscriptionExternalId\n          subscriptionName\n          subscriptionTags\n          tags\n          externalId\n        }\n        notes {\n          createdAt\n          updatedAt\n          text\n          user {\n            name\n            email\n          }\n          serviceAccount {\n            name\n          }\n        }\n        serviceTickets {\n          externalId\n          name\n          url\n        }\n      }\n      pageInfo {\n        hasNextPage\n        endCursor\n      }\n    }\n  }`,\n  variables: {\n    first: 100,\n    after: endCursor,\n    filterBy: {\n      sourceRule: {\n        id: []\n      },\n      relatedEntity: {\n        type: []\n      },\n      status: [\"OPEN\",\"IN_PROGRESS\",\"RESOLVED\",\"REJECTED\"],\n      severity: [\"INFORMATIONAL\",\"LOW\",\"MEDIUM\",\"HIGH\",\"CRITICAL\"],\n      type: [\"TOXIC_COMBINATION\",\"THREAT_DETECTION\",\"CLOUD_CONFIGURATION\"],\n      createdAt: {\n        after: `${C.Time.strftime(state.latestTime || earliest, '%Y-%m-%dT%H:%M:%SZ')}`,\n        before: `${C.Time.strftime(latest, '%Y-%m-%dT%H:%M:%SZ')}`\n      }\n    }\n  }\n})",
      "cronSchedule": "0 */12 * * *",
      "earliest": "-12h@h",
      "latest": "now",
      "jobTimeout": "0",
      "logLevel": "info",
      "maxPages": 0,
      "enabled": false,
      "stateTracking": false,
      "stateUpdateExpression": "__timestampExtracted !== false && {latestTime: (state.latestTime || 0) > _time ? state.latestTime : _time}",
      "stateMergeExpression": "prevState.latestTime > newState.latestTime ? prevState : newState"
    },
    {
      "contentType": "Vulnerabilities",
      "contentDescription": "Get Vulnerability Findings",
      "contentQuery": "({\n  query: `query VulnerabilityFindingsPage($filterBy: VulnerabilityFindingFilters, $first: Int, $after: String, $orderBy: VulnerabilityFindingOrder) {\n    vulnerabilityFindings(filterBy: $filterBy first: $first after: $after orderBy: $orderBy) {\n      nodes {\n        id\n        portalUrl\n        name\n        CVEDescription\n        CVSSSeverity\n        score\n        exploitabilityScore\n        impactScore\n        hasExploit\n        hasCisaKevExploit\n        status\n        vendorSeverity\n        firstDetectedAt\n        lastDetectedAt\n        resolvedAt\n        description\n        remediation\n        detailedName\n        version\n        fixedVersion\n        detectionMethod\n        link\n        locationPath\n        resolutionReason\n        vulnerableAsset {\n          ... on VulnerableAssetBase {\n            id\n            type\n            name\n            region\n            providerUniqueId\n            cloudProviderURL\n            cloudPlatform\n            status\n            subscriptionName\n            subscriptionExternalId\n            subscriptionId\n            tags\n          }\n          ... on VulnerableAssetVirtualMachine {\n            operatingSystem\n            ipAddresses\n          }\n          ... on VulnerableAssetServerless {\n            runtime\n          }\n          ... on VulnerableAssetContainerImage {\n            imageId\n          }\n          ... on VulnerableAssetContainer {\n            ImageExternalId\n            VmExternalId\n            ServerlessContainer\n            PodNamespace\n            PodName\n            NodeName\n          }\n        }\n      }\n      pageInfo {\n        hasNextPage\n        endCursor\n      }\n    }\n  }`,\n  variables: {\n    first: 100,\n    after: endCursor,\n    filterBy: {\n      id: [],\n      status: [],\n      assetType: [],\n      vendorSeverity: [],\n      assetId: [],\n      vulnerabilityId: [],\n      detectionMethod: [],\n      assetStatus: [],\n      firstSeenAt: {\n        after: `${C.Time.strftime(state.latestTime || earliest, '%Y-%m-%dT%H:%M:%SZ')}`,\n        before: `${C.Time.strftime(latest, '%Y-%m-%dT%H:%M:%SZ')}`\n      }\n    }\n  }\n})",
      "cronSchedule": "0 */12 * * *",
      "earliest": "-12h@h",
      "latest": "now",
      "jobTimeout": "0",
      "logLevel": "info",
      "maxPages": 0,
      "enabled": false,
      "stateTracking": false,
      "stateUpdateExpression": "__timestampExtracted !== false && {latestTime: (state.latestTime || 0) > _time ? state.latestTime : _time}",
      "stateMergeExpression": "prevState.latestTime > newState.latestTime ? prevState : newState"
    }
  ],
  "requestTimeout": 300,
  "keepAliveTime": 30,
  "maxMissedKeepAlives": 3,
  "ttl": "4h",
  "ignoreGroupJobsLimit": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "retryRules": {
    "type": "none",
    "interval": 1000,
    "limit": 5,
    "multiplier": 2,
    "codes": [
      429,
      503
    ],
    "enableHeader": true,
    "retryConnectTimeout": false,
    "retryConnectReset": false
  },
  "authType": "manual",
  "description": "string",
  "clientSecret": "string",
  "textSecret": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|endpoint|string|true|none|The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql|
|authUrl|string|true|none|The authentication URL to generate an OAuth token|
|authAudienceOverride|string|false|none|The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.|
|clientId|string|true|none|The client ID of the Wiz application|
|contentConfig|[object]|true|none|none|
|» contentType|string|true|none|The name of the Wiz query|
|» contentDescription|string|false|none|none|
|» enabled|boolean|false|none|none|
|requestTimeout|number|false|none|HTTP request inactivity timeout. Use 0 to disable.|
|keepAliveTime|number|false|none|How often workers should check in with the scheduler to keep job subscription alive|
|maxMissedKeepAlives|number|false|none|The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.|
|ttl|string|false|none|Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.|
|ignoreGroupJobsLimit|boolean|false|none|When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|retryRules|object|false|none|none|
|» type|string|true|none|The algorithm to use when performing HTTP retries|
|» interval|number|false|none|Time interval between failed request and first retry (kickoff). Maximum allowed value is 20,000 ms (1/3 minute).|
|» limit|number|false|none|The maximum number of times to retry a failed HTTP request|
|» multiplier|number|false|none|Base for exponential backoff, e.g., base 2 means that retries will occur after 2, then 4, then 8 seconds, and so on|
|» codes|[number]|false|none|List of HTTP codes that trigger a retry. Leave empty to use the default list of 429 and 503.|
|» enableHeader|boolean|false|none|Honor any Retry-After header that specifies a delay (in seconds) or a timestamp after which to retry the request. The delay is limited to 20 seconds, even if the Retry-After header specifies a longer delay. When disabled, all Retry-After headers are ignored.|
|» retryConnectTimeout|boolean|false|none|Make a single retry attempt when a connection timeout (ETIMEDOUT) error occurs|
|» retryConnectReset|boolean|false|none|Retry request when a connection reset (ECONNRESET) error occurs|
|authType|string|false|none|Enter client secret directly, or select a stored secret|
|description|string|false|none|none|
|clientSecret|string|false|none|The client secret of the Wiz application|
|textSecret|string|false|none|Select or create a stored text secret|

#### Enumerated Values

|Property|Value|
|---|---|
|type|wiz|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|type|none|
|type|backoff|
|type|static|
|authType|manual|
|authType|secret|

<h2 id="tocS_InputWizWebhook">InputWizWebhook</h2>
<!-- backwards compatibility -->
<a id="schemainputwizwebhook"></a>
<a id="schema_InputWizWebhook"></a>
<a id="tocSinputwizwebhook"></a>
<a id="tocsinputwizwebhook"></a>

```json
{
  "id": "string",
  "type": "wiz_webhook",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    "string"
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "breakerRulesets": [
    "Cribl - Do Not Break Ruleset"
  ],
  "staleChannelFlushMs": 10000,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "allowedPaths": [
    "*"
  ],
  "allowedMethods": [
    "*"
  ],
  "authTokensExt": [
    {
      "token": "string",
      "description": "string",
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[string]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|boolean|false|none|Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|allowedPaths|[string]|false|none|List of URI paths accepted by this input. Wildcards are supported (such as /api/v*/hook). Defaults to allow all.|
|allowedMethods|[string]|false|none|List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.|
|authTokensExt|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|» token|string|true|none|Shared secret to be provided by any client (Authorization: <token>)|
|» description|string|false|none|none|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|wiz_webhook|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputNetflow">InputNetflow</h2>
<!-- backwards compatibility -->
<a id="schemainputnetflow"></a>
<a id="schema_InputNetflow"></a>
<a id="tocSinputnetflow"></a>
<a id="tocsinputnetflow"></a>

```json
{
  "id": "string",
  "type": "netflow",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 2055,
  "enablePassThrough": false,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "udpSocketRxBufSize": 256,
  "templateCacheMinutes": 30,
  "v5Enabled": true,
  "v9Enabled": true,
  "ipfixEnabled": false,
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.|
|port|number|true|none|Port to listen on|
|enablePassThrough|boolean|false|none|Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist.|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|udpSocketRxBufSize|number|false|none|Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.|
|templateCacheMinutes|number|false|none|Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.|
|v5Enabled|boolean|false|none|Accept messages in Netflow V5 format.|
|v9Enabled|boolean|false|none|Accept messages in Netflow V9 format.|
|ipfixEnabled|boolean|false|none|Accept messages in IPFIX format.|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|netflow|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|

<h2 id="tocS_InputSecurityLake">InputSecurityLake</h2>
<!-- backwards compatibility -->
<a id="schemainputsecuritylake"></a>
<a id="schema_InputSecurityLake"></a>
<a id="tocSinputsecuritylake"></a>
<a id="tocsinputsecuritylake"></a>

```json
{
  "id": "string",
  "type": "security_lake",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "queueName": "string",
  "fileFilter": "/.*/",
  "awsAccountId": "string",
  "awsAuthenticationMethod": "auto",
  "awsSecretKey": "string",
  "region": "string",
  "endpoint": "string",
  "signatureVersion": "v2",
  "reuseConnections": true,
  "rejectUnauthorized": true,
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "maxMessages": 1,
  "visibilityTimeout": 600,
  "numReceivers": 1,
  "socketTimeout": 300,
  "skipOnError": false,
  "includeSqsMetadata": false,
  "enableAssumeRole": true,
  "assumeRoleArn": "stringstringstringst",
  "assumeRoleExternalId": "string",
  "durationSeconds": 3600,
  "enableSQSAssumeRole": false,
  "preprocess": {
    "disabled": true,
    "command": "string",
    "args": [
      "string"
    ]
  },
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "parquetChunkSizeMB": 5,
  "parquetChunkDownloadTimeout": 600,
  "checkpointing": {
    "enabled": false,
    "retries": 5
  },
  "pollTimeout": 10,
  "encoding": "string",
  "description": "string",
  "awsApiKey": "string",
  "awsSecret": "string",
  "tagAfterProcessing": false,
  "processedTagKey": "string",
  "processedTagValue": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|queueName|string|true|none|The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.|
|fileFilter|string|false|none|Regex matching file names to download and process. Defaults to: .*|
|awsAccountId|string|false|none|SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.|
|awsAuthenticationMethod|string|false|none|AWS authentication method. Choose Auto to use IAM roles.|
|awsSecretKey|string|false|none|none|
|region|string|false|none|AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.|
|endpoint|string|false|none|S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.|
|signatureVersion|string|false|none|Signature version to use for signing S3 requests|
|reuseConnections|boolean|false|none|Reuse connections between requests, which can improve performance|
|rejectUnauthorized|boolean|false|none|Reject certificates that cannot be verified against a valid CA, such as self-signed certificates|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|maxMessages|number|false|none|The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.|
|visibilityTimeout|number|false|none|After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).|
|numReceivers|number|false|none|How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.|
|socketTimeout|number|false|none|Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.|
|skipOnError|boolean|false|none|Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.|
|includeSqsMetadata|boolean|false|none|Attach SQS notification metadata to a __sqsMetadata field on each event|
|enableAssumeRole|boolean|false|none|Use Assume Role credentials to access Amazon S3|
|assumeRoleArn|string|false|none|Amazon Resource Name (ARN) of the role to assume|
|assumeRoleExternalId|string|false|none|External ID to use when assuming role|
|durationSeconds|number|false|none|Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).|
|enableSQSAssumeRole|boolean|false|none|Use Assume Role credentials when accessing Amazon SQS|
|preprocess|object|false|none|none|
|» disabled|boolean|true|none|none|
|» command|string|false|none|Command to feed the data through (via stdin) and process its output (stdout)|
|» args|[string]|false|none|Arguments to be added to the custom command|
|metadata|[object]|false|none|Fields to add to events from this input|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|parquetChunkSizeMB|number|false|none|Maximum file size for each Parquet chunk|
|parquetChunkDownloadTimeout|number|false|none|The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.|
|checkpointing|object|false|none|none|
|» enabled|boolean|true|none|Resume processing files after an interruption|
|» retries|number|false|none|The number of times to retry processing when a processing error occurs. If Skip file on error is enabled, this setting is ignored.|
|pollTimeout|number|false|none|How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.|
|encoding|string|false|none|Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.|
|description|string|false|none|none|
|awsApiKey|string|false|none|none|
|awsSecret|string|false|none|Select or create a stored secret that references your access key and secret key|
|tagAfterProcessing|any|false|none|none|
|processedTagKey|string|false|none|The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|
|processedTagValue|string|false|none|The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.|

#### Enumerated Values

|Property|Value|
|---|---|
|type|security_lake|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|awsAuthenticationMethod|auto|
|awsAuthenticationMethod|manual|
|awsAuthenticationMethod|secret|
|signatureVersion|v2|
|signatureVersion|v4|
|tagAfterProcessing|false|
|tagAfterProcessing|true|

<h2 id="tocS_InputZscalerHec">InputZscalerHec</h2>
<!-- backwards compatibility -->
<a id="schemainputzscalerhec"></a>
<a id="schema_InputZscalerHec"></a>
<a id="tocSinputzscalerhec"></a>
<a id="tocsinputzscalerhec"></a>

```json
{
  "id": "string",
  "type": "zscaler_hec",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    {
      "authType": "manual",
      "tokenSecret": null,
      "token": null,
      "enabled": true,
      "description": "string",
      "allowedIndexesAtToken": [
        "string"
      ],
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": null,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "hecAPI": "/services/collector",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "allowedIndexes": [
    "string"
  ],
  "hecAcks": false,
  "accessControlAllowOrigin": [
    "string"
  ],
  "accessControlAllowHeaders": [
    "string"
  ],
  "emitTokenMetrics": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|» authType|string|false|none|Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate|
|» tokenSecret|any|false|none|none|
|» token|any|true|none|none|
|» enabled|boolean|false|none|none|
|» description|string|false|none|none|
|» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|any|false|none|none|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|hecAPI|string|true|none|Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.|
|metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|hecAcks|boolean|false|none|Whether to enable Zscaler HEC acknowledgements|
|accessControlAllowOrigin|[string]|false|none|Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|accessControlAllowHeaders|[string]|false|none|Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|emitTokenMetrics|boolean|false|none|Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|zscaler_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|manual|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_InputCloudflareHec">InputCloudflareHec</h2>
<!-- backwards compatibility -->
<a id="schemainputcloudflarehec"></a>
<a id="schema_InputCloudflareHec"></a>
<a id="tocSinputcloudflarehec"></a>
<a id="tocsinputcloudflarehec"></a>

```json
{
  "id": "string",
  "type": "cloudflare_hec",
  "disabled": false,
  "pipeline": "string",
  "sendToRoutes": true,
  "environment": "string",
  "pqEnabled": false,
  "streamtags": [],
  "connections": [
    {
      "pipeline": "string",
      "output": "string"
    }
  ],
  "pq": {
    "mode": "smart",
    "maxBufferSize": 1000,
    "commitFrequency": 42,
    "maxFileSize": "1 MB",
    "maxSize": "5GB",
    "path": "$CRIBL_HOME/state/queues",
    "compress": "none",
    "pqControls": {}
  },
  "host": "0.0.0.0",
  "port": 65535,
  "authTokens": [
    {
      "authType": "secret",
      "tokenSecret": null,
      "token": null,
      "enabled": true,
      "description": "string",
      "allowedIndexesAtToken": [
        "string"
      ],
      "metadata": [
        {
          "name": "string",
          "value": "string"
        }
      ]
    }
  ],
  "tls": {
    "disabled": true,
    "requestCert": false,
    "rejectUnauthorized": true,
    "commonNameRegex": "/.*/",
    "certificateName": "string",
    "privKeyPath": "string",
    "passphrase": "string",
    "certPath": "string",
    "caPath": "string",
    "minVersion": "TLSv1",
    "maxVersion": "TLSv1"
  },
  "maxActiveReq": 256,
  "maxRequestsPerSocket": 0,
  "enableProxyHeader": false,
  "captureHeaders": false,
  "activityLogSampleRate": 100,
  "requestTimeout": 0,
  "socketTimeout": 0,
  "keepAliveTimeout": 5,
  "enableHealthCheck": null,
  "ipAllowlistRegex": "/.*/",
  "ipDenylistRegex": "/^$/",
  "hecAPI": "string",
  "metadata": [
    {
      "name": "string",
      "value": "string"
    }
  ],
  "allowedIndexes": [
    "string"
  ],
  "breakerRulesets": [
    "string"
  ],
  "staleChannelFlushMs": 10000,
  "accessControlAllowOrigin": [
    "string"
  ],
  "accessControlAllowHeaders": [
    "string"
  ],
  "emitTokenMetrics": false,
  "description": "string"
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|id|string|false|none|Unique ID for this input|
|type|string|true|none|none|
|disabled|boolean|false|none|none|
|pipeline|string|false|none|Pipeline to process data from this Source before sending it through the Routes|
|sendToRoutes|boolean|false|none|Select whether to send data to Routes, or directly to Destinations.|
|environment|string|false|none|Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.|
|pqEnabled|boolean|false|none|Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).|
|streamtags|[string]|false|none|Tags for filtering and grouping in @{product}|
|connections|[object]|false|none|Direct connections to Destinations, and optionally via a Pipeline or a Pack|
|» pipeline|string|false|none|none|
|» output|string|true|none|none|
|pq|object|false|none|none|
|» mode|string|false|none|With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.|
|» maxBufferSize|number|false|none|The maximum number of events to hold in memory before writing the events to disk|
|» commitFrequency|number|false|none|The number of events to send downstream before committing that Stream has read them|
|» maxFileSize|string|false|none|The maximum size to store in each queue file before closing and optionally compressing. Enter a numeral with units of KB, MB, etc.|
|» maxSize|string|false|none|The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.|
|» path|string|false|none|The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>|
|» compress|string|false|none|Codec to use to compress the persisted data|
|» pqControls|object|false|none|none|
|host|string|true|none|Address to bind on. Defaults to 0.0.0.0 (all addresses).|
|port|number|true|none|Port to listen on|
|authTokens|[object]|false|none|Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.|
|» authType|string|false|none|Select Secret to use a text secret to authenticate|
|» tokenSecret|any|false|none|none|
|» token|any|false|none|none|
|» enabled|boolean|false|none|none|
|» description|string|false|none|none|
|» allowedIndexesAtToken|[string]|false|none|Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.|
|» metadata|[object]|false|none|Fields to add to events referencing this token|
|»» name|string|true|none|none|
|»» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|tls|object|false|none|none|
|» disabled|boolean|false|none|none|
|» requestCert|boolean|false|none|Require clients to present their certificates. Used to perform client authentication using SSL certs.|
|» rejectUnauthorized|boolean|false|none|Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's)|
|» commonNameRegex|string|false|none|Regex matching allowable common names in peer certificates' subject attribute|
|» certificateName|string|false|none|The name of the predefined certificate|
|» privKeyPath|string|false|none|Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.|
|» passphrase|string|false|none|Passphrase to use to decrypt private key|
|» certPath|string|false|none|Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.|
|» caPath|string|false|none|Path on server containing CA certificates to use. PEM format. Can reference $ENV_VARS.|
|» minVersion|string|false|none|none|
|» maxVersion|string|false|none|none|
|maxActiveReq|number|false|none|Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.|
|maxRequestsPerSocket|integer|false|none|Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).|
|enableProxyHeader|boolean|false|none|Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.|
|captureHeaders|boolean|false|none|Add request headers to events, in the __headers field|
|activityLogSampleRate|number|false|none|How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.|
|requestTimeout|number|false|none|How long to wait for an incoming request to complete before aborting it. Use 0 to disable.|
|socketTimeout|number|false|none|How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.|
|keepAliveTimeout|number|false|none|After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).|
|enableHealthCheck|any|false|none|none|
|ipAllowlistRegex|string|false|none|Messages from matched IP addresses will be processed, unless also matched by the denylist|
|ipDenylistRegex|string|false|none|Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.|
|hecAPI|string|true|none|Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.|
|metadata|[object]|false|none|Fields to add to every event. May be overridden by fields added at the token or request level.|
|» name|string|true|none|none|
|» value|string|true|none|JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)|
|allowedIndexes|[string]|false|none|List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.|
|breakerRulesets|[string]|false|none|A list of event-breaking rulesets that will be applied, in order, to the input data stream|
|staleChannelFlushMs|number|false|none|How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines|
|accessControlAllowOrigin|[string]|false|none|HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.|
|accessControlAllowHeaders|[string]|false|none|HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.|
|emitTokenMetrics|boolean|false|none|Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics|
|description|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|type|cloudflare_hec|
|mode|smart|
|mode|always|
|compress|none|
|compress|gzip|
|authType|secret|
|minVersion|TLSv1|
|minVersion|TLSv1.1|
|minVersion|TLSv1.2|
|minVersion|TLSv1.3|
|maxVersion|TLSv1|
|maxVersion|TLSv1.1|
|maxVersion|TLSv1.2|
|maxVersion|TLSv1.3|

<h2 id="tocS_HealthStringType">HealthStringType</h2>
<!-- backwards compatibility -->
<a id="schemahealthstringtype"></a>
<a id="schema_HealthStringType"></a>
<a id="tocShealthstringtype"></a>
<a id="tocshealthstringtype"></a>

```json
"Green"

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|*anonymous*|string|false|none|none|

#### Enumerated Values

|Property|Value|
|---|---|
|*anonymous*|Green|
|*anonymous*|Yellow|
|*anonymous*|Red|
|*anonymous*|Unknown|

<h2 id="tocS_HealthCountType">HealthCountType</h2>
<!-- backwards compatibility -->
<a id="schemahealthcounttype"></a>
<a id="schema_HealthCountType"></a>
<a id="tocShealthcounttype"></a>
<a id="tocshealthcounttype"></a>

```json
{
  "Green": 0,
  "Yellow": 0,
  "Red": 0,
  "Unknown": 0
}

```

### Properties

|Name|Type|Required|Restrictions|Description|
|---|---|---|---|---|
|Green|number|true|none|none|
|Yellow|number|true|none|none|
|Red|number|true|none|none|
|Unknown|number|true|none|none|

